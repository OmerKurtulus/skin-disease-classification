{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "machine_shape": "hm",
   "gpuType": "A100"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU",
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "b689353e27e04174b9f39cabee8f472c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_a53cc150408f4339a93d8453120b7f65",
       "IPY_MODEL_99a757a83b4443b9802a083dfcd2a20f",
       "IPY_MODEL_2e8987d6071e49e6a055d1caf99798e2"
      ],
      "layout": "IPY_MODEL_1e7d71e81c3b4e7496fc2d8317416bac"
     }
    },
    "a53cc150408f4339a93d8453120b7f65": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_de6d16d4fd414ceaa645b05b40e31ec6",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_c8cdc853c18b45e6ba0565bb646fdd1e",
      "value": "SÄ±nÄ±flar:â€‡100%"
     }
    },
    "99a757a83b4443b9802a083dfcd2a20f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_82b3bb195e1f4f078c39220420768fbc",
      "max": 5,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_8a3c45d0d4c14b0c9ccc3da436f35826",
      "value": 5
     }
    },
    "2e8987d6071e49e6a055d1caf99798e2": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_550d6e2cb82345a2a1015a6ec9535e6c",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_8ff9160291254d9592c8303db2b6e20f",
      "value": "â€‡5/5â€‡[00:06&lt;00:00,â€‡â€‡1.30s/it]"
     }
    },
    "1e7d71e81c3b4e7496fc2d8317416bac": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "de6d16d4fd414ceaa645b05b40e31ec6": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c8cdc853c18b45e6ba0565bb646fdd1e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "82b3bb195e1f4f078c39220420768fbc": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8a3c45d0d4c14b0c9ccc3da436f35826": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "550d6e2cb82345a2a1015a6ec9535e6c": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8ff9160291254d9592c8303db2b6e20f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "40d2d259f921456fbefc08c78c8d2c3d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_44c5d7854a064edd96e694d5e0211b07",
       "IPY_MODEL_8a20384affb14b9e8923b573f4f61125",
       "IPY_MODEL_2a7c13b9686747ed82fe3d1a914bddb5"
      ],
      "layout": "IPY_MODEL_732c3847b9414176a0d6bca79c9f2d16"
     }
    },
    "44c5d7854a064edd96e694d5e0211b07": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c7024ce4e78c458bac4357ee968798cf",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_c8d994c4eb974eabae23b9d289054e46",
      "value": "ðŸ”â€‡Boyutâ€‡analizi:â€‡100%"
     }
    },
    "8a20384affb14b9e8923b573f4f61125": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ae2d6bcc424b4f71ac1d55bf7f6b6392",
      "max": 3411,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_370e9c382e784cbcb128cb6c9e5ca934",
      "value": 3411
     }
    },
    "2a7c13b9686747ed82fe3d1a914bddb5": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_dea9e1822ad648c2b0ed6336c8ce7769",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_d58935b8eb34424faaf65802f4d8c13e",
      "value": "â€‡3411/3411â€‡[00:53&lt;00:00,â€‡39.42â€‡gÃ¶rÃ¼ntÃ¼/s]"
     }
    },
    "732c3847b9414176a0d6bca79c9f2d16": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c7024ce4e78c458bac4357ee968798cf": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c8d994c4eb974eabae23b9d289054e46": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ae2d6bcc424b4f71ac1d55bf7f6b6392": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "370e9c382e784cbcb128cb6c9e5ca934": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "dea9e1822ad648c2b0ed6336c8ce7769": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d58935b8eb34424faaf65802f4d8c13e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "# =============================================================================\n",
    "# CELL 1: GOOGLE DRIVE CONNECTION AND BASIC SETUP\n",
    "# =============================================================================\n",
    "# This cell enables mounting Google Drive in the Google Colab environment.\n",
    "# The dataset is located in the \"Skin_Disease_Classification_Dataset\" folder on Drive.\n",
    "# =============================================================================\n",
    "\n",
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Set working directory\n",
    "import os\n",
    "\n",
    "# Base directory path of the dataset\n",
    "# NOTE: You may need to update this path according to your Drive structure\n",
    "BASE_PATH = '/content/drive/MyDrive/Skin_Disease_Classification_Dataset/dataset'\n",
    "\n",
    "# Verify directory existence\n",
    "if os.path.exists(BASE_PATH):\n",
    "    print(\"Dataset directory found successfully.\")\n",
    "    print(f\"Dizin yolu: {BASE_PATH}\")\n",
    "\n",
    "    # List subdirectories (disease classes)\n",
    "    classes = os.listdir(BASE_PATH)\n",
    "    print(f\"\\nClasses found ({len(classes)}):\")\n",
    "    for cls in sorted(classes):\n",
    "        cls_path = os.path.join(BASE_PATH, cls)\n",
    "        if os.path.isdir(cls_path):\n",
    "            num_images = len([f for f in os.listdir(cls_path) if f.lower().endswith(('.jpg', '.jpeg', '.png'))])\n",
    "            print(f\"   - {cls}: {num_images} images\")\n",
    "else:\n",
    "    print(\"ERROR: Dataset directory not found!\")\n",
    "    print(f\"   Checked path: {BASE_PATH}\")\n",
    "    print(\"   Please verify the directory path.\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SNVwfvH3cYX0",
    "outputId": "ec2950f4-0142-4e29-9fe3-a846fd35f650"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# =============================================================================\n",
    "# CELL 2: IMPORTING REQUIRED LIBRARIES\n",
    "# =============================================================================\n",
    "# This cell imports all libraries used throughout the project.\n",
    "# Libraries are organized by category.\n",
    "# =============================================================================\n",
    "\n",
    "# --- Core Python Libraries ---\n",
    "import os                          # Operating system operations (file/directory management)\n",
    "import random                      # Random number generation\n",
    "import warnings                    # Warning message management\n",
    "warnings.filterwarnings('ignore')  # Suppress unnecessary warnings\n",
    "\n",
    "# --- Data Processing and Mathematical Operations ---\n",
    "import numpy as np                 # Numerical computations and array operations\n",
    "import pandas as pd                # DataFrame operations\n",
    "\n",
    "# --- Visualization Libraries ---\n",
    "import matplotlib.pyplot as plt    # Basic plotting\n",
    "import seaborn as sns              # Statistical visualization\n",
    "from PIL import Image              # Image processing (Python Imaging Library)\n",
    "\n",
    "# --- Scikit-learn: Machine Learning Utilities ---\n",
    "from sklearn.model_selection import train_test_split  # Data partitioning\n",
    "from sklearn.metrics import (\n",
    "    classification_report,         # Detailed classification report\n",
    "    confusion_matrix,              # Confusion matrix\n",
    "    accuracy_score,                # Accuracy score\n",
    "    precision_score,               # Kesinlik skoru\n",
    "    recall_score,                  # Recall score\n",
    "    f1_score,                      # F1 skoru\n",
    "    roc_curve,                     # ROC curve\n",
    "    auc,                           # AUC (Area Under Curve)\n",
    "    roc_auc_score,                 # ROC AUC skoru\n",
    "    cohen_kappa_score              # Cohen's Kappa coefficient\n",
    ")\n",
    "from sklearn.preprocessing import label_binarize      # Convert labels to binary format\n",
    "from sklearn.utils.class_weight import compute_class_weight  # Class weight computation\n",
    "\n",
    "# --- TensorFlow and Keras: Deep Learning Framework ---\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, Model, Sequential\n",
    "from tensorflow.keras.layers import (\n",
    "    Conv2D,                        # 2D Convolution layer\n",
    "    MaxPooling2D,                  # Max pooling\n",
    "    AveragePooling2D,              # Ortalama havuzlama\n",
    "    GlobalAveragePooling2D,        # Global ortalama havuzlama\n",
    "    Dense,                         # Fully connected layer\n",
    "    Dropout,                       # Dropout regularizasyon\n",
    "    Flatten,                       # Flatten layer\n",
    "    BatchNormalization,            # Batch normalizasyon\n",
    "    Input,                         # Input layer\n",
    "    Concatenate,                   # Concatenation layer\n",
    "    Add,                           # Addition layer\n",
    "    Multiply,                      # Multiplication layer\n",
    "    Activation,                    # Activation layer\n",
    "    Reshape,                       # Reshape layer\n",
    "    Lambda                         # Custom operation layer\n",
    ")\n",
    "from tensorflow.keras.optimizers import Adam, SGD, RMSprop  # Optimization algorithms\n",
    "from tensorflow.keras.callbacks import (\n",
    "    EarlyStopping,                 # Erken durdurma\n",
    "    ModelCheckpoint,               # Model checkpointing\n",
    "    ReduceLROnPlateau,             # Learning rate reduction\n",
    "    TensorBoard,                   # TensorBoard visualization\n",
    "    LearningRateScheduler          # Learning rate scheduler\n",
    ")\n",
    "from tensorflow.keras.regularizers import l2           # L2 regularizasyon\n",
    "from tensorflow.keras.preprocessing.image import (\n",
    "    ImageDataGenerator,            # Data augmentation and streaming\n",
    "    load_img,                      # Image loading\n",
    "    img_to_array                   # Image to array conversion\n",
    ")\n",
    "\n",
    "# --- Pre-trained Models for Transfer Learning ---\n",
    "from tensorflow.keras.applications import (\n",
    "    VGG16,                         # VGG16 model\n",
    "    ResNet50,                      # ResNet50 model\n",
    "    InceptionV3,                   # InceptionV3 model\n",
    "    EfficientNetB3,                # EfficientNetB3 model\n",
    "    DenseNet121,                   # DenseNet121 model\n",
    "    MobileNetV2,                   # MobileNetV2 model\n",
    "    Xception                       # Xception model\n",
    ")\n",
    "\n",
    "# --- Additional Utility Libraries ---\n",
    "from collections import Counter   # Element counting\n",
    "import cv2                         # OpenCV - Advanced image processing\n",
    "from tqdm.notebook import tqdm    # Progress bar (notebook compatible)\n",
    "import gc                          # Garbage collector (bellek yÃ¶netimi)\n",
    "from datetime import datetime      # Date and time operations\n",
    "\n",
    "# --- GPU Detection and Configuration ---\n",
    "print(\"=\" * 60)\n",
    "print(\"HARDWARE AND SOFTWARE INFORMATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# TensorFlow version\n",
    "print(f\"TensorFlow Version: {tf.__version__}\")\n",
    "\n",
    "# GPU detection\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    print(f\"ðŸ–¥ï¸  GPU Bulundu: {len(gpus)} adet\")\n",
    "    for gpu in gpus:\n",
    "        print(f\"   â€¢ {gpu.name}\")\n",
    "\n",
    "    # Enable dynamic GPU memory growth (to prevent OOM errors)\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(\"GPU memory growth set to dynamic.\")\n",
    "    except RuntimeError as e:\n",
    "        print(f\"[WARNING] GPU configuration failed: {e}\")\n",
    "else:\n",
    "    print(\"[WARNING] No GPU detected; falling back to CPU.\")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "print(f\"\\nRandom seed: {SEED}\")\n",
    "\n",
    "print(\"\\nAll libraries imported successfully.\")\n",
    "print(\"=\" * 60)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TnlyD6Pzcdx6",
    "outputId": "ce9085ab-4685-4f6b-f8fc-669133be7d10"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# =============================================================================\n",
    "# CELL 3: DEFINING CONSTANTS\n",
    "# =============================================================================\n",
    "# This cell centrally defines all constant values used throughout the project.\n",
    "# Centralizing constants facilitates modification and ensures consistency.\n",
    "# =============================================================================\n",
    "\n",
    "# --- Dataset Paths ---\n",
    "BASE_PATH = '/content/drive/MyDrive/Skin_Disease_Classification_Dataset/dataset'\n",
    "MODEL_SAVE_PATH = '/content/drive/MyDrive/Skin_Disease_Classification_Dataset/models'\n",
    "RESULTS_PATH = '/content/drive/MyDrive/Skin_Disease_Classification_Dataset/results'\n",
    "\n",
    "# Create directories if they do not exist\n",
    "os.makedirs(MODEL_SAVE_PATH, exist_ok=True)\n",
    "os.makedirs(RESULTS_PATH, exist_ok=True)\n",
    "\n",
    "# --- Class Names ---\n",
    "# 5 skin disease classes in the dataset\n",
    "CLASS_NAMES = ['acne', 'hyperpigmentation', 'Nail_psoriasis', 'SJS-TEN', 'Vitiligo']\n",
    "NUM_CLASSES = len(CLASS_NAMES)  # 5 classes\n",
    "\n",
    "# Class descriptions (for documentation)\n",
    "CLASS_NAMES_TR = {\n",
    "    'acne': 'Akne (Sivilce)',\n",
    "    'hyperpigmentation': 'Hiperpigmentasyon',\n",
    "    'Nail_psoriasis': 'Nail Psoriasis',\n",
    "    'SJS-TEN': 'Stevens-Johnson Sendromu / Toksik Epidermal Nekroliz',\n",
    "    'Vitiligo': 'Vitiligo'\n",
    "}\n",
    "\n",
    "# --- Image Parameters ---\n",
    "IMG_HEIGHT = 224          # Image height (pixels)\n",
    "IMG_WIDTH = 224           # Image width (pixels)\n",
    "IMG_SIZE = (IMG_HEIGHT, IMG_WIDTH)  # Tuple format\n",
    "IMG_SHAPE = (IMG_HEIGHT, IMG_WIDTH, 3)  # 3 kanal (RGB)\n",
    "\n",
    "# Different input size for InceptionV3 and Xception\n",
    "IMG_SIZE_INCEPTION = (299, 299)\n",
    "IMG_SHAPE_INCEPTION = (299, 299, 3)\n",
    "\n",
    "# --- Training Parameters ---\n",
    "BATCH_SIZE = 32           # Number of images processed per iteration\n",
    "EPOCHS = 100              # Maximum epoch count (controlled by early stopping)\n",
    "LEARNING_RATE = 1e-4      # Initial learning rate\n",
    "VALIDATION_SPLIT = 0.15   # Validation set ratio (15%)\n",
    "TEST_SPLIT = 0.15         # Test set ratio (15%)\n",
    "\n",
    "# --- Regularization Parameters ---\n",
    "DROPOUT_RATE = 0.5        # Dropout rate\n",
    "L2_LAMBDA = 0.001         # L2 regularization coefficient\n",
    "\n",
    "# --- Early Stopping and Callback Parameters ---\n",
    "ES_PATIENCE = 15          # Early stopping patience (epoch count)\n",
    "LR_PATIENCE = 5           # Learning rate reduction patience\n",
    "LR_FACTOR = 0.5           # Learning rate reduction factor\n",
    "MIN_LR = 1e-7             # Minimum learning rate\n",
    "\n",
    "# --- Data Augmentation Parameters ---\n",
    "ROTATION_RANGE = 30       # Rotation range (degrees)\n",
    "WIDTH_SHIFT = 0.2         # Horizontal shift ratio\n",
    "HEIGHT_SHIFT = 0.2        # Vertical shift ratio\n",
    "SHEAR_RANGE = 0.2         # Shear range\n",
    "ZOOM_RANGE = 0.2          # Zoom range\n",
    "BRIGHTNESS_RANGE = [0.8, 1.2]  # Brightness range\n",
    "\n",
    "# --- Random Seed Value ---\n",
    "SEED = 42\n",
    "\n",
    "# Print configuration\n",
    "print(\"=\" * 60)\n",
    "print(\"PROJECT CONSTANTS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Dataset Path: {BASE_PATH}\")\n",
    "print(f\"Model Save Path: {MODEL_SAVE_PATH}\")\n",
    "print(f\"Results Save Path: {RESULTS_PATH}\")\n",
    "print(f\"\\nNumber of Classes: {NUM_CLASSES}\")\n",
    "print(f\"Image Size: {IMG_SIZE}\")\n",
    "print(f\"Batch Size: {BATCH_SIZE}\")\n",
    "print(f\"Maksimum Epoch: {EPOCHS}\")\n",
    "print(f\"Learning Rate: {LEARNING_RATE}\")\n",
    "print(f\"Validation Ratio: {VALIDATION_SPLIT*100:.0f}%\")\n",
    "print(f\"Test Ratio: {TEST_SPLIT*100:.0f}%\")\n",
    "print(f\"Dropout Rate: {DROPOUT_RATE}\")\n",
    "print(f\"Early Stopping Patience: {ES_PATIENCE}\")\n",
    "print(\"=\" * 60)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9bdtx9_scgas",
    "outputId": "12b00169-1722-4244-bb3d-d31fd84aad94"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# =============================================================================\n",
    "# CELL 4: LOADING DATASET AND CREATING DATAFRAME\n",
    "# =============================================================================\n",
    "# This cell loads all image file paths and class labels into a pandas\n",
    "# DataFrame, serving as the foundation for data analysis and model training.\n",
    "# =============================================================================\n",
    "\n",
    "def load_dataset_info(base_path, class_names):\n",
    "    \"\"\"\n",
    "    Collects metadata for all images in the dataset and creates a DataFrame.\n",
    "\n",
    "    Parametreler:\n",
    "    -----------\n",
    "    base_path : str\n",
    "        Veri setinin ana dizin yolu\n",
    "    class_names : list\n",
    "        List of class names\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame\n",
    "        DataFrame containing image paths, class names, and labels\n",
    "    \"\"\"\n",
    "\n",
    "    data = []  # Veri listesi\n",
    "\n",
    "    print(\"Scanning dataset...\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    for class_idx, class_name in enumerate(class_names):\n",
    "        class_path = os.path.join(base_path, class_name)\n",
    "\n",
    "        # Check if directory exists\n",
    "        if not os.path.exists(class_path):\n",
    "            print(f\"[WARNING] {class_name} directory not found!\")\n",
    "            continue\n",
    "\n",
    "        # List images in directory\n",
    "        images = [f for f in os.listdir(class_path)\n",
    "                  if f.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp', '.gif'))]\n",
    "\n",
    "        # Record metadata for each image\n",
    "        for img_name in images:\n",
    "            img_path = os.path.join(class_path, img_name)\n",
    "            data.append({\n",
    "                'filepath': img_path,           # Tam dosya yolu\n",
    "                'filename': img_name,           # Filename\n",
    "                'class_name': class_name,       # Class name\n",
    "                'class_idx': class_idx          # Class index (0-4)\n",
    "            })\n",
    "\n",
    "        print(f\"   [OK] {class_name}: {len(images)} images loaded\")\n",
    "\n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"\\nTotal number of images: {len(df)}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "# Load dataset\n",
    "df = load_dataset_info(BASE_PATH, CLASS_NAMES)\n",
    "\n",
    "# Display first rows of the DataFrame\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"DATASET PREVIEW (First 10 Rows)\")\n",
    "print(\"=\" * 60)\n",
    "print(df.head(10))\n",
    "\n",
    "# DataFrame bilgileri\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"DATAFRAME INFORMATION\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Shape: {df.shape[0]} rows x {df.shape[1]} columns\")\n",
    "print(f\"Columns: {list(df.columns)}\")\n",
    "print(f\"Memory Usage: {df.memory_usage(deep=True).sum() / 1024:.2f} KB\")\n",
    "\n",
    "# Class distribution\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"CLASS DISTRIBUTION\")\n",
    "print(\"=\" * 60)\n",
    "class_distribution = df['class_name'].value_counts()\n",
    "for class_name, count in class_distribution.items():\n",
    "    percentage = (count / len(df)) * 100\n",
    "    print(f\"   {class_name}: {count} images ({percentage:.2f}%)\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NFEnsgp0cjdv",
    "outputId": "62b642a6-48f8-4684-b2f9-7272614b87bd"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# EDA"
   ],
   "metadata": {
    "id": "wwHm6lPFdO6H"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# =============================================================================\n",
    "# CELL 5: CLASS DISTRIBUTION VISUALIZATION\n",
    "# =============================================================================\n",
    "# This cell visualizes class distribution via bar chart and pie chart.\n",
    "# The imbalanced dataset condition is analyzed.\n",
    "# =============================================================================\n",
    "\n",
    "# Visualization style settings\n",
    "plt.style.use('seaborn-v0_8-whitegrid')  # Clean and professional appearance\n",
    "sns.set_palette(\"husl\")  # Renk paleti\n",
    "\n",
    "# Create figure with 2 side-by-side subplots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# --- Left Plot: Bar Chart ---\n",
    "# Compute class counts\n",
    "class_counts = df['class_name'].value_counts()\n",
    "\n",
    "# Create color palette (distinct color per class)\n",
    "colors = sns.color_palette(\"husl\", n_colors=NUM_CLASSES)\n",
    "\n",
    "# Draw bar chart\n",
    "bars = axes[0].bar(range(len(class_counts)), class_counts.values, color=colors, edgecolor='black', linewidth=1.2)\n",
    "\n",
    "# Annotate each bar with its value\n",
    "for i, (bar, count) in enumerate(zip(bars, class_counts.values)):\n",
    "    axes[0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 50,\n",
    "                 f'{count}', ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
    "    # Add percentage annotation\n",
    "    percentage = (count / len(df)) * 100\n",
    "    axes[0].text(bar.get_x() + bar.get_width()/2, bar.get_height()/2,\n",
    "                 f'%{percentage:.1f}', ha='center', va='center', fontsize=10,\n",
    "                 color='white', fontweight='bold')\n",
    "\n",
    "# X ekseni etiketleri\n",
    "axes[0].set_xticks(range(len(class_counts)))\n",
    "axes[0].set_xticklabels(class_counts.index, rotation=45, ha='right', fontsize=11)\n",
    "\n",
    "# Axis labels and title\n",
    "axes[0].set_xlabel('Skin Disease Class', fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylabel('Image Count', fontsize=12, fontweight='bold')\n",
    "axes[0].set_title('Image Distribution Per Class\\n(Bar Chart)', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Adjust y-axis limit for label visibility\n",
    "axes[0].set_ylim(0, max(class_counts.values) * 1.15)\n",
    "\n",
    "# Grid ekle\n",
    "axes[0].yaxis.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "# --- Right Plot: Pie Chart ---\n",
    "# Slightly explode pie slices\n",
    "explode = [0.02] * NUM_CLASSES  # Slight separation for each slice\n",
    "\n",
    "# Draw pie chart\n",
    "wedges, texts, autotexts = axes[1].pie(\n",
    "    class_counts.values,\n",
    "    labels=class_counts.index,\n",
    "    autopct='%1.1f%%',           # Percentage display\n",
    "    explode=explode,             # Slice separation\n",
    "    colors=colors,               # Renkler\n",
    "    shadow=True,                 # GÃ¶lge efekti\n",
    "    startangle=90,               # Starting angle\n",
    "    wedgeprops={'edgecolor': 'black', 'linewidth': 1}  # Edge line\n",
    ")\n",
    "\n",
    "# Style percentage labels\n",
    "for autotext in autotexts:\n",
    "    autotext.set_fontsize(11)\n",
    "    autotext.set_fontweight('bold')\n",
    "\n",
    "# Style label text\n",
    "for text in texts:\n",
    "    text.set_fontsize(10)\n",
    "\n",
    "axes[1].set_title('Class Distribution Ratios\\n(Pie Chart)', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Main title\n",
    "fig.suptitle('SKIN DISEASE DATASET - CLASS DISTRIBUTION ANALYSIS',\n",
    "             fontsize=16, fontweight='bold', y=1.02)\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save figure\n",
    "plt.savefig(f'{RESULTS_PATH}/01_sinif_dagilimi.png', dpi=300, bbox_inches='tight',\n",
    "            facecolor='white', edgecolor='none')\n",
    "print(f\"Figure saved: {RESULTS_PATH}/01_sinif_dagilimi.png\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# --- Imbalance Analysis ---\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"DATA IMBALANCE ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "max_class = class_counts.idxmax()\n",
    "min_class = class_counts.idxmin()\n",
    "max_count = class_counts.max()\n",
    "min_count = class_counts.min()\n",
    "imbalance_ratio = max_count / min_count\n",
    "\n",
    "print(f\"Majority class: {max_class} ({max_count} images)\")\n",
    "print(f\"Minority class: {min_class} ({min_count} images)\")\n",
    "print(f\"Imbalance ratio (max/min): {imbalance_ratio:.2f}x\")\n",
    "\n",
    "if imbalance_ratio > 2:\n",
    "    print(f\"\\n[WARNING] Dataset is imbalanced!\")\n",
    "    print(\"   Mitigation strategies:\")\n",
    "    print(\"   1. Class weights will be applied\")\n",
    "    print(\"   2. Additional data augmentation for minority classes\")\n",
    "    print(\"   3. Stratified sampling for data splitting\")\n",
    "else:\n",
    "    print(\"\\nDataset is relatively balanced.\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 612
    },
    "id": "Ip406qd9dQIA",
    "outputId": "3d94d22b-1677-48e4-f448-134c88fcc0d5"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# =============================================================================\n",
    "# CELL 6: IMAGE DIMENSION ANALYSIS (FULL DATASET - MULTI-THREADED)\n",
    "# =============================================================================\n",
    "# This cell analyzes dimensions of ALL images in the dataset.\n",
    "# Multi-threading is used for parallel processing to optimize execution time.\n",
    "# Full data analysis is essential for academic rigor - no data points are skipped.\n",
    "# =============================================================================\n",
    "\n",
    "import concurrent.futures\n",
    "\n",
    "def get_image_info_fast(args):\n",
    "    \"\"\"\n",
    "    Quickly reads the dimensions of a single image.\n",
    "    Uses PIL lazy loading - reads only header information,\n",
    "    without loading the full image into memory.\n",
    "\n",
    "    Parametreler:\n",
    "    -----------\n",
    "    args : tuple\n",
    "        (filepath, class_name) tuple\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    dict veya None\n",
    "        Image dimension information or None on error\n",
    "    \"\"\"\n",
    "    filepath, class_name = args\n",
    "    try:\n",
    "        with Image.open(filepath) as img:\n",
    "            width, height = img.size\n",
    "            return {\n",
    "                'width': width,\n",
    "                'height': height,\n",
    "                'aspect_ratio': width / height if height > 0 else 0,\n",
    "                'class_name': class_name\n",
    "            }\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "def analyze_all_images_parallel(df, max_workers=32):\n",
    "    \"\"\"\n",
    "    Analyzes dimensions of all images in parallel.\n",
    "\n",
    "    Parametreler:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        DataFrame containing image metadata\n",
    "    max_workers : int\n",
    "        Number of parallel threads (32 optimal for A100)\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame\n",
    "        DataFrame containing dimension statistics for all images\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"Analyzing all {len(df)} images (Multi-threading: {max_workers} workers)\")\n",
    "    print(\"This operation may take approximately 5-10 minutes...\\n\")\n",
    "\n",
    "    # Prepare arguments\n",
    "    args_list = [(row['filepath'], row['class_name']) for _, row in df.iterrows()]\n",
    "\n",
    "    # Parallel processing via multi-threading\n",
    "    dimensions = []\n",
    "    failed_count = 0\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        # Progress bar via tqdm\n",
    "        results = list(tqdm(\n",
    "            executor.map(get_image_info_fast, args_list),\n",
    "            total=len(args_list),\n",
    "            desc=\"Boyut analizi\",\n",
    "            unit=\" images\"\n",
    "        ))\n",
    "\n",
    "    # Filter results (exclude None values)\n",
    "    for result in results:\n",
    "        if result is not None:\n",
    "            dimensions.append(result)\n",
    "        else:\n",
    "            failed_count += 1\n",
    "\n",
    "    if failed_count > 0:\n",
    "        print(f\"\\n[WARNING] {failed_count} images could not be read (corrupt or inaccessible)\")\n",
    "\n",
    "    print(f\"{len(dimensions)} images analyzed successfully.\")\n",
    "\n",
    "    return pd.DataFrame(dimensions)\n",
    "\n",
    "# =============================================================================\n",
    "# RUN ANALYSIS\n",
    "# =============================================================================\n",
    "print(\"=\" * 60)\n",
    "print(\"IMAGE DIMENSION ANALYSIS (FULL DATASET)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Analyze all images\n",
    "dim_df = analyze_all_images_parallel(df, max_workers=32)\n",
    "\n",
    "# =============================================================================\n",
    "# STATISTICAL SUMMARY\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STATISTICAL SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nWIDTH Statistics:\")\n",
    "print(f\"   â€¢ Minimum  : {dim_df['width'].min()} piksel\")\n",
    "print(f\"   â€¢ Maksimum : {dim_df['width'].max()} piksel\")\n",
    "print(f\"   â€¢ Ortalama : {dim_df['width'].mean():.2f} piksel\")\n",
    "print(f\"   â€¢ Medyan   : {dim_df['width'].median():.2f} piksel\")\n",
    "print(f\"   â€¢ Std Sapma: {dim_df['width'].std():.2f} piksel\")\n",
    "\n",
    "print(f\"\\nHEIGHT Statistics:\")\n",
    "print(f\"   â€¢ Minimum  : {dim_df['height'].min()} piksel\")\n",
    "print(f\"   â€¢ Maksimum : {dim_df['height'].max()} piksel\")\n",
    "print(f\"   â€¢ Ortalama : {dim_df['height'].mean():.2f} piksel\")\n",
    "print(f\"   â€¢ Medyan   : {dim_df['height'].median():.2f} piksel\")\n",
    "print(f\"   â€¢ Std Sapma: {dim_df['height'].std():.2f} piksel\")\n",
    "\n",
    "print(f\"\\nASPECT RATIO Statistics:\")\n",
    "print(f\"   â€¢ Minimum  : {dim_df['aspect_ratio'].min():.3f}\")\n",
    "print(f\"   â€¢ Maksimum : {dim_df['aspect_ratio'].max():.3f}\")\n",
    "print(f\"   â€¢ Ortalama : {dim_df['aspect_ratio'].mean():.3f}\")\n",
    "print(f\"   â€¢ Medyan   : {dim_df['aspect_ratio'].median():.3f}\")\n",
    "\n",
    "# =============================================================================\n",
    "# VISUALIZATION\n",
    "# =============================================================================\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 14))\n",
    "\n",
    "# Renk paleti\n",
    "colors = sns.color_palette(\"husl\", n_colors=NUM_CLASSES)\n",
    "\n",
    "# --- 1. Width Distribution (Histogram) ---\n",
    "axes[0, 0].hist(dim_df['width'], bins=50, color='steelblue', edgecolor='black', alpha=0.7)\n",
    "axes[0, 0].axvline(dim_df['width'].mean(), color='red', linestyle='--', linewidth=2,\n",
    "                   label=f'Ortalama: {dim_df[\"width\"].mean():.0f} px')\n",
    "axes[0, 0].axvline(dim_df['width'].median(), color='orange', linestyle=':', linewidth=2,\n",
    "                   label=f'Medyan: {dim_df[\"width\"].median():.0f} px')\n",
    "axes[0, 0].axvline(224, color='green', linestyle='-', linewidth=2, label='Model Hedef: 224 px')\n",
    "axes[0, 0].set_xlabel('Width (pixels)', fontsize=11)\n",
    "axes[0, 0].set_ylabel('Frekans', fontsize=11)\n",
    "axes[0, 0].set_title('Image Width Distribution', fontsize=13, fontweight='bold')\n",
    "axes[0, 0].legend(loc='upper right', fontsize=9)\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# --- 2. Height Distribution (Histogram) ---\n",
    "axes[0, 1].hist(dim_df['height'], bins=50, color='coral', edgecolor='black', alpha=0.7)\n",
    "axes[0, 1].axvline(dim_df['height'].mean(), color='red', linestyle='--', linewidth=2,\n",
    "                   label=f'Ortalama: {dim_df[\"height\"].mean():.0f} px')\n",
    "axes[0, 1].axvline(dim_df['height'].median(), color='orange', linestyle=':', linewidth=2,\n",
    "                   label=f'Medyan: {dim_df[\"height\"].median():.0f} px')\n",
    "axes[0, 1].axvline(224, color='green', linestyle='-', linewidth=2, label='Model Hedef: 224 px')\n",
    "axes[0, 1].set_xlabel('Height (pixels)', fontsize=11)\n",
    "axes[0, 1].set_ylabel('Frekans', fontsize=11)\n",
    "axes[0, 1].set_title('Image Height Distribution', fontsize=13, fontweight='bold')\n",
    "axes[0, 1].legend(loc='upper right', fontsize=9)\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# --- 3. Width vs Height (Scatter Plot - Per Class) ---\n",
    "for i, class_name in enumerate(CLASS_NAMES):\n",
    "    class_data = dim_df[dim_df['class_name'] == class_name]\n",
    "    axes[1, 0].scatter(class_data['width'], class_data['height'],\n",
    "                       alpha=0.5, s=15, label=class_name, color=colors[i])\n",
    "\n",
    "# 1:1 aspect ratio line\n",
    "max_dim = max(dim_df['width'].max(), dim_df['height'].max())\n",
    "axes[1, 0].plot([0, max_dim], [0, max_dim], 'r--', linewidth=2, label='1:1 Ratio (Square)')\n",
    "\n",
    "# Target size marker\n",
    "axes[1, 0].scatter([224], [224], color='black', s=200, marker='*',\n",
    "                   zorder=5, label='Hedef: 224Ã—224')\n",
    "\n",
    "axes[1, 0].set_xlabel('Width (pixels)', fontsize=11)\n",
    "axes[1, 0].set_ylabel('Height (pixels)', fontsize=11)\n",
    "axes[1, 0].set_title('Width vs Height (Per Class)', fontsize=13, fontweight='bold')\n",
    "axes[1, 0].legend(loc='upper left', fontsize=8, ncol=2)\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# --- 4. Per-Class Dimension Distribution (Box Plot) ---\n",
    "dim_df['avg_size'] = (dim_df['width'] + dim_df['height']) / 2\n",
    "box_plot = sns.boxplot(data=dim_df, x='class_name', y='avg_size', palette='husl', ax=axes[1, 1])\n",
    "axes[1, 1].axhline(224, color='green', linestyle='--', linewidth=2, label='Hedef: 224 px')\n",
    "axes[1, 1].set_xlabel('Skin Disease Class', fontsize=11)\n",
    "axes[1, 1].set_ylabel('Ortalama Boyut (piksel)', fontsize=11)\n",
    "axes[1, 1].set_title('Per-Class Image Dimension Distribution', fontsize=13, fontweight='bold')\n",
    "axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "axes[1, 1].legend(loc='upper right', fontsize=9)\n",
    "axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Main title\n",
    "fig.suptitle('IMAGE DIMENSION ANALYSIS - FULL DATASET (n=3411)',\n",
    "             fontsize=15, fontweight='bold', y=1.02)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save figure\n",
    "save_path = f'{RESULTS_PATH}/02_goruntu_boyutlari.png'\n",
    "plt.savefig(save_path, dpi=300, bbox_inches='tight', facecolor='white', edgecolor='none')\n",
    "print(f\"\\nFigure saved: {save_path}\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# =============================================================================\n",
    "# DETAILED STATISTICS\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"PER-CLASS DIMENSION STATISTICS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "class_stats = dim_df.groupby('class_name').agg({\n",
    "    'width': ['min', 'max', 'mean', 'std'],\n",
    "    'height': ['min', 'max', 'mean', 'std'],\n",
    "    'aspect_ratio': ['min', 'max', 'mean']\n",
    "}).round(2)\n",
    "\n",
    "print(class_stats)\n",
    "\n",
    "# =============================================================================\n",
    "# UNIQUE DIMENSIONS\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"UNIQUE IMAGE DIMENSIONS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "unique_sizes = dim_df.groupby(['width', 'height']).size().reset_index(name='count')\n",
    "unique_sizes = unique_sizes.sort_values('count', ascending=False)\n",
    "print(f\"Total unique dimension combinations: {len(unique_sizes)}\")\n",
    "print(f\"\\nTop 15 most common dimensions:\")\n",
    "print(unique_sizes.head(15).to_string(index=False))\n",
    "\n",
    "# =============================================================================\n",
    "# OUTLIER ANALYSIS\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"OUTLIER ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Outlier detection via IQR method\n",
    "Q1_w = dim_df['width'].quantile(0.25)\n",
    "Q3_w = dim_df['width'].quantile(0.75)\n",
    "IQR_w = Q3_w - Q1_w\n",
    "\n",
    "Q1_h = dim_df['height'].quantile(0.25)\n",
    "Q3_h = dim_df['height'].quantile(0.75)\n",
    "IQR_h = Q3_h - Q1_h\n",
    "\n",
    "outliers_width = dim_df[(dim_df['width'] < Q1_w - 1.5*IQR_w) | (dim_df['width'] > Q3_w + 1.5*IQR_w)]\n",
    "outliers_height = dim_df[(dim_df['height'] < Q1_h - 1.5*IQR_h) | (dim_df['height'] > Q3_h + 1.5*IQR_h)]\n",
    "\n",
    "print(f\"Width outlier count: {len(outliers_width)} ({len(outliers_width)/len(dim_df)*100:.2f}%)\")\n",
    "print(f\"Height outlier count: {len(outliers_height)} ({len(outliers_height)/len(dim_df)*100:.2f}%)\")\n",
    "\n",
    "# Very small and very large images\n",
    "very_small = dim_df[(dim_df['width'] < 100) | (dim_df['height'] < 100)]\n",
    "very_large = dim_df[(dim_df['width'] > 1000) | (dim_df['height'] > 1000)]\n",
    "\n",
    "print(f\"\\nVery small images (<100px): {len(very_small)}\")\n",
    "print(f\"Very large images (>1000px): {len(very_large)}\")\n",
    "\n",
    "# =============================================================================\n",
    "# RECOMMENDATIONS FOR MODEL TRAINING\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"RECOMMENDATIONS FOR MODEL TRAINING\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"[OK] All images will be resized to 224x224\")\n",
    "print(f\"Ortalama orijinal boyut: {dim_df['width'].mean():.0f}Ã—{dim_df['height'].mean():.0f}\")\n",
    "print(f\"[OK] High dimensional variability - resizing is necessary\")\n",
    "print(f\"[OK] Bilinear/bicubic interpolation will be used to preserve image quality\")\n",
    "print(f\"[OK] Data augmentation will increase size variation coverage\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "40d2d259f921456fbefc08c78c8d2c3d",
      "44c5d7854a064edd96e694d5e0211b07",
      "8a20384affb14b9e8923b573f4f61125",
      "2a7c13b9686747ed82fe3d1a914bddb5",
      "732c3847b9414176a0d6bca79c9f2d16",
      "c7024ce4e78c458bac4357ee968798cf",
      "c8d994c4eb974eabae23b9d289054e46",
      "ae2d6bcc424b4f71ac1d55bf7f6b6392",
      "370e9c382e784cbcb128cb6c9e5ca934",
      "dea9e1822ad648c2b0ed6336c8ce7769",
      "d58935b8eb34424faaf65802f4d8c13e"
     ]
    },
    "id": "3zAVRpNWdSu8",
    "outputId": "a06b72f0-d476-402e-d99d-0edf3625142d"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# =============================================================================\n",
    "# CELL 7: SAMPLE IMAGE VISUALIZATION PER CLASS\n",
    "# =============================================================================\n",
    "# This cell displays randomly selected sample images from each skin disease class.\n",
    "# This is an essential EDA step for understanding dataset content.\n",
    "# =============================================================================\n",
    "\n",
    "def display_sample_images(df, class_names, samples_per_class=5, figsize=(20, 15)):\n",
    "    \"\"\"\n",
    "    Displays random sample images from each class.\n",
    "\n",
    "    Parametreler:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        DataFrame containing image metadata\n",
    "    class_names : list\n",
    "        List of class names\n",
    "    samples_per_class : int\n",
    "        Number of samples to display per class\n",
    "    figsize : tuple\n",
    "        Figure boyutu\n",
    "    \"\"\"\n",
    "\n",
    "    num_classes = len(class_names)\n",
    "\n",
    "    # Create figure\n",
    "    fig, axes = plt.subplots(num_classes, samples_per_class, figsize=figsize)\n",
    "\n",
    "    # Select and display sample images for each class\n",
    "    for row_idx, class_name in enumerate(class_names):\n",
    "        # Filter images belonging to this class\n",
    "        class_df = df[df['class_name'] == class_name]\n",
    "\n",
    "        # Select random samples\n",
    "        samples = class_df.sample(n=min(samples_per_class, len(class_df)), random_state=SEED)\n",
    "\n",
    "        for col_idx, (_, sample) in enumerate(samples.iterrows()):\n",
    "            ax = axes[row_idx, col_idx]\n",
    "\n",
    "            try:\n",
    "                # Load image\n",
    "                img = Image.open(sample['filepath'])\n",
    "\n",
    "                # Display image\n",
    "                ax.imshow(img)\n",
    "\n",
    "                # Write class name in first column\n",
    "                if col_idx == 0:\n",
    "                    ax.set_ylabel(f\"{class_name}\\n({CLASS_NAMES_TR[class_name]})\",\n",
    "                                  fontsize=10, fontweight='bold', rotation=0,\n",
    "                                  ha='right', va='center', labelpad=60)\n",
    "\n",
    "                # Add image dimensions as subtitle\n",
    "                ax.set_title(f'{img.size[0]}Ã—{img.size[1]}', fontsize=9)\n",
    "\n",
    "            except Exception as e:\n",
    "                ax.text(0.5, 0.5, f'Load failed\\n{e}', ha='center', va='center',\n",
    "                       transform=ax.transAxes, fontsize=8)\n",
    "\n",
    "            # Eksenleri kapat\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "\n",
    "    # Main title\n",
    "    fig.suptitle('SAMPLE IMAGES FROM SKIN DISEASE CLASSES',\n",
    "                 fontsize=16, fontweight='bold', y=1.02)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "# Display sample images\n",
    "print(\"=\" * 60)\n",
    "print(\"SAMPLE IMAGES\")\n",
    "print(\"=\" * 60)\n",
    "print(\"Displaying 5 random sample images per class...\\n\")\n",
    "\n",
    "fig = display_sample_images(df, CLASS_NAMES, samples_per_class=5, figsize=(18, 14))\n",
    "\n",
    "# Save figure\n",
    "plt.savefig(f'{RESULTS_PATH}/03_ornek_goruntuler.png', dpi=300, bbox_inches='tight',\n",
    "            facecolor='white', edgecolor='none')\n",
    "print(f\"Figure saved: {RESULTS_PATH}/03_ornek_goruntuler.png\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# --- Disease Descriptions ---\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"SKIN DISEASE INFORMATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "disease_info = {\n",
    "    'acne': \"\"\"\n",
    "    ACNE: A common skin condition resulting from hair follicles becoming clogged\n",
    "    with oil and dead skin cells. Typically appears on the face, forehead, chest,\n",
    "    back, and shoulders.\"\"\",\n",
    "\n",
    "    'hyperpigmentation': \"\"\"\n",
    "    HYPERPIGMENTATION: Dark-colored patches caused by excess melanin production\n",
    "    in specific areas of the skin. Can result from sun damage, hormonal changes,\n",
    "    or skin injuries.\"\"\",\n",
    "\n",
    "    'Nail_psoriasis': \"\"\"\n",
    "    NAIL PSORIASIS: A form of psoriasis affecting the nails.\n",
    "    Symptoms include pitting, discoloration, thickening, and separation of\n",
    "    belirtilere neden olur.\"\"\",\n",
    "\n",
    "    'SJS-TEN': \"\"\"\n",
    "    STEVENS-JOHNSON SYNDROME / TOXIC EPIDERMAL NECROLYSIS: A severe and\n",
    "    potentially fatal skin reaction, usually drug-induced.\n",
    "    Characterized by rash, blistering, and skin peeling.\"\"\",\n",
    "\n",
    "    'Vitiligo': \"\"\"\n",
    "    VITILIGO: A pigmentation disorder characterized by white patches\n",
    "    resulting from the destruction of melanin-producing cells (melanocytes)\n",
    "    in the skin.\"\"\"\n",
    "}\n",
    "\n",
    "for disease, info in disease_info.items():\n",
    "    print(f\"\\nðŸ”¬ {CLASS_NAMES_TR[disease].upper()}\")\n",
    "    print(\"-\" * 40)\n",
    "    print(info.strip())"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "GfDkz9MTdpAs",
    "outputId": "4dcb12db-7cfe-4993-a077-1577e585c3a9"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# =============================================================================\n",
    "# CELL 8: COLOR DISTRIBUTION ANALYSIS (RGB HISTOGRAM)\n",
    "# =============================================================================\n",
    "# This cell analyzes the RGB channel distributions for images in each class.\n",
    "# Understanding the color characteristics of different skin diseases provides\n",
    "# valuable insights for model performance.\n",
    "# =============================================================================\n",
    "\n",
    "def analyze_color_distribution(df, class_names, sample_size=100):\n",
    "    \"\"\"\n",
    "    Analyzes RGB color distribution for each class.\n",
    "\n",
    "    Parametreler:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        DataFrame containing image metadata\n",
    "    class_names : list\n",
    "        Class names\n",
    "    sample_size : int\n",
    "        Number of samples to analyze per class\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Mean RGB values per class\n",
    "    \"\"\"\n",
    "\n",
    "    color_stats = {cls: {'R': [], 'G': [], 'B': []} for cls in class_names}\n",
    "\n",
    "    print(\"Performing color analysis...\")\n",
    "\n",
    "    for class_name in tqdm(class_names, desc=\"Classes\"):\n",
    "        # Get images for this class\n",
    "        class_df = df[df['class_name'] == class_name]\n",
    "\n",
    "        # Select random samples\n",
    "        samples = class_df.sample(n=min(sample_size, len(class_df)), random_state=SEED)\n",
    "\n",
    "        for _, row in samples.iterrows():\n",
    "            try:\n",
    "                # Load image and convert to numpy array\n",
    "                img = Image.open(row['filepath']).convert('RGB')\n",
    "                img_array = np.array(img)\n",
    "\n",
    "                # Compute mean value for each channel\n",
    "                color_stats[class_name]['R'].extend(img_array[:, :, 0].flatten())\n",
    "                color_stats[class_name]['G'].extend(img_array[:, :, 1].flatten())\n",
    "                color_stats[class_name]['B'].extend(img_array[:, :, 2].flatten())\n",
    "\n",
    "            except Exception as e:\n",
    "                continue\n",
    "\n",
    "    return color_stats\n",
    "\n",
    "# Run color analysis\n",
    "print(\"=\" * 60)\n",
    "print(\"RGB COLOR DISTRIBUTION ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "color_stats = analyze_color_distribution(df, CLASS_NAMES, sample_size=100)\n",
    "\n",
    "# --- Visualization ---\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# RGB histogram per class\n",
    "for idx, class_name in enumerate(CLASS_NAMES):\n",
    "    ax = axes[idx]\n",
    "\n",
    "    # Draw RGB histograms\n",
    "    ax.hist(color_stats[class_name]['R'], bins=50, alpha=0.6, color='red',\n",
    "            label='Red', density=True)\n",
    "    ax.hist(color_stats[class_name]['G'], bins=50, alpha=0.6, color='green',\n",
    "            label='Green', density=True)\n",
    "    ax.hist(color_stats[class_name]['B'], bins=50, alpha=0.6, color='blue',\n",
    "            label='Blue', density=True)\n",
    "\n",
    "    # Compute and display mean values\n",
    "    r_mean = np.mean(color_stats[class_name]['R'])\n",
    "    g_mean = np.mean(color_stats[class_name]['G'])\n",
    "    b_mean = np.mean(color_stats[class_name]['B'])\n",
    "\n",
    "    ax.axvline(r_mean, color='darkred', linestyle='--', linewidth=2)\n",
    "    ax.axvline(g_mean, color='darkgreen', linestyle='--', linewidth=2)\n",
    "    ax.axvline(b_mean, color='darkblue', linestyle='--', linewidth=2)\n",
    "\n",
    "    ax.set_xlabel('Pixel Value (0-255)', fontsize=10)\n",
    "    ax.set_ylabel('Density', fontsize=10)\n",
    "    ax.set_title(f'{class_name}\\nR:{r_mean:.1f} | G:{g_mean:.1f} | B:{b_mean:.1f}',\n",
    "                 fontsize=11, fontweight='bold')\n",
    "    ax.legend(loc='upper right', fontsize=9)\n",
    "    ax.set_xlim(0, 255)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Use last subplot for cross-class comparison\n",
    "ax = axes[5]\n",
    "\n",
    "# Compute mean RGB values per class\n",
    "class_means = []\n",
    "for class_name in CLASS_NAMES:\n",
    "    r_mean = np.mean(color_stats[class_name]['R'])\n",
    "    g_mean = np.mean(color_stats[class_name]['G'])\n",
    "    b_mean = np.mean(color_stats[class_name]['B'])\n",
    "    class_means.append([r_mean, g_mean, b_mean])\n",
    "\n",
    "class_means = np.array(class_means)\n",
    "\n",
    "# Grouped bar chart\n",
    "x = np.arange(len(CLASS_NAMES))\n",
    "width = 0.25\n",
    "\n",
    "bars1 = ax.bar(x - width, class_means[:, 0], width, label='Red', color='red', alpha=0.7)\n",
    "bars2 = ax.bar(x, class_means[:, 1], width, label='Green', color='green', alpha=0.7)\n",
    "bars3 = ax.bar(x + width, class_means[:, 2], width, label='Blue', color='blue', alpha=0.7)\n",
    "\n",
    "ax.set_xlabel('Class', fontsize=10)\n",
    "ax.set_ylabel('Mean Pixel Value', fontsize=10)\n",
    "ax.set_title('Per-Class Mean RGB Values', fontsize=11, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(CLASS_NAMES, rotation=45, ha='right', fontsize=9)\n",
    "ax.legend(loc='upper right', fontsize=9)\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Main title\n",
    "fig.suptitle('SKIN DISEASE CLASS COLOR DISTRIBUTION ANALYSIS',\n",
    "             fontsize=14, fontweight='bold', y=1.02)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save figure\n",
    "plt.savefig(f'{RESULTS_PATH}/04_renk_dagilimi.png', dpi=300, bbox_inches='tight',\n",
    "            facecolor='white', edgecolor='none')\n",
    "print(f\"\\nFigure saved: {RESULTS_PATH}/04_renk_dagilimi.png\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# --- Color Statistics Table ---\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"PER-CLASS MEAN RGB VALUES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "color_table = []\n",
    "for idx, class_name in enumerate(CLASS_NAMES):\n",
    "    r_mean = np.mean(color_stats[class_name]['R'])\n",
    "    g_mean = np.mean(color_stats[class_name]['G'])\n",
    "    b_mean = np.mean(color_stats[class_name]['B'])\n",
    "    brightness = (r_mean + g_mean + b_mean) / 3\n",
    "    color_table.append({\n",
    "        'Class': class_name,\n",
    "        'R (Red)': f'{r_mean:.2f}',\n",
    "        'G (Green)': f'{g_mean:.2f}',\n",
    "        'B (Mavi)': f'{b_mean:.2f}',\n",
    "        'Brightness': f'{brightness:.2f}'\n",
    "    })\n",
    "\n",
    "color_df = pd.DataFrame(color_table)\n",
    "print(color_df.to_string(index=False))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 862,
     "referenced_widgets": [
      "b689353e27e04174b9f39cabee8f472c",
      "a53cc150408f4339a93d8453120b7f65",
      "99a757a83b4443b9802a083dfcd2a20f",
      "2e8987d6071e49e6a055d1caf99798e2",
      "1e7d71e81c3b4e7496fc2d8317416bac",
      "de6d16d4fd414ceaa645b05b40e31ec6",
      "c8cdc853c18b45e6ba0565bb646fdd1e",
      "82b3bb195e1f4f078c39220420768fbc",
      "8a3c45d0d4c14b0c9ccc3da436f35826",
      "550d6e2cb82345a2a1015a6ec9535e6c",
      "8ff9160291254d9592c8303db2b6e20f"
     ]
    },
    "id": "yB9Umz31d4W9",
    "outputId": "88a12188-ba69-456e-b2c2-5209e5973aa6"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Cell 9: Train/Validation/Test Split"
   ],
   "metadata": {
    "id": "HJa3v2rTk0Ca"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# =============================================================================\n",
    "# CELL 9: TRAIN/VALIDATION/TEST SPLIT\n",
    "# =============================================================================\n",
    "# This cell splits the dataset into three partitions via stratified sampling:\n",
    "# - Training Set (70%): For model training\n",
    "# - Validation Set (15%): For hyperparameter tuning and early stopping\n",
    "# - Test Set (15%): For final model evaluation\n",
    "#\n",
    "# Stratified sampling preserves class proportions across all partitions.\n",
    "# This is critically important for imbalanced datasets.\n",
    "# =============================================================================\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def create_stratified_split(df, test_size=0.15, val_size=0.15, random_state=42):\n",
    "    \"\"\"\n",
    "    Splits the dataset into train/validation/test partitions using stratified sampling.\n",
    "\n",
    "    Parametreler:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        DataFrame containing the full dataset\n",
    "    test_size : float\n",
    "        Test set ratio (default: 0.15 = 15%)\n",
    "    val_size : float\n",
    "        Validation set ratio (default: 0.15 = 15%)\n",
    "    random_state : int\n",
    "        Random seed for reproducibility\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    tuple\n",
    "        (train_df, val_df, test_df) - Three separate DataFrames\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"=\" * 60)\n",
    "    print(\"DATASET PARTITIONING (STRATIFIED SPLIT)\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # First, separate the test set\n",
    "    # The stratify parameter preserves class proportions\n",
    "    train_val_df, test_df = train_test_split(\n",
    "        df,\n",
    "        test_size=test_size,\n",
    "        stratify=df['class_name'],  # Preserve class distribution\n",
    "        random_state=random_state\n",
    "    )\n",
    "\n",
    "    # Separate validation set from remaining data\n",
    "    # Recalculate val_size (relative to train_val)\n",
    "    val_ratio = val_size / (1 - test_size)\n",
    "\n",
    "    train_df, val_df = train_test_split(\n",
    "        train_val_df,\n",
    "        test_size=val_ratio,\n",
    "        stratify=train_val_df['class_name'],\n",
    "        random_state=random_state\n",
    "    )\n",
    "\n",
    "    # Reset DataFrame indices\n",
    "    train_df = train_df.reset_index(drop=True)\n",
    "    val_df = val_df.reset_index(drop=True)\n",
    "    test_df = test_df.reset_index(drop=True)\n",
    "\n",
    "    return train_df, val_df, test_df\n",
    "\n",
    "# Split the dataset\n",
    "train_df, val_df, test_df = create_stratified_split(\n",
    "    df,\n",
    "    test_size=TEST_SPLIT,\n",
    "    val_size=VALIDATION_SPLIT,\n",
    "    random_state=SEED\n",
    ")\n",
    "\n",
    "# =============================================================================\n",
    "# DISPLAY PARTITIONING RESULTS\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"\\nPARTITIONING RESULTS\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'Set':<15} {'Image Count':<20} {'Ratio':<15}\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'Training':<15} {len(train_df):<20} {len(train_df)/len(df)*100:.1f}%\")\n",
    "print(f\"{'Validation':<15} {len(val_df):<20} {len(val_df)/len(df)*100:.1f}%\")\n",
    "print(f\"{'Test':<15} {len(test_df):<20} {len(test_df)/len(df)*100:.1f}%\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'TOPLAM':<15} {len(train_df)+len(val_df)+len(test_df):<20} 100.0%\")\n",
    "\n",
    "# =============================================================================\n",
    "# VERIFY CLASS DISTRIBUTIONS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"CLASS DISTRIBUTION VERIFICATION (Stratified Sampling Validation)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Compute class distribution for each set\n",
    "def get_class_distribution(dataframe, set_name):\n",
    "    \"\"\"Computes class distribution within a DataFrame.\"\"\"\n",
    "    dist = dataframe['class_name'].value_counts()\n",
    "    percentages = (dist / len(dataframe) * 100).round(2)\n",
    "    return percentages\n",
    "\n",
    "train_dist = get_class_distribution(train_df, \"Training\")\n",
    "val_dist = get_class_distribution(val_df, \"Validation\")\n",
    "test_dist = get_class_distribution(test_df, \"Test\")\n",
    "original_dist = get_class_distribution(df, \"Original\")\n",
    "\n",
    "# Build comparison table\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Original (%)': original_dist,\n",
    "    'Train (%)': train_dist,\n",
    "    'Validation (%)': val_dist,\n",
    "    'Test (%)': test_dist\n",
    "})\n",
    "\n",
    "print(\"\\nClass Distribution Comparison:\")\n",
    "print(comparison_df.to_string())\n",
    "\n",
    "# Verify distribution consistency\n",
    "max_deviation = 0\n",
    "for class_name in CLASS_NAMES:\n",
    "    orig = original_dist[class_name]\n",
    "    train_pct = train_dist[class_name]\n",
    "    val_pct = val_dist[class_name]\n",
    "    test_pct = test_dist[class_name]\n",
    "\n",
    "    deviation = max(abs(orig - train_pct), abs(orig - val_pct), abs(orig - test_pct))\n",
    "    max_deviation = max(max_deviation, deviation)\n",
    "\n",
    "print(f\"\\nMaksimum sapma: {max_deviation:.2f}%\")\n",
    "if max_deviation < 1.0:\n",
    "    print(\"   Stratified sampling successful - distributions are consistent.\")\n",
    "else:\n",
    "    print(\"   [WARNING] Minor deviations detected (expected behavior).\")\n",
    "\n",
    "# =============================================================================\n",
    "# VISUALIZATION\n",
    "# =============================================================================\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "sets = [('Training Set', train_df), ('Validation Set', val_df), ('Test Set', test_df)]\n",
    "colors = sns.color_palette(\"husl\", n_colors=NUM_CLASSES)\n",
    "\n",
    "for idx, (set_name, set_df) in enumerate(sets):\n",
    "    class_counts = set_df['class_name'].value_counts()\n",
    "\n",
    "    # Draw bar chart preserving class order\n",
    "    counts = [class_counts.get(cls, 0) for cls in CLASS_NAMES]\n",
    "\n",
    "    bars = axes[idx].bar(range(len(CLASS_NAMES)), counts, color=colors, edgecolor='black')\n",
    "\n",
    "    # Annotate values above bars\n",
    "    for bar, count in zip(bars, counts):\n",
    "        axes[idx].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 5,\n",
    "                       str(count), ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "    axes[idx].set_xticks(range(len(CLASS_NAMES)))\n",
    "    axes[idx].set_xticklabels(CLASS_NAMES, rotation=45, ha='right', fontsize=9)\n",
    "    axes[idx].set_ylabel('Image Count', fontsize=11)\n",
    "    axes[idx].set_title(f'{set_name}\\n(n={len(set_df)})', fontsize=12, fontweight='bold')\n",
    "    axes[idx].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "fig.suptitle('DATASET PARTITIONING - CLASS DISTRIBUTIONS', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save figure\n",
    "plt.savefig(f'{RESULTS_PATH}/05_veri_bolumlemesi.png', dpi=300, bbox_inches='tight',\n",
    "            facecolor='white', edgecolor='none')\n",
    "print(f\"\\nFigure saved: {RESULTS_PATH}/05_veri_bolumlemesi.png\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# =============================================================================\n",
    "# DETAILED STATISTICS PER SET\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"DETAILED SET STATISTICS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for set_name, set_df in [('Training', train_df), ('Validation', val_df), ('Test', test_df)]:\n",
    "    print(f\"\\n{set_name} Set:\")\n",
    "    for class_name in CLASS_NAMES:\n",
    "        count = len(set_df[set_df['class_name'] == class_name])\n",
    "        pct = count / len(set_df) * 100\n",
    "        print(f\"   - {class_name}: {count} images ({pct:.2f}%)\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "y6ujfDonk0sT",
    "outputId": "764c29cb-f021-4198-bb4a-1989c2b81d36"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Cell 10: Class Weights Computation (Imbalanced Data)"
   ],
   "metadata": {
    "id": "TBEivU77k4BU"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# =============================================================================\n",
    "# CELL 10: CLASS WEIGHTS COMPUTATION (FOR IMBALANCED DATA)\n",
    "# =============================================================================\n",
    "# In imbalanced datasets, class weights ensure adequate learning of minority classes.\n",
    "# These weights adjust each class's contribution to the loss function.\n",
    "# Formula: weight_i = n_samples / (n_classes * n_samples_i)\n",
    "#\n",
    "# This formula assigns higher weights to underrepresented classes.\n",
    "\n",
    "# =============================================================================\n",
    "\n",
    "def calculate_class_weights(train_df, class_names):\n",
    "    \"\"\"\n",
    "    Computes class weights for the training set.\n",
    "\n",
    "    Parametreler:\n",
    "    -----------\n",
    "    train_df : pandas.DataFrame\n",
    "        Training veri seti\n",
    "    class_names : list\n",
    "        List of class names\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Class index -> weight mapping\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"=\" * 60)\n",
    "    print(\"CLASS WEIGHTS HESAPLAMA\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Extract class labels\n",
    "    y_train = train_df['class_idx'].values\n",
    "\n",
    "    # Compute class weights via sklearn\n",
    "    # 'balanced' modu: n_samples / (n_classes * np.bincount(y))\n",
    "    class_weights_array = compute_class_weight(\n",
    "        class_weight='balanced',\n",
    "        classes=np.unique(y_train),\n",
    "        y=y_train\n",
    "    )\n",
    "\n",
    "    # Convert to dictionary format\n",
    "    class_weights_dict = {i: weight for i, weight in enumerate(class_weights_array)}\n",
    "\n",
    "    # Print results\n",
    "    print(\"\\nComputed Class Weights:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"{'Class Index':<15} {'Class Name':<20} {'Weight':<15}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    for idx, class_name in enumerate(class_names):\n",
    "        weight = class_weights_dict[idx]\n",
    "        sample_count = len(train_df[train_df['class_idx'] == idx])\n",
    "        print(f\"{idx:<15} {class_name:<20} {weight:.4f}\")\n",
    "\n",
    "    return class_weights_dict\n",
    "\n",
    "# Compute class weights\n",
    "class_weights = calculate_class_weights(train_df, CLASS_NAMES)\n",
    "\n",
    "# =============================================================================\n",
    "# VISUALIZATION\n",
    "# =============================================================================\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: Sample count\n",
    "sample_counts = [len(train_df[train_df['class_name'] == cls]) for cls in CLASS_NAMES]\n",
    "colors = sns.color_palette(\"husl\", n_colors=NUM_CLASSES)\n",
    "\n",
    "bars1 = axes[0].bar(range(len(CLASS_NAMES)), sample_counts, color=colors, edgecolor='black')\n",
    "for bar, count in zip(bars1, sample_counts):\n",
    "    axes[0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 20,\n",
    "                 str(count), ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "axes[0].set_xticks(range(len(CLASS_NAMES)))\n",
    "axes[0].set_xticklabels(CLASS_NAMES, rotation=45, ha='right', fontsize=10)\n",
    "axes[0].set_ylabel('Sample Count', fontsize=11)\n",
    "axes[0].set_title('Training Set - Class Distribution', fontsize=12, fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Right: Class weights\n",
    "weights = [class_weights[i] for i in range(len(CLASS_NAMES))]\n",
    "bars2 = axes[1].bar(range(len(CLASS_NAMES)), weights, color=colors, edgecolor='black')\n",
    "for bar, weight in zip(bars2, weights):\n",
    "    axes[1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02,\n",
    "                 f'{weight:.3f}', ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "axes[1].set_xticks(range(len(CLASS_NAMES)))\n",
    "axes[1].set_xticklabels(CLASS_NAMES, rotation=45, ha='right', fontsize=10)\n",
    "axes[1].set_ylabel('Class Weight', fontsize=11)\n",
    "axes[1].set_title('Hesaplanan Class Weights', fontsize=12, fontweight='bold')\n",
    "axes[1].axhline(y=1.0, color='red', linestyle='--', linewidth=2, label='Baseline (1.0)')\n",
    "axes[1].legend(loc='upper right')\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "fig.suptitle('IMBALANCED DATA MANAGEMENT - CLASS WEIGHTS', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save figure\n",
    "plt.savefig(f'{RESULTS_PATH}/06_class_weights.png', dpi=300, bbox_inches='tight',\n",
    "            facecolor='white', edgecolor='none')\n",
    "print(f\"\\nFigure saved: {RESULTS_PATH}/06_class_weights.png\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# =============================================================================\n",
    "# EXPLANATION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"CLASS WEIGHTS EXPLANATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\"\"\n",
    "How Do Class Weights Work?\n",
    "\n",
    "1. OBJECTIVE: Ensure adequate learning of minority classes in imbalanced datasets\n",
    "\n",
    "2. FORMULA: weight = n_samples / (n_classes x n_samples_in_class)\n",
    "   - Fewer samples -> Higher weight\n",
    "   - More samples -> Lower weight\n",
    "\n",
    "3. EFFECT:\n",
    "   - Each sample's loss contribution is multiplied by its weight\n",
    "   - Errors on minority classes are penalized more heavily\n",
    "   - The model learns all classes in a balanced manner\n",
    "\n",
    "4. UYGULAMA:\n",
    "   - Used via model.fit(..., class_weight=class_weights)\n",
    "   - Applied automatically at each epoch\n",
    "\"\"\")\n",
    "\n",
    "# Weight interpretation\n",
    "max_weight_idx = np.argmax(weights)\n",
    "min_weight_idx = np.argmin(weights)\n",
    "\n",
    "print(f\"\\nYORUM:\")\n",
    "print(f\"   - Highest weight: {CLASS_NAMES[max_weight_idx]} ({weights[max_weight_idx]:.3f})\")\n",
    "print(f\"     -> The least represented class receives the highest weight\")\n",
    "print(f\"   - Lowest weight: {CLASS_NAMES[min_weight_idx]} ({weights[min_weight_idx]:.3f})\")\n",
    "print(f\"     -> The most represented class receives the lowest weight\")\n",
    "print(f\"   - Weight ratio: {weights[max_weight_idx]/weights[min_weight_idx]:.2f}x\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "55rPQXTUk7Ad",
    "outputId": "c8285e25-c919-40fc-fdd4-d1a01393b5a9"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Cell 11: Data Generator Construction (Data Augmentation)"
   ],
   "metadata": {
    "id": "_xNLq1XBlJNl"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# =============================================================================\n",
    "# CELL 11: TF.DATA PIPELINE (PREPROCESSING HANDLED INSIDE MODEL)\n",
    "# =============================================================================\n",
    "# IMPORTANT CHANGE:\n",
    "# - Normalizasyon (rescaling) KALDIRILDI\n",
    "# - Each model will use its own preprocessing\n",
    "# - This ensures EfficientNet, ResNet, VGG all receive correctly preprocessed inputs\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"TF.DATA PIPELINE (PREPROCESSING INSIDE MODEL)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "BATCH_SIZE = 64  # Reduced from 256 to 64 (more stable training)\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "SHUFFLE_BUFFER = 1000\n",
    "\n",
    "print(f\"\\nâš¡ Ayarlar:\")\n",
    "print(f\"   â€¢ Batch Size: {BATCH_SIZE}\")\n",
    "print(f\"   â€¢ Shuffle Buffer: {SHUFFLE_BUFFER}\")\n",
    "print(f\"   - Preprocessing: INSIDE MODEL (model-specific)\")\n",
    "\n",
    "# =============================================================================\n",
    "# CLASS MAPPING\n",
    "# =============================================================================\n",
    "\n",
    "class_names_sorted = sorted(df['class_name'].unique().tolist())\n",
    "class_indices = {name: idx for idx, name in enumerate(class_names_sorted)}\n",
    "idx_to_class = {idx: name for name, idx in class_indices.items()}\n",
    "class_names_ordered = class_names_sorted\n",
    "\n",
    "print(f\"\\nClass Mapping:\")\n",
    "for name, idx in class_indices.items():\n",
    "    print(f\"   {name} â†’ {idx}\")\n",
    "\n",
    "# =============================================================================\n",
    "# DATA AUGMENTATION (GPU TABANLI)\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"DATA AUGMENTATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "data_augmentation = tf.keras.Sequential([\n",
    "    tf.keras.layers.RandomFlip(\"horizontal\"),\n",
    "    tf.keras.layers.RandomRotation(0.1),\n",
    "    tf.keras.layers.RandomZoom(0.1),\n",
    "], name=\"data_augmentation\")\n",
    "\n",
    "print(\"Augmentation: RandomFlip, RandomRotation(Â±10%), RandomZoom(Â±10%)\")\n",
    "\n",
    "# =============================================================================\n",
    "# IMAGE LOADING (NO PREPROCESSING)\n",
    "# =============================================================================\n",
    "\n",
    "def load_image_no_preprocess(filepath, label):\n",
    "    \"\"\"\n",
    "    Loads and resizes the image.\n",
    "    NO PREPROCESSING - handled inside the model.\n",
    "    \"\"\"\n",
    "    image = tf.io.read_file(filepath)\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "    image = tf.image.resize(image, IMG_SIZE)\n",
    "    # NOTE: No normalization applied. Images remain in [0, 255] range.\n",
    "    return image, label\n",
    "\n",
    "# =============================================================================\n",
    "# DATASET CONSTRUCTION\n",
    "# =============================================================================\n",
    "\n",
    "def create_dataset_no_preprocess(filepaths, labels, is_training=False):\n",
    "    \"\"\"\n",
    "    Creates dataset WITHOUT preprocessing.\n",
    "    Her model kendi preprocessing'ini yapacak.\n",
    "    \"\"\"\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((filepaths, labels))\n",
    "\n",
    "    if is_training:\n",
    "        dataset = dataset.shuffle(buffer_size=SHUFFLE_BUFFER, seed=SEED)\n",
    "\n",
    "    dataset = dataset.map(load_image_no_preprocess, num_parallel_calls=AUTOTUNE)\n",
    "\n",
    "    # Augmentation applied to training set only\n",
    "    if is_training:\n",
    "        dataset = dataset.map(\n",
    "            lambda x, y: (data_augmentation(x, training=True), y),\n",
    "            num_parallel_calls=AUTOTUNE\n",
    "        )\n",
    "\n",
    "    dataset = dataset.cache()\n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "    dataset = dataset.prefetch(AUTOTUNE)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "# =============================================================================\n",
    "# DATASETLER\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"CONSTRUCTING DATASETS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "def prepare_labels(dataframe):\n",
    "    labels = dataframe['class_name'].map(class_indices).values\n",
    "    return tf.keras.utils.to_categorical(labels, num_classes=NUM_CLASSES)\n",
    "\n",
    "print(\"\\nTraining Dataset...\")\n",
    "train_filepaths = train_df['filepath'].values\n",
    "train_labels = prepare_labels(train_df)\n",
    "train_dataset = create_dataset_no_preprocess(train_filepaths, train_labels, is_training=True)\n",
    "print(\"   [OK] Preprocessing: None (handled inside model)\")\n",
    "print(\"   [OK] Augmentation: ENABLED\")\n",
    "\n",
    "print(\"\\nValidation Dataset...\")\n",
    "val_filepaths = val_df['filepath'].values\n",
    "val_labels = prepare_labels(val_df)\n",
    "val_dataset = create_dataset_no_preprocess(val_filepaths, val_labels, is_training=False)\n",
    "print(\"   [OK] Preprocessing: None (handled inside model)\")\n",
    "print(\"   Augmentation: KAPALI\")\n",
    "\n",
    "print(\"\\nTest Dataset...\")\n",
    "test_filepaths = test_df['filepath'].values\n",
    "test_labels = prepare_labels(test_df)\n",
    "test_dataset = create_dataset_no_preprocess(test_filepaths, test_labels, is_training=False)\n",
    "print(\"   [OK] Preprocessing: None (handled inside model)\")\n",
    "print(\"   Augmentation: KAPALI\")\n",
    "\n",
    "# =============================================================================\n",
    "# INFORMATION\n",
    "# =============================================================================\n",
    "\n",
    "steps_per_epoch = int(np.ceil(len(train_df) / BATCH_SIZE))\n",
    "validation_steps = int(np.ceil(len(val_df) / BATCH_SIZE))\n",
    "test_steps = int(np.ceil(len(test_df) / BATCH_SIZE))\n",
    "\n",
    "print(f\"\\nDataset Bilgileri:\")\n",
    "print(f\"   - Training: {len(train_df)} images, {steps_per_epoch} steps\")\n",
    "print(f\"   - Validation: {len(val_df)} images, {validation_steps} steps\")\n",
    "print(f\"   - Test: {len(test_df)} images, {test_steps} steps\")\n",
    "print(f\"   â€¢ Batch Size: {BATCH_SIZE}\")\n",
    "\n",
    "# Visual inspection\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"IMAGE INSPECTION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for images, labels in train_dataset.take(1):\n",
    "    print(f\"   â€¢ Image shape: {images.shape}\")\n",
    "    print(f\"   â€¢ Image dtype: {images.dtype}\")\n",
    "    print(f\"   â€¢ Image min: {tf.reduce_min(images).numpy():.2f}\")\n",
    "    print(f\"   â€¢ Image max: {tf.reduce_max(images).numpy():.2f}\")\n",
    "    print(f\"   â€¢ Label shape: {labels.shape}\")\n",
    "\n",
    "print(\"\\ntf.data pipeline ready.\")\n",
    "print(\"\\n[IMPORTANT] Each model must include its own preprocessing layer.\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1saKSc1PlL7h",
    "outputId": "e614011a-9012-483b-f412-14957502a02c"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Cell 12: Class Weights Correction and Callback Definition\n",
    ""
   ],
   "metadata": {
    "id": "wq5DHCqglgF4"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# =============================================================================\n",
    "# CELL 12: CLASS WEIGHTS AND CALLBACK FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"CLASS WEIGHTS HESAPLAMA\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Class distribution in training set\n",
    "class_counts = train_df['class_name'].value_counts()\n",
    "total_samples = len(train_df)\n",
    "n_classes = NUM_CLASSES\n",
    "\n",
    "# Compute class weights\n",
    "class_weights = {}\n",
    "\n",
    "print(\"\\nClass Weights:\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'Index':<8} {'Class':<20} {'Samples':<10} {'Weight':<10}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for class_name, idx in class_indices.items():\n",
    "    count = class_counts[class_name]\n",
    "    weight = total_samples / (n_classes * count)\n",
    "    class_weights[idx] = weight\n",
    "    print(f\"{idx:<8} {class_name:<20} {count:<10} {weight:.4f}\")\n",
    "\n",
    "print(\"-\" * 60)\n",
    "\n",
    "max_weight_idx = max(class_weights, key=class_weights.get)\n",
    "min_weight_idx = min(class_weights, key=class_weights.get)\n",
    "\n",
    "print(f\"\\nAnalysis:\")\n",
    "print(f\"   - Highest weight: {idx_to_class[max_weight_idx]} ({class_weights[max_weight_idx]:.4f})\")\n",
    "print(f\"   - Lowest weight: {idx_to_class[min_weight_idx]} ({class_weights[min_weight_idx]:.4f})\")\n",
    "print(f\"   - Imbalance ratio: {class_weights[max_weight_idx]/class_weights[min_weight_idx]:.2f}x\")\n",
    "\n",
    "# =============================================================================\n",
    "# CALLBACK FUNCTION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"CALLBACK FUNCTIONS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "def create_callbacks(model_name):\n",
    "    \"\"\"Creates callbacks for model training.\"\"\"\n",
    "    callbacks = [\n",
    "        EarlyStopping(\n",
    "            monitor='val_accuracy',\n",
    "            patience=15,\n",
    "            restore_best_weights=True,\n",
    "            mode='max',\n",
    "            verbose=1\n",
    "        ),\n",
    "        ModelCheckpoint(\n",
    "            filepath=f'{MODEL_SAVE_PATH}/{model_name}_best.keras',\n",
    "            monitor='val_accuracy',\n",
    "            save_best_only=True,\n",
    "            mode='max',\n",
    "            verbose=1\n",
    "        ),\n",
    "        ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=5,\n",
    "            min_lr=1e-7,\n",
    "            verbose=1\n",
    "        )\n",
    "    ]\n",
    "    return callbacks\n",
    "\n",
    "print(\"\"\"\n",
    "Callback'ler:\n",
    "   â€¢ EarlyStopping: val_accuracy, patience=15, mode=max\n",
    "   â€¢ ModelCheckpoint: val_accuracy, en iyi modeli kaydet\n",
    "   â€¢ ReduceLROnPlateau: val_loss, factor=0.5, patience=5\n",
    "\"\"\")\n",
    "\n",
    "# =============================================================================\n",
    "# RESULTS LIST\n",
    "# =============================================================================\n",
    "\n",
    "all_results = []\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"Preparation complete - ready for model training.\")\n",
    "print(\"=\" * 70)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nSK8xn58lgiQ",
    "outputId": "6fe5125e-3345-4d33-f1a2-95f207b3c5cf"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# DATA LEAKAGE VERIFICATION"
   ],
   "metadata": {
    "id": "xkHHIl9Uh3aJ"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# =============================================================================\n",
    "# DATA LEAKAGE VERIFICATION\n",
    "# =============================================================================\n",
    "# Run this cell once to verify results.\n",
    "# If \"NO DATA LEAKAGE\" is reported, proceed.\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"DATA LEAKAGE VERIFICATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Convert file paths to sets\n",
    "train_files = set(train_df['filepath'].values)\n",
    "val_files = set(val_df['filepath'].values)\n",
    "test_files = set(test_df['filepath'].values)\n",
    "\n",
    "# Check intersections\n",
    "train_val_overlap = train_files.intersection(val_files)\n",
    "train_test_overlap = train_files.intersection(test_files)\n",
    "val_test_overlap = val_files.intersection(test_files)\n",
    "\n",
    "print(f\"\\nIntersection Check:\")\n",
    "print(f\"   â€¢ Train âˆ© Validation : {len(train_val_overlap)} dosya\")\n",
    "print(f\"   â€¢ Train âˆ© Test       : {len(train_test_overlap)} dosya\")\n",
    "print(f\"   â€¢ Validation âˆ© Test  : {len(val_test_overlap)} dosya\")\n",
    "\n",
    "# Result\n",
    "if len(train_val_overlap) == 0 and len(train_test_overlap) == 0 and len(val_test_overlap) == 0:\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"DATA LEAKAGE YOK!\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"   All datasets are completely disjoint. Results are reliable.\")\n",
    "else:\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"[ERROR] DATA LEAKAGE DETECTED!\")\n",
    "    print(\"=\" * 70)\n",
    "    if len(train_val_overlap) > 0:\n",
    "        print(f\"   Train-Validation ortak dosyalar: {len(train_val_overlap)}\")\n",
    "    if len(train_test_overlap) > 0:\n",
    "        print(f\"   Train-Test ortak dosyalar: {len(train_test_overlap)}\")\n",
    "    if len(val_test_overlap) > 0:\n",
    "        print(f\"   Validation-Test ortak dosyalar: {len(val_test_overlap)}\")\n",
    "\n",
    "# Summary table\n",
    "print(f\"\\nDataset Summary:\")\n",
    "print(f\"   â€¢ Training   : {len(train_files):,} dosya\")\n",
    "print(f\"   â€¢ Validation : {len(val_files):,} dosya\")\n",
    "print(f\"   â€¢ Test       : {len(test_files):,} dosya\")\n",
    "print(f\"   - Total     : {len(train_files) + len(val_files) + len(test_files):,} files\")\n",
    "print(f\"   â€¢ Orijinal   : {len(df):,} dosya\")\n",
    "\n",
    "# Validation\n",
    "if len(train_files) + len(val_files) + len(test_files) == len(df):\n",
    "    print(\"\\nTotal file count is consistent.\")\n",
    "else:\n",
    "    print(\"\\n[WARNING] Total file count mismatch!\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TI54-OvJhu6b",
    "outputId": "9b5337f5-bef0-4937-d996-513f4c420808"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# MODELS"
   ],
   "metadata": {
    "id": "aBQgndqUo48J"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, BatchNormalization, Activation, MaxPooling2D, Dropout, GlobalAveragePooling2D, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, cohen_kappa_score, classification_report, confusion_matrix, roc_curve, auc\n",
    "from sklearn.preprocessing import label_binarize\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import gc"
   ],
   "metadata": {
    "id": "yhwow-qQ7Dwl"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Cell 13: MODEL 1 - CUSTOM CNN"
   ],
   "metadata": {
    "id": "mkiUj0wmo6k3"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# =============================================================================\n",
    "# CELL 13: MODEL 1 - CUSTOM CNN (DESIGNED FROM SCRATCH)\n",
    "# =============================================================================\n",
    "# This model is a custom CNN architecture designed from scratch without transfer learning.\n",
    "# It fulfills the project requirement for an originally developed model.\n",
    "#\n",
    "# Architectural Features:\n",
    "# - 4 Convolutional Block (32 â†’ 64 â†’ 128 â†’ 256 filtre)\n",
    "# - Batch Normalization (training stability)\n",
    "# - He Normal initialization (optimized for ReLU)\n",
    "# - L2 Regularization (overfitting prevention)\n",
    "# - Global Average Pooling (parameter reduction)\n",
    "# - Dropout (generalization)\n",
    "#\n",
    "# A100 GPU Optimizasyonu:\n",
    "# - Batch Size: 256\n",
    "# - Fast data feeding via tf.data pipeline\n",
    "# - Mixed precision support\n",
    "# =============================================================================\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "MODEL_NAME = \"Model_01_Custom_CNN\"\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"MODEL 1: CUSTOM CNN (SIFIRDAN TASARIM)\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Model Name: {MODEL_NAME}\")\n",
    "print(f\"Batch Size: {BATCH_SIZE}\")\n",
    "print(f\"Epochs: {EPOCHS}\")\n",
    "\n",
    "# =============================================================================\n",
    "# MODEL ARCHITECTURE\n",
    "# =============================================================================\n",
    "\n",
    "def build_custom_cnn(input_shape, num_classes):\n",
    "    \"\"\"\n",
    "    Custom CNN architecture for skin disease classification.\n",
    "\n",
    "    Architecture:\n",
    "    - 4 Convolutional Block\n",
    "    - Her blokta: Conv â†’ BN â†’ ReLU â†’ Conv â†’ BN â†’ ReLU â†’ MaxPool â†’ Dropout\n",
    "    - Global Average Pooling\n",
    "    - Classification via Dense layers\n",
    "\n",
    "    Args:\n",
    "        input_shape: Input dimensions (224, 224, 3)\n",
    "        num_classes: Number of classes (5)\n",
    "\n",
    "    Returns:\n",
    "        tf.keras.Model\n",
    "    \"\"\"\n",
    "\n",
    "    model = Sequential(name='Custom_CNN')\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # BLOCK 1: 32 Filters - Low-level feature extraction\n",
    "    # -------------------------------------------------------------------------\n",
    "    model.add(Conv2D(32, (3, 3), padding='same', kernel_initializer='he_normal',\n",
    "                     kernel_regularizer=l2(1e-4), input_shape=input_shape))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(32, (3, 3), padding='same', kernel_initializer='he_normal',\n",
    "                     kernel_regularizer=l2(1e-4)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # BLOCK 2: 64 Filters - Mid-level features\n",
    "    # -------------------------------------------------------------------------\n",
    "    model.add(Conv2D(64, (3, 3), padding='same', kernel_initializer='he_normal',\n",
    "                     kernel_regularizer=l2(1e-4)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(64, (3, 3), padding='same', kernel_initializer='he_normal',\n",
    "                     kernel_regularizer=l2(1e-4)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # BLOCK 3: 128 Filters - High-level features\n",
    "    # -------------------------------------------------------------------------\n",
    "    model.add(Conv2D(128, (3, 3), padding='same', kernel_initializer='he_normal',\n",
    "                     kernel_regularizer=l2(1e-4)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(128, (3, 3), padding='same', kernel_initializer='he_normal',\n",
    "                     kernel_regularizer=l2(1e-4)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # BLOCK 4: 256 Filters - Deep features\n",
    "    # -------------------------------------------------------------------------\n",
    "    model.add(Conv2D(256, (3, 3), padding='same', kernel_initializer='he_normal',\n",
    "                     kernel_regularizer=l2(1e-4)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(256, (3, 3), padding='same', kernel_initializer='he_normal',\n",
    "                     kernel_regularizer=l2(1e-4)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # CLASSIFICATION HEAD\n",
    "    # -------------------------------------------------------------------------\n",
    "    model.add(GlobalAveragePooling2D())\n",
    "    model.add(Dense(256, kernel_initializer='he_normal', kernel_regularizer=l2(1e-4)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    return model\n",
    "\n",
    "# Build model\n",
    "print(\"\\nBuilding model...\")\n",
    "model_1 = build_custom_cnn(IMG_SHAPE, NUM_CLASSES)\n",
    "\n",
    "# Model summary\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"MODEL ARCHITECTURE\")\n",
    "print(\"=\" * 70)\n",
    "model_1.summary()\n",
    "\n",
    "total_params = model_1.count_params()\n",
    "trainable_params = np.sum([np.prod(v.shape) for v in model_1.trainable_weights])\n",
    "print(f\"\\nParameter Information:\")\n",
    "print(f\"   - Total: {total_params:,}\")\n",
    "print(f\"   - Trainable: {trainable_params:,}\")\n",
    "\n",
    "# =============================================================================\n",
    "# MODEL COMPILATION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"MODEL DERLEME\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "LEARNING_RATE_M1 = 1e-3  # Slightly higher LR for Custom CNN\n",
    "\n",
    "optimizer = Adam(learning_rate=LEARNING_RATE_M1)\n",
    "\n",
    "model_1.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(f\"Optimizer: Adam\")\n",
    "print(f\"Learning Rate: {LEARNING_RATE_M1}\")\n",
    "print(f\"Loss: Categorical Crossentropy\")\n",
    "print(f\"Metrics: Accuracy\")\n",
    "\n",
    "# =============================================================================\n",
    "# CALLBACKS\n",
    "# =============================================================================\n",
    "\n",
    "callbacks_m1 = create_callbacks(MODEL_NAME)\n",
    "print(f\"\\n[OK] Callbacks created: EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\")\n",
    "\n",
    "# =============================================================================\n",
    "# MODEL TRAINING\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"MODEL TRAINING STARTING\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\"\"\n",
    "Training Parameters:\n",
    "   â€¢ Model: {MODEL_NAME}\n",
    "   â€¢ Epochs: {EPOCHS}\n",
    "   â€¢ Batch Size: {BATCH_SIZE}\n",
    "   â€¢ Learning Rate: {LEARNING_RATE_M1}\n",
    "   â€¢ Training Samples: {len(train_df)}\n",
    "   â€¢ Validation Samples: {len(val_df)}\n",
    "   â€¢ Steps per Epoch: {steps_per_epoch}\n",
    "   â€¢ Class Weights: Aktif\n",
    "\"\"\")\n",
    "\n",
    "# Measure training time\n",
    "start_time = time.time()\n",
    "\n",
    "history_1 = model_1.fit(\n",
    "    train_dataset,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=val_dataset,\n",
    "    callbacks=callbacks_m1,\n",
    "    class_weight=class_weights,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\n{'=' * 70}\")\n",
    "print(f\"Training complete.\")\n",
    "print(f\"   - Duration: {training_time/60:.2f} minutes\")\n",
    "print(f\"   â€¢ Tamamlanan Epoch: {len(history_1.history['loss'])}\")\n",
    "print(f\"{'=' * 70}\")\n",
    "\n",
    "# =============================================================================\n",
    "# TRAINING CURVES\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"TRAINING CURVES\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Accuracy plot\n",
    "axes[0].plot(history_1.history['accuracy'], 'b-', label='Training Accuracy', linewidth=2)\n",
    "axes[0].plot(history_1.history['val_accuracy'], 'r-', label='Validation Accuracy', linewidth=2)\n",
    "axes[0].set_title(f'{MODEL_NAME}\\nModel Accuracy', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xlabel('Epoch', fontsize=11)\n",
    "axes[0].set_ylabel('Accuracy', fontsize=11)\n",
    "axes[0].legend(loc='lower right', fontsize=10)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].set_ylim([0, 1])\n",
    "\n",
    "# Loss plot\n",
    "axes[1].plot(history_1.history['loss'], 'b-', label='Training Loss', linewidth=2)\n",
    "axes[1].plot(history_1.history['val_loss'], 'r-', label='Validation Loss', linewidth=2)\n",
    "axes[1].set_title(f'{MODEL_NAME}\\nModel Loss', fontsize=12, fontweight='bold')\n",
    "axes[1].set_xlabel('Epoch', fontsize=11)\n",
    "axes[1].set_ylabel('Loss', fontsize=11)\n",
    "axes[1].legend(loc='upper right', fontsize=10)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "history_path = f\"{RESULTS_PATH}/{MODEL_NAME}_history.png\"\n",
    "plt.savefig(history_path, dpi=300, bbox_inches='tight', facecolor='white')\n",
    "plt.show()\n",
    "print(f\"Figure saved: {history_path}\")\n",
    "\n",
    "# =============================================================================\n",
    "# MODEL EVALUATION (TEST SET)\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"MODEL EVALUATION (TEST SET)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Generate predictions on test set\n",
    "print(\"\\nGenerating predictions on test set...\")\n",
    "y_pred_proba = model_1.predict(test_dataset, verbose=1)\n",
    "y_pred = np.argmax(y_pred_proba, axis=1)\n",
    "\n",
    "# Extract ground-truth labels\n",
    "y_true = []\n",
    "for _, labels in test_dataset:\n",
    "    y_true.extend(np.argmax(labels.numpy(), axis=1))\n",
    "y_true = np.array(y_true)\n",
    "\n",
    "# =============================================================================\n",
    "# PERFORMANCE METRICS\n",
    "# =============================================================================\n",
    "\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "precision = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "recall = recall_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "f1 = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "kappa = cohen_kappa_score(y_true, y_pred)\n",
    "\n",
    "print(f\"\\nPERFORMANCE METRICS:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"   Accuracy  : {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "print(f\"   Precision : {precision:.4f}\")\n",
    "print(f\"   Recall    : {recall:.4f}\")\n",
    "print(f\"   F1-Score  : {f1:.4f}\")\n",
    "print(f\"   Kappa     : {kappa:.4f}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Classification Report\n",
    "print(f\"\\nPER-CLASS PERFORMANCE:\")\n",
    "print(\"=\" * 70)\n",
    "print(classification_report(y_true, y_pred, target_names=class_names_ordered, zero_division=0))\n",
    "\n",
    "# =============================================================================\n",
    "# CONFUSION MATRIX\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"CONFUSION MATRIX\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=class_names_ordered,\n",
    "            yticklabels=class_names_ordered,\n",
    "            annot_kws={'size': 14, 'fontweight': 'bold'})\n",
    "plt.title(f'{MODEL_NAME}\\nConfusion Matrix', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Predicted Class', fontsize=12)\n",
    "plt.ylabel('True Class', fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right', fontsize=10)\n",
    "plt.yticks(rotation=0, fontsize=10)\n",
    "plt.tight_layout()\n",
    "\n",
    "cm_path = f\"{RESULTS_PATH}/{MODEL_NAME}_confusion_matrix.png\"\n",
    "plt.savefig(cm_path, dpi=300, bbox_inches='tight', facecolor='white')\n",
    "plt.show()\n",
    "print(f\"Figure saved: {cm_path}\")\n",
    "\n",
    "# =============================================================================\n",
    "# ROC CURVES (BUG-FIXED)\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"ROC CURVES\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# y_true is already a numpy array (converted above)\n",
    "n_classes = NUM_CLASSES\n",
    "\n",
    "# One-hot encoding for ROC\n",
    "y_true_bin = label_binarize(y_true, classes=range(n_classes))\n",
    "\n",
    "# Compute ROC for each class\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_true_bin[:, i], y_pred_proba[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# Micro-average ROC\n",
    "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_true_bin.ravel(), y_pred_proba.ravel())\n",
    "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "# Macro-average ROC\n",
    "all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "mean_tpr = np.zeros_like(all_fpr)\n",
    "for i in range(n_classes):\n",
    "    mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])\n",
    "mean_tpr /= n_classes\n",
    "fpr[\"macro\"] = all_fpr\n",
    "tpr[\"macro\"] = mean_tpr\n",
    "roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "# ROC Plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Renk paleti\n",
    "colors = ['#e41a1c', '#377eb8', '#4daf4a', '#984ea3', '#ff7f00']\n",
    "\n",
    "# ROC curve per class\n",
    "for i, (class_name, color) in enumerate(zip(class_names_ordered, colors)):\n",
    "    plt.plot(fpr[i], tpr[i], color=color, lw=2,\n",
    "             label=f'{class_name} (AUC = {roc_auc[i]:.3f})')\n",
    "\n",
    "# Micro ve Macro average\n",
    "plt.plot(fpr[\"micro\"], tpr[\"micro\"], color='deeppink', linestyle=':', lw=3,\n",
    "         label=f'Micro-average (AUC = {roc_auc[\"micro\"]:.3f})')\n",
    "plt.plot(fpr[\"macro\"], tpr[\"macro\"], color='navy', linestyle=':', lw=3,\n",
    "         label=f'Macro-average (AUC = {roc_auc[\"macro\"]:.3f})')\n",
    "\n",
    "# Reference diagonal\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=2, label='Random Classifier (AUC = 0.500)')\n",
    "\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate (1 - Specificity)', fontsize=12)\n",
    "plt.ylabel('True Positive Rate (Sensitivity)', fontsize=12)\n",
    "plt.title(f'{MODEL_NAME}\\nROC Curves', fontsize=14, fontweight='bold')\n",
    "plt.legend(loc='lower right', fontsize=9)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "\n",
    "roc_path = f\"{RESULTS_PATH}/{MODEL_NAME}_roc_curves.png\"\n",
    "plt.savefig(roc_path, dpi=300, bbox_inches='tight', facecolor='white')\n",
    "plt.show()\n",
    "print(f\"Figure saved: {roc_path}\")\n",
    "\n",
    "# Print ROC AUC values\n",
    "print(f\"\\nROC AUC Values:\")\n",
    "print(\"-\" * 40)\n",
    "for i, class_name in enumerate(class_names_ordered):\n",
    "    print(f\"   {class_name}: {roc_auc[i]:.4f}\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"   Macro-average: {roc_auc['macro']:.4f}\")\n",
    "print(f\"   Micro-average: {roc_auc['micro']:.4f}\")\n",
    "\n",
    "# =============================================================================\n",
    "# SAVE RESULTS\n",
    "# =============================================================================\n",
    "\n",
    "results_1 = {\n",
    "    'model_name': MODEL_NAME,\n",
    "    'accuracy': accuracy,\n",
    "    'precision': precision,\n",
    "    'recall': recall,\n",
    "    'f1_score': f1,\n",
    "    'kappa': kappa,\n",
    "    'roc_auc_macro': roc_auc['macro'],\n",
    "    'roc_auc_micro': roc_auc['micro'],\n",
    "    'training_time_min': training_time / 60,\n",
    "    'total_epochs': len(history_1.history['loss']),\n",
    "    'total_params': total_params\n",
    "}\n",
    "\n",
    "# Append AUC per class\n",
    "for i, class_name in enumerate(class_names_ordered):\n",
    "    results_1[f'auc_{class_name}'] = roc_auc[i]\n",
    "\n",
    "all_results.append(results_1)\n",
    "\n",
    "# =============================================================================\n",
    "# SAVE MODEL\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"MODEL KAYDETME\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Final model\n",
    "final_model_path = f\"{MODEL_SAVE_PATH}/{MODEL_NAME}_final.keras\"\n",
    "model_1.save(final_model_path)\n",
    "print(f\"Model kaydedildi: {final_model_path}\")\n",
    "\n",
    "# Training history\n",
    "history_df = pd.DataFrame(history_1.history)\n",
    "history_csv_path = f\"{RESULTS_PATH}/{MODEL_NAME}_training_history.csv\"\n",
    "history_df.to_csv(history_csv_path, index=False)\n",
    "print(f\"[OK] Training history saved: {history_csv_path}\")\n",
    "\n",
    "# =============================================================================\n",
    "# MODEL 1 SUMMARY REPORT\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"MODEL 1 - SUMMARY REPORT\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\"\"\n",
    "MODEL INFORMATION\n",
    "{'â”€' * 50}\n",
    "   Model Name       : {MODEL_NAME}\n",
    "   Model Type       : Custom CNN (Designed from Scratch)\n",
    "   Toplam Parametre : {total_params:,}\n",
    "\n",
    "ARCHITECTURE\n",
    "{'â”€' * 50}\n",
    "   â€¢ 4 Convolutional Block (32â†’64â†’128â†’256)\n",
    "   â€¢ Batch Normalization + ReLU\n",
    "   â€¢ MaxPooling2D + Dropout\n",
    "   â€¢ Global Average Pooling\n",
    "   â€¢ Dense(256) + Softmax(5)\n",
    "\n",
    "TRAINING CONFIGURATION\n",
    "{'â”€' * 50}\n",
    "   â€¢ Optimizer       : Adam (lr={LEARNING_RATE_M1})\n",
    "   â€¢ Loss            : Categorical Crossentropy\n",
    "   â€¢ Batch Size      : {BATCH_SIZE}\n",
    "   â€¢ Tamamlanan Epoch: {results_1['total_epochs']}\n",
    "   â€¢ Training Duration: {results_1['training_time_min']:.2f} minutes\n",
    "\n",
    "TEST RESULTS\n",
    "{'â”€' * 50}\n",
    "   â€¢ Accuracy        : {results_1['accuracy']*100:.2f}%\n",
    "   â€¢ Precision       : {results_1['precision']:.4f}\n",
    "   â€¢ Recall          : {results_1['recall']:.4f}\n",
    "   â€¢ F1-Score        : {results_1['f1_score']:.4f}\n",
    "   â€¢ Cohen's Kappa   : {results_1['kappa']:.4f}\n",
    "   â€¢ ROC AUC (Macro) : {results_1['roc_auc_macro']:.4f}\n",
    "   â€¢ ROC AUC (Micro) : {results_1['roc_auc_micro']:.4f}\n",
    "\n",
    "SAVED FILES\n",
    "{'â”€' * 50}\n",
    "   â€¢ {final_model_path}\n",
    "   â€¢ {history_csv_path}\n",
    "   â€¢ {history_path}\n",
    "   â€¢ {cm_path}\n",
    "   â€¢ {roc_path}\n",
    "\"\"\")\n",
    "\n",
    "# =============================================================================\n",
    "# MEMORY CLEANUP\n",
    "# =============================================================================\n",
    "\n",
    "# Keep Model 1 in memory (for comparison)\n",
    "# Only clean unnecessary variables\n",
    "gc.collect()\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"MODEL 1 (CUSTOM CNN) TAMAMLANDI!\")\n",
    "print(\"=\" * 70)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "CgKZgFUHo8qI",
    "outputId": "be196e06-55dc-4433-c4fe-8d18b9b02774"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Cell 14: MODEL 2 - EfficientNetV2-S (TRANSFER LEARNING)"
   ],
   "metadata": {
    "id": "2O4oZs7j_nHl"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# =============================================================================\n",
    "# CELL 14: MODEL 2 - EfficientNetV2-S (CORRECTED)\n",
    "# =============================================================================\n",
    "# CRITICAL FIX:\n",
    "# - EfficientNetV2 includes its own preprocessing (include_preprocessing=True)\n",
    "# - Images enter as [0, 255] and are transformed inside the model\n",
    "# =============================================================================\n",
    "\n",
    "import time\n",
    "from tensorflow.keras.applications import EfficientNetV2S\n",
    "from tensorflow.keras.layers import (Input, GlobalAveragePooling2D, Dense,\n",
    "                                      Dropout, BatchNormalization, Activation)\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "MODEL_NAME = \"Model_02_EfficientNetV2S\"\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"MODEL 2: EfficientNetV2-S (CORRECTED)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# =============================================================================\n",
    "# MODEL ARCHITECTURE\n",
    "# =============================================================================\n",
    "\n",
    "def build_efficientnet_v2(input_shape, num_classes):\n",
    "    \"\"\"\n",
    "    EfficientNetV2-S modeli.\n",
    "\n",
    "    IMPORTANT: With include_preprocessing=True, the model handles its own preprocessing.\n",
    "    Input: [0, 255] -> Inside model: [-1, 1]\n",
    "    \"\"\"\n",
    "\n",
    "    inputs = Input(shape=input_shape, name='input')\n",
    "\n",
    "    # Base model - PREPROCESSING INCLUDED\n",
    "    base_model = EfficientNetV2S(\n",
    "        include_top=False,\n",
    "        weights='imagenet',\n",
    "        input_tensor=inputs,\n",
    "        include_preprocessing=True  # CRITICAL: Handles its own preprocessing!\n",
    "    )\n",
    "\n",
    "    # Freeze base model\n",
    "    base_model.trainable = False\n",
    "\n",
    "    # Classification head\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D(name='gap')(x)\n",
    "    x = BatchNormalization(name='bn')(x)\n",
    "    x = Dense(256, kernel_regularizer=l2(1e-4), name='dense')(x)\n",
    "    x = Activation('relu', name='relu')(x)\n",
    "    x = Dropout(0.5, name='dropout')(x)\n",
    "    outputs = Dense(num_classes, activation='softmax', name='predictions')(x)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=outputs, name='EfficientNetV2S')\n",
    "\n",
    "    return model, base_model\n",
    "\n",
    "# Build model\n",
    "print(\"\\nBuilding model...\")\n",
    "model_2, base_model_2 = build_efficientnet_v2(IMG_SHAPE, NUM_CLASSES)\n",
    "\n",
    "# Parameter information\n",
    "total_params = model_2.count_params()\n",
    "trainable_params = sum([tf.keras.backend.count_params(w) for w in model_2.trainable_weights])\n",
    "non_trainable_params = total_params - trainable_params\n",
    "\n",
    "print(f\"\\nParameter Information:\")\n",
    "print(f\"   - Total: {total_params:,}\")\n",
    "print(f\"   - Trainable: {trainable_params:,}\")\n",
    "print(f\"   - Frozen: {non_trainable_params:,}\")\n",
    "print(f\"\\n[OK] include_preprocessing=True -> Model handles its own preprocessing\")\n",
    "\n",
    "# =============================================================================\n",
    "# PHASE 1: FEATURE EXTRACTION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"PHASE 1: FEATURE EXTRACTION (BASE FROZEN)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "model_2.compile(\n",
    "    optimizer=Adam(learning_rate=1e-3),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "callbacks_p1 = [\n",
    "    EarlyStopping(monitor='val_accuracy', patience=10, restore_best_weights=True, mode='max', verbose=1),\n",
    "    ModelCheckpoint(f'{MODEL_SAVE_PATH}/{MODEL_NAME}_p1_best.keras', monitor='val_accuracy',\n",
    "                    save_best_only=True, mode='max', verbose=1),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-7, verbose=1)\n",
    "]\n",
    "\n",
    "print(\"Phase 1 starting (20 epochs)...\")\n",
    "start_p1 = time.time()\n",
    "\n",
    "history_p1 = model_2.fit(\n",
    "    train_dataset,\n",
    "    epochs=20,\n",
    "    validation_data=val_dataset,\n",
    "    callbacks=callbacks_p1,\n",
    "    class_weight=class_weights,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "time_p1 = time.time() - start_p1\n",
    "print(f\"\\nPhase 1: {time_p1/60:.2f} minutes\")\n",
    "\n",
    "# =============================================================================\n",
    "# PHASE 2: FINE-TUNING\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"PHASE 2: FINE-TUNING\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Unfreeze last 50 layers\n",
    "base_model_2.trainable = True\n",
    "fine_tune_at = len(base_model_2.layers) - 50\n",
    "\n",
    "for layer in base_model_2.layers[:fine_tune_at]:\n",
    "    layer.trainable = False\n",
    "\n",
    "trainable_count = sum([1 for l in model_2.layers if l.trainable])\n",
    "print(f\"Last {len(base_model_2.layers) - fine_tune_at} layers unfrozen\")\n",
    "\n",
    "# Very low LR\n",
    "model_2.compile(\n",
    "    optimizer=Adam(learning_rate=1e-5),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "callbacks_p2 = [\n",
    "    EarlyStopping(monitor='val_accuracy', patience=10, restore_best_weights=True, mode='max', verbose=1),\n",
    "    ModelCheckpoint(f'{MODEL_SAVE_PATH}/{MODEL_NAME}_best.keras', monitor='val_accuracy',\n",
    "                    save_best_only=True, mode='max', verbose=1),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-8, verbose=1)\n",
    "]\n",
    "\n",
    "print(\"Phase 2 starting (30 epochs, LR=1e-5)...\")\n",
    "start_p2 = time.time()\n",
    "\n",
    "history_p2 = model_2.fit(\n",
    "    train_dataset,\n",
    "    epochs=30,\n",
    "    validation_data=val_dataset,\n",
    "    callbacks=callbacks_p2,\n",
    "    class_weight=class_weights,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "time_p2 = time.time() - start_p2\n",
    "total_time = time_p1 + time_p2\n",
    "print(f\"\\nPhase 2: {time_p2/60:.2f} minutes\")\n",
    "print(f\"Total: {total_time/60:.2f} minutes\")\n",
    "\n",
    "# =============================================================================\n",
    "# TRAINING CURVES\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"TRAINING CURVES\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Merge histories\n",
    "history_combined = {\n",
    "    'accuracy': history_p1.history['accuracy'] + history_p2.history['accuracy'],\n",
    "    'val_accuracy': history_p1.history['val_accuracy'] + history_p2.history['val_accuracy'],\n",
    "    'loss': history_p1.history['loss'] + history_p2.history['loss'],\n",
    "    'val_loss': history_p1.history['val_loss'] + history_p2.history['val_loss']\n",
    "}\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "epochs_range = range(1, len(history_combined['accuracy']) + 1)\n",
    "phase1_end = len(history_p1.history['accuracy'])\n",
    "\n",
    "# Accuracy\n",
    "axes[0].plot(epochs_range, history_combined['accuracy'], 'b-', label='Training', lw=2)\n",
    "axes[0].plot(epochs_range, history_combined['val_accuracy'], 'r-', label='Validation', lw=2)\n",
    "axes[0].axvline(x=phase1_end, color='g', linestyle='--', label=f'Fine-tune Start')\n",
    "axes[0].set_title(f'{MODEL_NAME} - Accuracy', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Accuracy')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].set_ylim([0, 1])\n",
    "\n",
    "# Loss\n",
    "axes[1].plot(epochs_range, history_combined['loss'], 'b-', label='Training', lw=2)\n",
    "axes[1].plot(epochs_range, history_combined['val_loss'], 'r-', label='Validation', lw=2)\n",
    "axes[1].axvline(x=phase1_end, color='g', linestyle='--', label=f'Fine-tune Start')\n",
    "axes[1].set_title(f'{MODEL_NAME} - Loss', fontsize=12, fontweight='bold')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Loss')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{RESULTS_PATH}/{MODEL_NAME}_history.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# =============================================================================\n",
    "# MODEL EVALUATION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"MODEL EVALUATION (TEST SET)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Predictions\n",
    "print(\"Generating predictions...\")\n",
    "y_pred_proba = model_2.predict(test_dataset, verbose=1)\n",
    "y_pred = np.argmax(y_pred_proba, axis=1)\n",
    "\n",
    "# Ground-truth labels\n",
    "y_true = []\n",
    "for _, labels in test_dataset:\n",
    "    y_true.extend(np.argmax(labels.numpy(), axis=1))\n",
    "y_true = np.array(y_true)\n",
    "\n",
    "# Metrikler\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "precision = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "recall = recall_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "f1 = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "kappa = cohen_kappa_score(y_true, y_pred)\n",
    "\n",
    "print(f\"\\nPERFORMANS:\")\n",
    "print(f\"   Accuracy : {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "print(f\"   Precision: {precision:.4f}\")\n",
    "print(f\"   Recall   : {recall:.4f}\")\n",
    "print(f\"   F1-Score : {f1:.4f}\")\n",
    "print(f\"   Kappa    : {kappa:.4f}\")\n",
    "\n",
    "# Classification Report\n",
    "print(f\"\\nPER-CLASS RESULTS:\")\n",
    "print(classification_report(y_true, y_pred, target_names=class_names_ordered, zero_division=0))\n",
    "\n",
    "# =============================================================================\n",
    "# CONFUSION MATRIX\n",
    "# =============================================================================\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=class_names_ordered, yticklabels=class_names_ordered,\n",
    "            annot_kws={'size': 14, 'fontweight': 'bold'})\n",
    "plt.title(f'{MODEL_NAME} - Confusion Matrix', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Tahmin')\n",
    "plt.ylabel('True')\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{RESULTS_PATH}/{MODEL_NAME}_confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# =============================================================================\n",
    "# ROC CURVES\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\nROC Curves...\")\n",
    "\n",
    "n_classes = NUM_CLASSES\n",
    "y_true_bin = label_binarize(y_true, classes=range(n_classes))\n",
    "\n",
    "fpr, tpr, roc_auc = {}, {}, {}\n",
    "\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_true_bin[:, i], y_pred_proba[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# Micro/Macro\n",
    "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_true_bin.ravel(), y_pred_proba.ravel())\n",
    "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "mean_tpr = np.zeros_like(all_fpr)\n",
    "for i in range(n_classes):\n",
    "    mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])\n",
    "mean_tpr /= n_classes\n",
    "fpr[\"macro\"], tpr[\"macro\"] = all_fpr, mean_tpr\n",
    "roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "colors = ['#e41a1c', '#377eb8', '#4daf4a', '#984ea3', '#ff7f00']\n",
    "\n",
    "for i, (name, color) in enumerate(zip(class_names_ordered, colors)):\n",
    "    plt.plot(fpr[i], tpr[i], color=color, lw=2, label=f'{name} (AUC={roc_auc[i]:.3f})')\n",
    "\n",
    "plt.plot(fpr[\"micro\"], tpr[\"micro\"], 'deeppink', ls=':', lw=3, label=f'Micro (AUC={roc_auc[\"micro\"]:.3f})')\n",
    "plt.plot(fpr[\"macro\"], tpr[\"macro\"], 'navy', ls=':', lw=3, label=f'Macro (AUC={roc_auc[\"macro\"]:.3f})')\n",
    "plt.plot([0,1], [0,1], 'k--', lw=2)\n",
    "\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title(f'{MODEL_NAME} - ROC Curves', fontsize=14, fontweight='bold')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{RESULTS_PATH}/{MODEL_NAME}_roc_curves.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nROC AUC: Macro={roc_auc['macro']:.4f}, Micro={roc_auc['micro']:.4f}\")\n",
    "\n",
    "# =============================================================================\n",
    "# KAYDET\n",
    "# =============================================================================\n",
    "\n",
    "results_2 = {\n",
    "    'model_name': MODEL_NAME,\n",
    "    'accuracy': accuracy,\n",
    "    'precision': precision,\n",
    "    'recall': recall,\n",
    "    'f1_score': f1,\n",
    "    'kappa': kappa,\n",
    "    'roc_auc_macro': roc_auc['macro'],\n",
    "    'roc_auc_micro': roc_auc['micro'],\n",
    "    'training_time_min': total_time / 60,\n",
    "    'total_epochs': len(history_combined['accuracy']),\n",
    "    'total_params': total_params\n",
    "}\n",
    "\n",
    "for i, name in enumerate(class_names_ordered):\n",
    "    results_2[f'auc_{name}'] = roc_auc[i]\n",
    "\n",
    "# Update all_results list (replace old Model 2 if exists)\n",
    "if len(all_results) > 1:\n",
    "    all_results[1] = results_2\n",
    "else:\n",
    "    all_results.append(results_2)\n",
    "\n",
    "model_2.save(f'{MODEL_SAVE_PATH}/{MODEL_NAME}_final.keras')\n",
    "pd.DataFrame(history_combined).to_csv(f'{RESULTS_PATH}/{MODEL_NAME}_history.csv', index=False)\n",
    "\n",
    "print(f\"\\nModel kaydedildi!\")\n",
    "\n",
    "# =============================================================================\n",
    "# SUMMARY\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(f\"MODEL 2 SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\"\"\n",
    "   Model: {MODEL_NAME}\n",
    "   Accuracy: {accuracy*100:.2f}%\n",
    "   F1-Score: {f1:.4f}\n",
    "   ROC AUC: {roc_auc['macro']:.4f}\n",
    "   Duration: {total_time/60:.2f} minutes\n",
    "\"\"\")\n",
    "\n",
    "gc.collect()\n",
    "print(\"=\" * 70)\n",
    "print(\"MODEL 2 TAMAMLANDI!\")\n",
    "print(\"=\" * 70)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "4UZfcIZD_oB3",
    "outputId": "b5e5749c-dc39-4b78-d7da-09495d69017b"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Cell 15: MODEL 3 - ConvNeXt Tiny"
   ],
   "metadata": {
    "id": "QJNO6DKOiCud"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# =============================================================================\n",
    "# CELL 15: MODEL 3 - ConvNeXt Tiny (Transformer-Style CNN)\n",
    "# =============================================================================\n",
    "# ConvNeXt applies Swin Transformer design principles to CNN:\n",
    "# - Patchify stem (4x4 non-overlapping conv)\n",
    "# - Inverted bottleneck\n",
    "# - Larger kernels (7x7 depthwise conv)\n",
    "# - LayerNorm instead of BatchNorm\n",
    "# - GELU activation\n",
    "# - Fewer activation functions\n",
    "#\n",
    "# This model bridges the gap between CNN and Transformer architectures.\n",
    "# It fulfills the project requirement for an alternative architectural approach.\n",
    "# =============================================================================\n",
    "\n",
    "import time\n",
    "from tensorflow.keras.applications import ConvNeXtTiny\n",
    "from tensorflow.keras.layers import (Input, GlobalAveragePooling2D, Dense,\n",
    "                                      Dropout, BatchNormalization, Activation)\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "MODEL_NAME = \"Model_03_ConvNeXtTiny\"\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"MODEL 3: ConvNeXt Tiny (Transformer-Style CNN)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\"\"\n",
    "MODEL HAKKINDA:\n",
    "   ConvNeXt was developed by Meta AI in 2022.\n",
    "   It modernized CNN architecture inspired by Swin Transformer's success.\n",
    "\n",
    "   Ã–zellikler:\n",
    "   - 7x7 depthwise convolution (large receptive field)\n",
    "   â€¢ LayerNorm (BatchNorm yerine)\n",
    "   â€¢ GELU activation (ReLU yerine)\n",
    "   â€¢ Inverted bottleneck (MobileNet'ten)\n",
    "   â€¢ Patchify stem (ViT'ten)\n",
    "\"\"\")\n",
    "\n",
    "# =============================================================================\n",
    "# MODEL ARCHITECTURE\n",
    "# =============================================================================\n",
    "\n",
    "def build_convnext_model(input_shape, num_classes):\n",
    "    \"\"\"\n",
    "    ConvNeXt Tiny modeli.\n",
    "    With include_preprocessing=True, the model accepts [0,255] input.\n",
    "    \"\"\"\n",
    "    inputs = Input(shape=input_shape, name='input_layer')\n",
    "\n",
    "    # Base model - ImageNet pretrained\n",
    "    base_model = ConvNeXtTiny(\n",
    "        include_top=False,\n",
    "        weights='imagenet',\n",
    "        input_tensor=inputs,\n",
    "        include_preprocessing=True  # [0,255] â†’ normalize\n",
    "    )\n",
    "\n",
    "    # Freeze initially\n",
    "    base_model.trainable = False\n",
    "\n",
    "    # Classification head\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D(name='global_avg_pool')(x)\n",
    "    x = BatchNormalization(name='batch_norm')(x)\n",
    "    x = Dense(256, kernel_regularizer=l2(1e-4), name='dense_256')(x)\n",
    "    x = Activation('relu', name='relu')(x)\n",
    "    x = Dropout(0.5, name='dropout')(x)\n",
    "    outputs = Dense(num_classes, activation='softmax', name='predictions')(x)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=outputs, name='ConvNeXtTiny_Transfer')\n",
    "\n",
    "    return model, base_model\n",
    "\n",
    "# Build model\n",
    "print(\"\\nBuilding model...\")\n",
    "model_3, base_model_3 = build_convnext_model(IMG_SHAPE, NUM_CLASSES)\n",
    "\n",
    "# Parameter information\n",
    "total_params_3 = model_3.count_params()\n",
    "trainable_params_3 = sum([tf.keras.backend.count_params(w) for w in model_3.trainable_weights])\n",
    "non_trainable_3 = total_params_3 - trainable_params_3\n",
    "\n",
    "print(f\"\\nParameter Information:\")\n",
    "print(f\"   - Total parameters  : {total_params_3:,}\")\n",
    "print(f\"   - Trainable          : {trainable_params_3:,}\")\n",
    "print(f\"   - Frozen             : {non_trainable_3:,}\")\n",
    "print(f\"\\n[OK] include_preprocessing=True (Model handles its own normalization)\")\n",
    "\n",
    "# =============================================================================\n",
    "# PHASE 1: FEATURE EXTRACTION (BASE FROZEN)\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"PHASE 1: FEATURE EXTRACTION (BASE MODEL FROZEN)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "model_3.compile(\n",
    "    optimizer=Adam(learning_rate=1e-3),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(f\"Optimizer: Adam (lr=1e-3)\")\n",
    "print(f\"Loss: Categorical Crossentropy\")\n",
    "print(f\"Base Model: FROZEN\")\n",
    "\n",
    "callbacks_p1_m3 = [\n",
    "    EarlyStopping(\n",
    "        monitor='val_accuracy',\n",
    "        patience=10,\n",
    "        restore_best_weights=True,\n",
    "        mode='max',\n",
    "        verbose=1\n",
    "    ),\n",
    "    ModelCheckpoint(\n",
    "        filepath=f'{MODEL_SAVE_PATH}/{MODEL_NAME}_p1_best.keras',\n",
    "        monitor='val_accuracy',\n",
    "        save_best_only=True,\n",
    "        mode='max',\n",
    "        verbose=1\n",
    "    ),\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=3,\n",
    "        min_lr=1e-7,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "EPOCHS_P1 = 20\n",
    "\n",
    "print(f\"\\nPhase 1 Training Starting...\")\n",
    "print(f\"   â€¢ Epochs: {EPOCHS_P1}\")\n",
    "print(f\"   â€¢ Batch Size: {BATCH_SIZE}\")\n",
    "\n",
    "start_p1_m3 = time.time()\n",
    "\n",
    "history_p1_m3 = model_3.fit(\n",
    "    train_dataset,\n",
    "    epochs=EPOCHS_P1,\n",
    "    validation_data=val_dataset,\n",
    "    callbacks=callbacks_p1_m3,\n",
    "    class_weight=class_weights,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "time_p1_m3 = time.time() - start_p1_m3\n",
    "epochs_completed_p1 = len(history_p1_m3.history['accuracy'])\n",
    "\n",
    "print(f\"\\nPhase 1 Complete: {time_p1_m3/60:.2f} minutes ({epochs_completed_p1} epochs)\")\n",
    "\n",
    "# =============================================================================\n",
    "# PHASE 2: FINE-TUNING\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"PHASE 2: FINE-TUNING (FINAL LAYERS UNFROZEN)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Unfreeze final layers of base model\n",
    "base_model_3.trainable = True\n",
    "\n",
    "# Train only last 30 layers\n",
    "fine_tune_at = len(base_model_3.layers) - 30\n",
    "for layer in base_model_3.layers[:fine_tune_at]:\n",
    "    layer.trainable = False\n",
    "\n",
    "trainable_layers = sum([1 for l in base_model_3.layers if l.trainable])\n",
    "print(f\"Fine-tuning configuration:\")\n",
    "print(f\"   - Total layers: {len(base_model_3.layers)}\")\n",
    "print(f\"   - Unfrozen layers: {len(base_model_3.layers) - fine_tune_at}\")\n",
    "print(f\"   - Frozen: {fine_tune_at}\")\n",
    "\n",
    "# Recompile with low learning rate\n",
    "model_3.compile(\n",
    "    optimizer=Adam(learning_rate=1e-5),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(f\"\\n[OK] Optimizer: Adam (lr=1e-5) - Very low\")\n",
    "print(f\"[OK] Pre-trained weights will be preserved\")\n",
    "\n",
    "callbacks_p2_m3 = [\n",
    "    EarlyStopping(\n",
    "        monitor='val_accuracy',\n",
    "        patience=10,\n",
    "        restore_best_weights=True,\n",
    "        mode='max',\n",
    "        verbose=1\n",
    "    ),\n",
    "    ModelCheckpoint(\n",
    "        filepath=f'{MODEL_SAVE_PATH}/{MODEL_NAME}_best.keras',\n",
    "        monitor='val_accuracy',\n",
    "        save_best_only=True,\n",
    "        mode='max',\n",
    "        verbose=1\n",
    "    ),\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=3,\n",
    "        min_lr=1e-8,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "EPOCHS_P2 = 30\n",
    "\n",
    "print(f\"\\nPhase 2 Training Starting...\")\n",
    "print(f\"   â€¢ Epochs: {EPOCHS_P2}\")\n",
    "print(f\"   â€¢ Learning Rate: 1e-5\")\n",
    "\n",
    "start_p2_m3 = time.time()\n",
    "\n",
    "history_p2_m3 = model_3.fit(\n",
    "    train_dataset,\n",
    "    epochs=EPOCHS_P2,\n",
    "    validation_data=val_dataset,\n",
    "    callbacks=callbacks_p2_m3,\n",
    "    class_weight=class_weights,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "time_p2_m3 = time.time() - start_p2_m3\n",
    "epochs_completed_p2 = len(history_p2_m3.history['accuracy'])\n",
    "\n",
    "# Total time and epochs\n",
    "total_time_m3 = time_p1_m3 + time_p2_m3\n",
    "total_epochs_m3 = epochs_completed_p1 + epochs_completed_p2\n",
    "avg_epoch_time_m3 = total_time_m3 / total_epochs_m3\n",
    "\n",
    "print(f\"\\nPhase 2 Complete: {time_p2_m3/60:.2f} minutes ({epochs_completed_p2} epochs)\")\n",
    "print(f\"Total Duration: {total_time_m3/60:.2f} minutes ({total_epochs_m3} epochs)\")\n",
    "print(f\"Average Epoch Duration: {avg_epoch_time_m3:.2f} seconds\")\n",
    "\n",
    "# =============================================================================\n",
    "# TRAINING CURVES\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"TRAINING CURVES\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Merge histories\n",
    "history_combined_m3 = {\n",
    "    'accuracy': history_p1_m3.history['accuracy'] + history_p2_m3.history['accuracy'],\n",
    "    'val_accuracy': history_p1_m3.history['val_accuracy'] + history_p2_m3.history['val_accuracy'],\n",
    "    'loss': history_p1_m3.history['loss'] + history_p2_m3.history['loss'],\n",
    "    'val_loss': history_p1_m3.history['val_loss'] + history_p2_m3.history['val_loss']\n",
    "}\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "epochs_range = range(1, len(history_combined_m3['accuracy']) + 1)\n",
    "\n",
    "# Accuracy\n",
    "axes[0].plot(epochs_range, history_combined_m3['accuracy'], 'b-', label='Training Accuracy', linewidth=2)\n",
    "axes[0].plot(epochs_range, history_combined_m3['val_accuracy'], 'r-', label='Validation Accuracy', linewidth=2)\n",
    "axes[0].axvline(x=epochs_completed_p1, color='green', linestyle='--', label=f'Fine-tuning Start (Epoch {epochs_completed_p1})')\n",
    "axes[0].set_title(f'{MODEL_NAME}\\nModel Accuracy', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Accuracy')\n",
    "axes[0].legend(loc='lower right')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].set_ylim([0, 1])\n",
    "\n",
    "# Loss\n",
    "axes[1].plot(epochs_range, history_combined_m3['loss'], 'b-', label='Training Loss', linewidth=2)\n",
    "axes[1].plot(epochs_range, history_combined_m3['val_loss'], 'r-', label='Validation Loss', linewidth=2)\n",
    "axes[1].axvline(x=epochs_completed_p1, color='green', linestyle='--', label=f'Fine-tuning Start (Epoch {epochs_completed_p1})')\n",
    "axes[1].set_title(f'{MODEL_NAME}\\nModel Loss', fontsize=12, fontweight='bold')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Loss')\n",
    "axes[1].legend(loc='upper right')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "history_path_m3 = f'{RESULTS_PATH}/{MODEL_NAME}_history.png'\n",
    "plt.savefig(history_path_m3, dpi=300, bbox_inches='tight', facecolor='white')\n",
    "plt.show()\n",
    "print(f\"Figure saved: {history_path_m3}\")\n",
    "\n",
    "# =============================================================================\n",
    "# MODEL EVALUATION (TEST SET)\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"MODEL EVALUATION (TEST SET)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\nGenerating predictions on test set...\")\n",
    "y_pred_proba_3 = model_3.predict(test_dataset, verbose=1)\n",
    "y_pred_3 = np.argmax(y_pred_proba_3, axis=1)\n",
    "\n",
    "# Ground-truth labels\n",
    "y_true_3 = []\n",
    "for _, labels in test_dataset:\n",
    "    y_true_3.extend(np.argmax(labels.numpy(), axis=1))\n",
    "y_true_3 = np.array(y_true_3)\n",
    "\n",
    "# Metrikler\n",
    "accuracy_3 = accuracy_score(y_true_3, y_pred_3)\n",
    "precision_3 = precision_score(y_true_3, y_pred_3, average='weighted', zero_division=0)\n",
    "recall_3 = recall_score(y_true_3, y_pred_3, average='weighted', zero_division=0)\n",
    "f1_3 = f1_score(y_true_3, y_pred_3, average='weighted', zero_division=0)\n",
    "kappa_3 = cohen_kappa_score(y_true_3, y_pred_3)\n",
    "\n",
    "print(f\"\\nPERFORMANCE METRICS:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"   Accuracy  : {accuracy_3:.4f} ({accuracy_3*100:.2f}%)\")\n",
    "print(f\"   Precision : {precision_3:.4f}\")\n",
    "print(f\"   Recall    : {recall_3:.4f}\")\n",
    "print(f\"   F1-Score  : {f1_3:.4f}\")\n",
    "print(f\"   Kappa     : {kappa_3:.4f}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Classification Report\n",
    "print(f\"\\nPER-CLASS PERFORMANCE:\")\n",
    "print(\"=\" * 70)\n",
    "print(classification_report(y_true_3, y_pred_3, target_names=class_names_ordered, zero_division=0))\n",
    "\n",
    "# =============================================================================\n",
    "# CONFUSION MATRIX\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"CONFUSION MATRIX\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "cm_3 = confusion_matrix(y_true_3, y_pred_3)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm_3, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=class_names_ordered,\n",
    "            yticklabels=class_names_ordered,\n",
    "            annot_kws={'size': 14, 'fontweight': 'bold'})\n",
    "plt.title(f'{MODEL_NAME}\\nConfusion Matrix', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Predicted Class', fontsize=12)\n",
    "plt.ylabel('True Class', fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "\n",
    "cm_path_3 = f'{RESULTS_PATH}/{MODEL_NAME}_confusion_matrix.png'\n",
    "plt.savefig(cm_path_3, dpi=300, bbox_inches='tight', facecolor='white')\n",
    "plt.show()\n",
    "print(f\"Figure saved: {cm_path_3}\")\n",
    "\n",
    "# =============================================================================\n",
    "# ROC CURVES\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"ROC CURVES\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "n_classes = NUM_CLASSES\n",
    "y_true_bin_3 = label_binarize(y_true_3, classes=range(n_classes))\n",
    "\n",
    "# ROC per class\n",
    "fpr_3, tpr_3, roc_auc_3 = {}, {}, {}\n",
    "\n",
    "for i in range(n_classes):\n",
    "    fpr_3[i], tpr_3[i], _ = roc_curve(y_true_bin_3[:, i], y_pred_proba_3[:, i])\n",
    "    roc_auc_3[i] = auc(fpr_3[i], tpr_3[i])\n",
    "\n",
    "# Micro average\n",
    "fpr_3[\"micro\"], tpr_3[\"micro\"], _ = roc_curve(y_true_bin_3.ravel(), y_pred_proba_3.ravel())\n",
    "roc_auc_3[\"micro\"] = auc(fpr_3[\"micro\"], tpr_3[\"micro\"])\n",
    "\n",
    "# Macro average\n",
    "all_fpr_3 = np.unique(np.concatenate([fpr_3[i] for i in range(n_classes)]))\n",
    "mean_tpr_3 = np.zeros_like(all_fpr_3)\n",
    "for i in range(n_classes):\n",
    "    mean_tpr_3 += np.interp(all_fpr_3, fpr_3[i], tpr_3[i])\n",
    "mean_tpr_3 /= n_classes\n",
    "fpr_3[\"macro\"] = all_fpr_3\n",
    "tpr_3[\"macro\"] = mean_tpr_3\n",
    "roc_auc_3[\"macro\"] = auc(fpr_3[\"macro\"], tpr_3[\"macro\"])\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "colors = ['#e41a1c', '#377eb8', '#4daf4a', '#984ea3', '#ff7f00']\n",
    "\n",
    "for i, (class_name, color) in enumerate(zip(class_names_ordered, colors)):\n",
    "    plt.plot(fpr_3[i], tpr_3[i], color=color, lw=2,\n",
    "             label=f'{class_name} (AUC = {roc_auc_3[i]:.4f})')\n",
    "\n",
    "plt.plot(fpr_3[\"micro\"], tpr_3[\"micro\"], color='deeppink', linestyle=':', lw=3,\n",
    "         label=f'Micro-average (AUC = {roc_auc_3[\"micro\"]:.4f})')\n",
    "plt.plot(fpr_3[\"macro\"], tpr_3[\"macro\"], color='navy', linestyle=':', lw=3,\n",
    "         label=f'Macro-average (AUC = {roc_auc_3[\"macro\"]:.4f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=2, label='Random (AUC = 0.500)')\n",
    "\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate', fontsize=12)\n",
    "plt.ylabel('True Positive Rate', fontsize=12)\n",
    "plt.title(f'{MODEL_NAME}\\nROC Curves', fontsize=14, fontweight='bold')\n",
    "plt.legend(loc='lower right', fontsize=9)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "\n",
    "roc_path_3 = f'{RESULTS_PATH}/{MODEL_NAME}_roc_curves.png'\n",
    "plt.savefig(roc_path_3, dpi=300, bbox_inches='tight', facecolor='white')\n",
    "plt.show()\n",
    "print(f\"Figure saved: {roc_path_3}\")\n",
    "\n",
    "# ROC AUC values\n",
    "print(f\"\\nROC AUC Values:\")\n",
    "print(\"-\" * 45)\n",
    "for i, class_name in enumerate(class_names_ordered):\n",
    "    print(f\"   {class_name:<20}: {roc_auc_3[i]:.4f}\")\n",
    "print(\"-\" * 45)\n",
    "print(f\"   {'Macro-average':<20}: {roc_auc_3['macro']:.4f}\")\n",
    "print(f\"   {'Micro-average':<20}: {roc_auc_3['micro']:.4f}\")\n",
    "\n",
    "# =============================================================================\n",
    "# SAVE RESULTS\n",
    "# =============================================================================\n",
    "\n",
    "results_3 = {\n",
    "    'model_name': MODEL_NAME,\n",
    "    'accuracy': accuracy_3,\n",
    "    'precision': precision_3,\n",
    "    'recall': recall_3,\n",
    "    'f1_score': f1_3,\n",
    "    'kappa': kappa_3,\n",
    "    'roc_auc_macro': roc_auc_3['macro'],\n",
    "    'roc_auc_micro': roc_auc_3['micro'],\n",
    "    'training_time_min': total_time_m3 / 60,\n",
    "    'avg_epoch_time_sec': avg_epoch_time_m3,\n",
    "    'total_epochs': total_epochs_m3,\n",
    "    'total_params': total_params_3\n",
    "}\n",
    "\n",
    "for i, class_name in enumerate(class_names_ordered):\n",
    "    results_3[f'auc_{class_name}'] = roc_auc_3[i]\n",
    "\n",
    "all_results.append(results_3)\n",
    "\n",
    "# =============================================================================\n",
    "# SAVE MODEL\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"MODEL KAYDETME\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "model_save_path_3 = f'{MODEL_SAVE_PATH}/{MODEL_NAME}_final.keras'\n",
    "model_3.save(model_save_path_3)\n",
    "print(f\"Model kaydedildi: {model_save_path_3}\")\n",
    "\n",
    "history_csv_path_3 = f'{RESULTS_PATH}/{MODEL_NAME}_history.csv'\n",
    "pd.DataFrame(history_combined_m3).to_csv(history_csv_path_3, index=False)\n",
    "print(f\"[OK] Training history saved: {history_csv_path_3}\")\n",
    "\n",
    "# =============================================================================\n",
    "# MODEL COMPARISON TABLE\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"MODEL COMPARISON TABLE\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\n{'Method':<40} {'Precision':>10} {'Recall':>10} {'F1-Score':>10} {'Accuracy':>10} {'Avg Epoch(s)':>12}\")\n",
    "print(\"-\" * 95)\n",
    "\n",
    "for r in all_results:\n",
    "    avg_epoch = r.get('avg_epoch_time_sec', (r['training_time_min'] * 60) / r['total_epochs'])\n",
    "    print(f\"{r['model_name']:<40} {r['precision']:>10.4f} {r['recall']:>10.4f} {r['f1_score']:>10.4f} {r['accuracy']:>10.4f} {avg_epoch:>12.2f}\")\n",
    "\n",
    "print(\"-\" * 95)\n",
    "\n",
    "# Best model\n",
    "best_model = max(all_results, key=lambda x: x['accuracy'])\n",
    "print(f\"\\nHighest accuracy: {best_model['model_name']} ({best_model['accuracy']*100:.2f}%)\")\n",
    "\n",
    "# =============================================================================\n",
    "# MODEL 3 DETAILED SUMMARY REPORT\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"MODEL 3 - DETAILED SUMMARY REPORT\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\"\"\n",
    "MODEL INFORMATION\n",
    "{'â”€' * 60}\n",
    "   Model Name        : {MODEL_NAME}\n",
    "   Model Tipi        : Transfer Learning (Transformer-Style CNN)\n",
    "   Base Model        : ConvNeXt Tiny (ImageNet pretrained)\n",
    "   Toplam Parametre  : {total_params_3:,}\n",
    "   Preprocessing     : include_preprocessing=True (Inside model)\n",
    "\n",
    "ARCHITECTURE DETAILS\n",
    "{'â”€' * 60}\n",
    "   ConvNeXt Tiny Ã–zellikleri:\n",
    "   â€¢ Patchify stem: 4x4 non-overlapping convolution\n",
    "   â€¢ Inverted bottleneck blocks\n",
    "   - 7x7 depthwise convolution (large receptive field)\n",
    "   â€¢ LayerNorm (BatchNorm yerine)\n",
    "   â€¢ GELU activation (ReLU yerine)\n",
    "\n",
    "   Classification Head:\n",
    "   â€¢ GlobalAveragePooling2D\n",
    "   â€¢ BatchNormalization\n",
    "   â€¢ Dense(256) + ReLU + Dropout(0.5)\n",
    "   â€¢ Dense(5) + Softmax\n",
    "\n",
    "TRAINING STRATEGY\n",
    "{'â”€' * 60}\n",
    "   Phase 1 - Feature Extraction:\n",
    "   â€¢ Base model: FROZEN\n",
    "   â€¢ Learning rate: 1e-3\n",
    "   â€¢ Epochs: {epochs_completed_p1}\n",
    "   â€¢ Duration: {time_p1_m3/60:.2f} minutes\n",
    "\n",
    "   Phase 2 - Fine-tuning:\n",
    "   â€¢ Unfrozen layers: Last {len(base_model_3.layers) - fine_tune_at}\n",
    "   â€¢ Learning rate: 1e-5\n",
    "   â€¢ Epochs: {epochs_completed_p2}\n",
    "   â€¢ Duration: {time_p2_m3/60:.2f} minutes\n",
    "\n",
    "PERFORMANCE METRICS\n",
    "{'â”€' * 60}\n",
    "   â€¢ Accuracy        : {accuracy_3*100:.2f}%\n",
    "   â€¢ Precision       : {precision_3:.4f}\n",
    "   â€¢ Recall          : {recall_3:.4f}\n",
    "   â€¢ F1-Score        : {f1_3:.4f}\n",
    "   â€¢ Cohen's Kappa   : {kappa_3:.4f}\n",
    "\n",
    "ROC AUC VALUES (Per Class)\n",
    "{'â”€' * 60}\"\"\")\n",
    "\n",
    "for i, class_name in enumerate(class_names_ordered):\n",
    "    print(f\"   â€¢ {class_name:<20}: {roc_auc_3[i]:.4f}\")\n",
    "\n",
    "print(f\"\"\"   {'â”€' * 40}\n",
    "   â€¢ Macro-average       : {roc_auc_3['macro']:.4f}\n",
    "   â€¢ Micro-average       : {roc_auc_3['micro']:.4f}\n",
    "\n",
    "TRAINING DURATION\n",
    "{'â”€' * 60}\n",
    "   â€¢ Phase 1            : {time_p1_m3/60:.2f} minutes\n",
    "   â€¢ Phase 2            : {time_p2_m3/60:.2f} minutes\n",
    "   â€¢ Toplam             : {total_time_m3/60:.2f} minutes\n",
    "   â€¢ Toplam Epoch       : {total_epochs_m3}\n",
    "   â€¢ Ortalama Epoch     : {avg_epoch_time_m3:.2f} saniye\n",
    "\n",
    "SAVED FILES\n",
    "{'â”€' * 60}\n",
    "   â€¢ {model_save_path_3}\n",
    "   â€¢ {history_csv_path_3}\n",
    "   â€¢ {history_path_m3}\n",
    "   â€¢ {cm_path_3}\n",
    "   â€¢ {roc_path_3}\n",
    "\"\"\")\n",
    "\n",
    "# Bellek temizle\n",
    "gc.collect()\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"MODEL 3 (ConvNeXt Tiny) TAMAMLANDI!\")\n",
    "print(\"=\" * 70)"
   ],
   "metadata": {
    "id": "cL4hd2M1Bx8a",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "outputId": "30ac4a05-498d-4491-85f6-8bec32235760"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Cell 16: MODEL 4 - DenseNet121 + InceptionV3 (HYBRID)"
   ],
   "metadata": {
    "id": "pvIr6OeHv6HY"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install keras-tuner -q"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZY223r2DwnHI",
    "outputId": "91f6391f-b91d-493f-fcc5-b25002730b9c"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# =============================================================================\n",
    "# CELL 16: MODEL 4 - DenseNet121 + InceptionV3 (HYBRID MODEL)\n",
    "# =============================================================================\n",
    "# This model is designed following the reference architecture schematic:\n",
    "# - Two backbones: DenseNet121 and InceptionV3\n",
    "# - Separate classification head per backbone\n",
    "# - Feature concatenation\n",
    "# - Final classification head\n",
    "#\n",
    "# KEY MODIFICATIONS:\n",
    "# 1. No fine-tuning - Base models fully FROZEN\n",
    "# 2. Hyperparameter optimization via Keras Tuner\n",
    "# 3. Architecture (Dense 256 -> 128 -> output)\n",
    "# =============================================================================\n",
    "\n",
    "import time\n",
    "import keras_tuner as kt\n",
    "from tensorflow.keras.applications import DenseNet121, InceptionV3\n",
    "from tensorflow.keras.layers import (Input, GlobalAveragePooling2D, Dense,\n",
    "                                      Dropout, BatchNormalization, Concatenate)\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam, SGD, RMSprop\n",
    "\n",
    "MODEL_NAME = \"Model_04_DenseNet121_InceptionV3_Hybrid\"\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"MODEL 4: DenseNet121 + InceptionV3 (HYBRID)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\"\"\n",
    "MODEL ARCHITECTURE:\n",
    "\n",
    "    Input Image (224x224x3)\n",
    "           |\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”\n",
    "    â–¼             â–¼\n",
    "DenseNet121   InceptionV3\n",
    "(FROZEN)      (FROZEN)\n",
    "    |             |\n",
    "   GAP           GAP\n",
    "    |             |\n",
    "Dense(256)    Dense(256)\n",
    "Dropout       Dropout\n",
    "Dense(128)    Dense(128)\n",
    "Dropout       Dropout\n",
    "    |             |\n",
    "    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜\n",
    "           |\n",
    "      Concatenate\n",
    "           |\n",
    "      Dense(256)\n",
    "      Dropout\n",
    "      Dense(128)\n",
    "      Dropout\n",
    "           |\n",
    "      Dense(5) + Softmax\n",
    "           |\n",
    "        Output\n",
    "\"\"\")\n",
    "\n",
    "# =============================================================================\n",
    "# ENHANCED DATA AUGMENTATION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"ENHANCED DATA AUGMENTATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Update current augmentation\n",
    "enhanced_augmentation = tf.keras.Sequential([\n",
    "    tf.keras.layers.RandomFlip(\"horizontal\"),\n",
    "    tf.keras.layers.RandomRotation(0.15),  # +/-15% (slightly increased)\n",
    "    tf.keras.layers.RandomZoom(0.15),       # Â±15%\n",
    "    tf.keras.layers.RandomContrast(0.1),    # Contrast variation\n",
    "    tf.keras.layers.RandomBrightness(0.1),  # Brightness variation\n",
    "], name=\"enhanced_augmentation\")\n",
    "\n",
    "print(\"[OK] Augmentation Layers:\")\n",
    "print(\"   â€¢ RandomFlip: horizontal\")\n",
    "print(\"   â€¢ RandomRotation: Â±15%\")\n",
    "print(\"   â€¢ RandomZoom: Â±15%\")\n",
    "print(\"   â€¢ RandomContrast: Â±10%\")\n",
    "print(\"   â€¢ RandomBrightness: Â±10%\")\n",
    "\n",
    "# Create augmented dataset\n",
    "def create_augmented_dataset(filepaths, labels, is_training=False):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((filepaths, labels))\n",
    "\n",
    "    if is_training:\n",
    "        dataset = dataset.shuffle(buffer_size=SHUFFLE_BUFFER, seed=SEED)\n",
    "\n",
    "    dataset = dataset.map(load_image_no_preprocess, num_parallel_calls=AUTOTUNE)\n",
    "\n",
    "    if is_training:\n",
    "        dataset = dataset.map(\n",
    "            lambda x, y: (enhanced_augmentation(x, training=True), y),\n",
    "            num_parallel_calls=AUTOTUNE\n",
    "        )\n",
    "\n",
    "    dataset = dataset.cache()\n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "    dataset = dataset.prefetch(AUTOTUNE)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "# Yeni datasetler\n",
    "train_dataset_aug = create_augmented_dataset(train_filepaths, train_labels, is_training=True)\n",
    "val_dataset_aug = create_augmented_dataset(val_filepaths, val_labels, is_training=False)\n",
    "test_dataset_aug = create_augmented_dataset(test_filepaths, test_labels, is_training=False)\n",
    "\n",
    "print(\"\\nDatasets prepared with enhanced augmentation.\")\n",
    "\n",
    "# =============================================================================\n",
    "# HYBRID MODEL ARCHITECTURE (FOR HYPERPARAMETER TUNING)\n",
    "# =============================================================================\n",
    "\n",
    "def build_hybrid_model(hp):\n",
    "    \"\"\"\n",
    "    Hyperparameter-tunable model builder for Keras Tuner.\n",
    "    Dual-backbone hybrid architecture per reference schematic.\n",
    "    \"\"\"\n",
    "\n",
    "    # Hyperparameters\n",
    "    learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
    "    dropout_1 = hp.Choice('dropout_1', values=[0.2, 0.3, 0.4])\n",
    "    dropout_2 = hp.Choice('dropout_2', values=[0.4, 0.5, 0.6])\n",
    "    dense_units_1 = hp.Choice('dense_units_1', values=[256, 512])\n",
    "    dense_units_2 = hp.Choice('dense_units_2', values=[128, 256])\n",
    "    optimizer_name = hp.Choice('optimizer', values=['adam', 'sgd', 'rmsprop'])\n",
    "\n",
    "    # Input\n",
    "    inputs = Input(shape=IMG_SHAPE, name='input')\n",
    "\n",
    "    # =========================================================================\n",
    "    # BACKBONE 1: DenseNet121\n",
    "    # =========================================================================\n",
    "    densenet = DenseNet121(\n",
    "        include_top=False,\n",
    "        weights='imagenet',\n",
    "        input_tensor=inputs\n",
    "    )\n",
    "    densenet.trainable = False  # FROZEN - Fine-tuning YOK!\n",
    "\n",
    "    # DenseNet preprocessing (scale to [0,1] then normalize)\n",
    "    x1 = tf.keras.applications.densenet.preprocess_input(inputs)\n",
    "    x1 = densenet(x1)\n",
    "    x1 = GlobalAveragePooling2D(name='gap_densenet')(x1)\n",
    "\n",
    "    # DenseNet classification head (per reference schematic)\n",
    "    x1 = Dense(dense_units_1, activation='relu', name='dense1_dn')(x1)\n",
    "    x1 = Dropout(dropout_1, name='drop1_dn')(x1)\n",
    "    x1 = Dense(dense_units_2, activation='relu', name='dense2_dn')(x1)\n",
    "    x1 = Dropout(dropout_2, name='drop2_dn')(x1)\n",
    "\n",
    "    # =========================================================================\n",
    "    # BACKBONE 2: InceptionV3\n",
    "    # =========================================================================\n",
    "    inception = InceptionV3(\n",
    "        include_top=False,\n",
    "        weights='imagenet',\n",
    "        input_tensor=inputs\n",
    "    )\n",
    "    inception.trainable = False  # FROZEN - Fine-tuning YOK!\n",
    "\n",
    "    # InceptionV3 preprocessing (scale to [-1,1])\n",
    "    x2 = tf.keras.applications.inception_v3.preprocess_input(inputs)\n",
    "    x2 = inception(x2)\n",
    "    x2 = GlobalAveragePooling2D(name='gap_inception')(x2)\n",
    "\n",
    "    # Inception classification head (per reference schematic)\n",
    "    x2 = Dense(dense_units_1, activation='relu', name='dense1_inc')(x2)\n",
    "    x2 = Dropout(dropout_1, name='drop1_inc')(x2)\n",
    "    x2 = Dense(dense_units_2, activation='relu', name='dense2_inc')(x2)\n",
    "    x2 = Dropout(dropout_2, name='drop2_inc')(x2)\n",
    "\n",
    "    # =========================================================================\n",
    "    # CONCATENATE & FINAL CLASSIFICATION\n",
    "    # =========================================================================\n",
    "    merged = Concatenate(name='concat')([x1, x2])\n",
    "\n",
    "    # Final classification head\n",
    "    x = Dense(dense_units_1, activation='relu', name='dense1_final')(merged)\n",
    "    x = Dropout(dropout_1, name='drop1_final')(x)\n",
    "    x = Dense(dense_units_2, activation='relu', name='dense2_final')(x)\n",
    "    x = Dropout(dropout_2, name='drop2_final')(x)\n",
    "\n",
    "    # Output\n",
    "    outputs = Dense(NUM_CLASSES, activation='softmax', name='output')(x)\n",
    "\n",
    "    # Model\n",
    "    model = Model(inputs=inputs, outputs=outputs, name='DenseNet121_InceptionV3_Hybrid')\n",
    "\n",
    "    # Optimizer selection\n",
    "    if optimizer_name == 'adam':\n",
    "        optimizer = Adam(learning_rate=learning_rate)\n",
    "    elif optimizer_name == 'sgd':\n",
    "        optimizer = SGD(learning_rate=learning_rate, momentum=0.9)\n",
    "    else:\n",
    "        optimizer = RMSprop(learning_rate=learning_rate)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "# =============================================================================\n",
    "# HYPERPARAMETER OPTIMIZATION VIA KERAS TUNER\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"HYPERPARAMETER OPTIMIZATION (KERAS TUNER)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Tuner dizini\n",
    "tuner_dir = f'{MODEL_SAVE_PATH}/tuner_{MODEL_NAME}'\n",
    "\n",
    "# Bayesian Optimization tuner\n",
    "tuner = kt.BayesianOptimization(\n",
    "    build_hybrid_model,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=15,  # Try 15 different combinations\n",
    "    num_initial_points=5,\n",
    "    directory=tuner_dir,\n",
    "    project_name='hybrid_tuning',\n",
    "    overwrite=True\n",
    ")\n",
    "\n",
    "print(f\"\"\"\n",
    "Keras Tuner Settings:\n",
    "   â€¢ Algoritma: Bayesian Optimization\n",
    "   â€¢ Max Trials: 15\n",
    "   â€¢ Objective: val_accuracy (maximize)\n",
    "\n",
    "Aranacak Hyperparameter'lar:\n",
    "   â€¢ Learning Rate: [0.01, 0.001, 0.0001]\n",
    "   â€¢ Dropout 1: [0.2, 0.3, 0.4]\n",
    "   â€¢ Dropout 2: [0.4, 0.5, 0.6]\n",
    "   â€¢ Dense Units 1: [256, 512]\n",
    "   â€¢ Dense Units 2: [128, 256]\n",
    "   â€¢ Optimizer: [Adam, SGD, RMSprop]\n",
    "\"\"\")\n",
    "\n",
    "# Early stopping callback\n",
    "early_stop = EarlyStopping(\n",
    "    monitor='val_accuracy',\n",
    "    patience=5,\n",
    "    restore_best_weights=True,\n",
    "    mode='max'\n",
    ")\n",
    "\n",
    "# Tuner search\n",
    "print(\"Hyperparameter search starting...\")\n",
    "print(\"   (This may take several minutes)\\n\")\n",
    "\n",
    "start_tuning = time.time()\n",
    "\n",
    "tuner.search(\n",
    "    train_dataset_aug,\n",
    "    epochs=20,\n",
    "    validation_data=val_dataset_aug,\n",
    "    callbacks=[early_stop],\n",
    "    class_weight=class_weights,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "tuning_time = time.time() - start_tuning\n",
    "print(f\"\\nTuning duration: {tuning_time/60:.2f} minutes\")\n",
    "\n",
    "# =============================================================================\n",
    "# BEST HYPERPARAMETERS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"BEST HYPERPARAMETERS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "print(f\"\"\"\n",
    "Optimal Hyperparameter'lar:\n",
    "   â€¢ Learning Rate: {best_hps.get('learning_rate')}\n",
    "   â€¢ Dropout 1: {best_hps.get('dropout_1')}\n",
    "   â€¢ Dropout 2: {best_hps.get('dropout_2')}\n",
    "   â€¢ Dense Units 1: {best_hps.get('dense_units_1')}\n",
    "   â€¢ Dense Units 2: {best_hps.get('dense_units_2')}\n",
    "   â€¢ Optimizer: {best_hps.get('optimizer')}\n",
    "\"\"\")\n",
    "\n",
    "# =============================================================================\n",
    "# FINAL TRAINING WITH BEST MODEL\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"FINAL MODEL TRAINING\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Best model\n",
    "model_4 = tuner.hypermodel.build(best_hps)\n",
    "\n",
    "# Model summary\n",
    "total_params_4 = model_4.count_params()\n",
    "trainable_params_4 = sum([tf.keras.backend.count_params(w) for w in model_4.trainable_weights])\n",
    "\n",
    "print(f\"\\nModel Bilgisi:\")\n",
    "print(f\"   - Total parameters: {total_params_4:,}\")\n",
    "print(f\"   - Trainable: {trainable_params_4:,}\")\n",
    "print(f\"   â€¢ Frozen (backbone): {total_params_4 - trainable_params_4:,}\")\n",
    "\n",
    "# Callbacks\n",
    "callbacks_final = [\n",
    "    EarlyStopping(\n",
    "        monitor='val_accuracy',\n",
    "        patience=15,\n",
    "        restore_best_weights=True,\n",
    "        mode='max',\n",
    "        verbose=1\n",
    "    ),\n",
    "    ModelCheckpoint(\n",
    "        filepath=f'{MODEL_SAVE_PATH}/{MODEL_NAME}_best.keras',\n",
    "        monitor='val_accuracy',\n",
    "        save_best_only=True,\n",
    "        mode='max',\n",
    "        verbose=1\n",
    "    ),\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=5,\n",
    "        min_lr=1e-7,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "# Final training\n",
    "print(\"\\nFinal training starting (50 epochs)...\")\n",
    "start_training = time.time()\n",
    "\n",
    "history_4 = model_4.fit(\n",
    "    train_dataset_aug,\n",
    "    epochs=50,\n",
    "    validation_data=val_dataset_aug,\n",
    "    callbacks=callbacks_final,\n",
    "    class_weight=class_weights,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "training_time = time.time() - start_training\n",
    "total_time_4 = tuning_time + training_time\n",
    "total_epochs_4 = len(history_4.history['accuracy'])\n",
    "avg_epoch_time_4 = training_time / total_epochs_4\n",
    "\n",
    "print(f\"\\nFinal training: {training_time/60:.2f} minutes ({total_epochs_4} epochs)\")\n",
    "print(f\"Total duration (tuning + training): {total_time_4/60:.2f} minutes\")\n",
    "\n",
    "# =============================================================================\n",
    "# TRAINING CURVES\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"TRAINING CURVES\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "epochs_range = range(1, len(history_4.history['accuracy']) + 1)\n",
    "\n",
    "# Accuracy\n",
    "axes[0].plot(epochs_range, history_4.history['accuracy'], 'b-', label='Training', lw=2)\n",
    "axes[0].plot(epochs_range, history_4.history['val_accuracy'], 'r-', label='Validation', lw=2)\n",
    "axes[0].set_title(f'{MODEL_NAME}\\nModel Accuracy', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Accuracy')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].set_ylim([0, 1])\n",
    "\n",
    "# Loss\n",
    "axes[1].plot(epochs_range, history_4.history['loss'], 'b-', label='Training', lw=2)\n",
    "axes[1].plot(epochs_range, history_4.history['val_loss'], 'r-', label='Validation', lw=2)\n",
    "axes[1].set_title(f'{MODEL_NAME}\\nModel Loss', fontsize=12, fontweight='bold')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Loss')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{RESULTS_PATH}/{MODEL_NAME}_history.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# =============================================================================\n",
    "# MODEL EVALUATION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"MODEL EVALUATION (TEST SET)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "y_pred_proba_4 = model_4.predict(test_dataset_aug, verbose=1)\n",
    "y_pred_4 = np.argmax(y_pred_proba_4, axis=1)\n",
    "\n",
    "y_true_4 = []\n",
    "for _, labels in test_dataset_aug:\n",
    "    y_true_4.extend(np.argmax(labels.numpy(), axis=1))\n",
    "y_true_4 = np.array(y_true_4)\n",
    "\n",
    "# Metrikler\n",
    "accuracy_4 = accuracy_score(y_true_4, y_pred_4)\n",
    "precision_4 = precision_score(y_true_4, y_pred_4, average='weighted', zero_division=0)\n",
    "recall_4 = recall_score(y_true_4, y_pred_4, average='weighted', zero_division=0)\n",
    "f1_4 = f1_score(y_true_4, y_pred_4, average='weighted', zero_division=0)\n",
    "kappa_4 = cohen_kappa_score(y_true_4, y_pred_4)\n",
    "\n",
    "print(f\"\\nPERFORMANCE METRICS:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"   Accuracy  : {accuracy_4:.4f} ({accuracy_4*100:.2f}%)\")\n",
    "print(f\"   Precision : {precision_4:.4f}\")\n",
    "print(f\"   Recall    : {recall_4:.4f}\")\n",
    "print(f\"   F1-Score  : {f1_4:.4f}\")\n",
    "print(f\"   Kappa     : {kappa_4:.4f}\")\n",
    "\n",
    "print(f\"\\nClassification Report:\")\n",
    "print(classification_report(y_true_4, y_pred_4, target_names=class_names_ordered, zero_division=0))\n",
    "\n",
    "# =============================================================================\n",
    "# CONFUSION MATRIX\n",
    "# =============================================================================\n",
    "\n",
    "cm_4 = confusion_matrix(y_true_4, y_pred_4)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm_4, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=class_names_ordered, yticklabels=class_names_ordered,\n",
    "            annot_kws={'size': 14, 'fontweight': 'bold'})\n",
    "plt.title(f'{MODEL_NAME}\\nConfusion Matrix', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Tahmin')\n",
    "plt.ylabel('True')\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{RESULTS_PATH}/{MODEL_NAME}_confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# =============================================================================\n",
    "# ROC CURVES\n",
    "# =============================================================================\n",
    "\n",
    "y_true_bin_4 = label_binarize(y_true_4, classes=range(NUM_CLASSES))\n",
    "fpr_4, tpr_4, roc_auc_4 = {}, {}, {}\n",
    "\n",
    "for i in range(NUM_CLASSES):\n",
    "    fpr_4[i], tpr_4[i], _ = roc_curve(y_true_bin_4[:, i], y_pred_proba_4[:, i])\n",
    "    roc_auc_4[i] = auc(fpr_4[i], tpr_4[i])\n",
    "\n",
    "fpr_4[\"micro\"], tpr_4[\"micro\"], _ = roc_curve(y_true_bin_4.ravel(), y_pred_proba_4.ravel())\n",
    "roc_auc_4[\"micro\"] = auc(fpr_4[\"micro\"], tpr_4[\"micro\"])\n",
    "\n",
    "all_fpr_4 = np.unique(np.concatenate([fpr_4[i] for i in range(NUM_CLASSES)]))\n",
    "mean_tpr_4 = np.zeros_like(all_fpr_4)\n",
    "for i in range(NUM_CLASSES):\n",
    "    mean_tpr_4 += np.interp(all_fpr_4, fpr_4[i], tpr_4[i])\n",
    "mean_tpr_4 /= NUM_CLASSES\n",
    "fpr_4[\"macro\"], tpr_4[\"macro\"] = all_fpr_4, mean_tpr_4\n",
    "roc_auc_4[\"macro\"] = auc(fpr_4[\"macro\"], tpr_4[\"macro\"])\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "colors = ['#e41a1c', '#377eb8', '#4daf4a', '#984ea3', '#ff7f00']\n",
    "for i, (name, color) in enumerate(zip(class_names_ordered, colors)):\n",
    "    plt.plot(fpr_4[i], tpr_4[i], color=color, lw=2, label=f'{name} (AUC={roc_auc_4[i]:.4f})')\n",
    "plt.plot(fpr_4[\"micro\"], tpr_4[\"micro\"], 'deeppink', ls=':', lw=3, label=f'Micro (AUC={roc_auc_4[\"micro\"]:.4f})')\n",
    "plt.plot(fpr_4[\"macro\"], tpr_4[\"macro\"], 'navy', ls=':', lw=3, label=f'Macro (AUC={roc_auc_4[\"macro\"]:.4f})')\n",
    "plt.plot([0,1], [0,1], 'k--', lw=2)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title(f'{MODEL_NAME}\\nROC Curves', fontsize=14, fontweight='bold')\n",
    "plt.legend(loc='lower right', fontsize=9)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{RESULTS_PATH}/{MODEL_NAME}_roc_curves.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nROC AUC Values:\")\n",
    "for i, name in enumerate(class_names_ordered):\n",
    "    print(f\"   {name}: {roc_auc_4[i]:.4f}\")\n",
    "print(f\"   Macro: {roc_auc_4['macro']:.4f}, Micro: {roc_auc_4['micro']:.4f}\")\n",
    "\n",
    "# =============================================================================\n",
    "# SAVE RESULTS\n",
    "# =============================================================================\n",
    "\n",
    "results_4 = {\n",
    "    'model_name': MODEL_NAME,\n",
    "    'accuracy': accuracy_4,\n",
    "    'precision': precision_4,\n",
    "    'recall': recall_4,\n",
    "    'f1_score': f1_4,\n",
    "    'kappa': kappa_4,\n",
    "    'roc_auc_macro': roc_auc_4['macro'],\n",
    "    'roc_auc_micro': roc_auc_4['micro'],\n",
    "    'training_time_min': total_time_4 / 60,\n",
    "    'avg_epoch_time_sec': avg_epoch_time_4,\n",
    "    'total_epochs': total_epochs_4,\n",
    "    'total_params': total_params_4,\n",
    "    'best_lr': best_hps.get('learning_rate'),\n",
    "    'best_dropout1': best_hps.get('dropout_1'),\n",
    "    'best_dropout2': best_hps.get('dropout_2'),\n",
    "    'best_optimizer': best_hps.get('optimizer')\n",
    "}\n",
    "\n",
    "for i, name in enumerate(class_names_ordered):\n",
    "    results_4[f'auc_{name}'] = roc_auc_4[i]\n",
    "\n",
    "all_results.append(results_4)\n",
    "\n",
    "# Save model\n",
    "model_4.save(f'{MODEL_SAVE_PATH}/{MODEL_NAME}_final.keras')\n",
    "pd.DataFrame(history_4.history).to_csv(f'{RESULTS_PATH}/{MODEL_NAME}_history.csv', index=False)\n",
    "\n",
    "# =============================================================================\n",
    "# MODEL COMPARISON TABLE\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"MODEL COMPARISON TABLE\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\n{'Method':<45} {'Precision':>10} {'Recall':>10} {'F1-Score':>10} {'Accuracy':>10} {'Avg Epoch(s)':>12}\")\n",
    "print(\"-\" * 100)\n",
    "\n",
    "for r in all_results:\n",
    "    avg_epoch = r.get('avg_epoch_time_sec', (r['training_time_min'] * 60) / max(r['total_epochs'], 1))\n",
    "    print(f\"{r['model_name']:<45} {r['precision']:>10.4f} {r['recall']:>10.4f} {r['f1_score']:>10.4f} {r['accuracy']:>10.4f} {avg_epoch:>12.2f}\")\n",
    "\n",
    "print(\"-\" * 100)\n",
    "best = max(all_results, key=lambda x: x['accuracy'])\n",
    "print(f\"\\nEn iyi: {best['model_name']} ({best['accuracy']*100:.2f}%)\")\n",
    "\n",
    "# =============================================================================\n",
    "# DETAILED SUMMARY REPORT\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"MODEL 4 - DETAILED SUMMARY REPORT\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\"\"\n",
    "MODEL INFORMATION\n",
    "{'â”€' * 60}\n",
    "   Model Name        : {MODEL_NAME}\n",
    "   Model Tipi        : Hybrid (Dual-Backbone)\n",
    "   Backbone 1        : DenseNet121 (FROZEN)\n",
    "   Backbone 2        : InceptionV3 (FROZEN)\n",
    "   Toplam Parametre  : {total_params_4:,}\n",
    "   Trainable          : {trainable_params_4:,}\n",
    "\n",
    "ARCHITECTURE (Per Reference Schematic)\n",
    "{'â”€' * 60}\n",
    "   â€¢ DenseNet121 â†’ GAP â†’ Dense({best_hps.get('dense_units_1')}) â†’ Dropout({best_hps.get('dropout_1')}) â†’ Dense({best_hps.get('dense_units_2')}) â†’ Dropout({best_hps.get('dropout_2')})\n",
    "   â€¢ InceptionV3 â†’ GAP â†’ Dense({best_hps.get('dense_units_1')}) â†’ Dropout({best_hps.get('dropout_1')}) â†’ Dense({best_hps.get('dense_units_2')}) â†’ Dropout({best_hps.get('dropout_2')})\n",
    "   â€¢ Concatenate â†’ Dense({best_hps.get('dense_units_1')}) â†’ Dense({best_hps.get('dense_units_2')}) â†’ Softmax(5)\n",
    "\n",
    "OPTIMAL HYPERPARAMETERS (Keras Tuner)\n",
    "{'â”€' * 60}\n",
    "   â€¢ Learning Rate   : {best_hps.get('learning_rate')}\n",
    "   â€¢ Dropout 1       : {best_hps.get('dropout_1')}\n",
    "   â€¢ Dropout 2       : {best_hps.get('dropout_2')}\n",
    "   â€¢ Dense Units 1   : {best_hps.get('dense_units_1')}\n",
    "   â€¢ Dense Units 2   : {best_hps.get('dense_units_2')}\n",
    "   â€¢ Optimizer       : {best_hps.get('optimizer')}\n",
    "\n",
    "PERFORMANCE METRICS\n",
    "{'â”€' * 60}\n",
    "   â€¢ Accuracy        : {accuracy_4*100:.2f}%\n",
    "   â€¢ Precision       : {precision_4:.4f}\n",
    "   â€¢ Recall          : {recall_4:.4f}\n",
    "   â€¢ F1-Score        : {f1_4:.4f}\n",
    "   â€¢ Cohen's Kappa   : {kappa_4:.4f}\n",
    "\n",
    "ROC AUC VALUES\n",
    "{'â”€' * 60}\"\"\")\n",
    "\n",
    "for i, name in enumerate(class_names_ordered):\n",
    "    print(f\"   â€¢ {name:<20}: {roc_auc_4[i]:.4f}\")\n",
    "\n",
    "print(f\"\"\"   {'â”€' * 40}\n",
    "   â€¢ Macro-average       : {roc_auc_4['macro']:.4f}\n",
    "   â€¢ Micro-average       : {roc_auc_4['micro']:.4f}\n",
    "\n",
    "TRAINING DURATION\n",
    "{'â”€' * 60}\n",
    "   â€¢ Hyperparameter Tuning : {tuning_time/60:.2f} minutes\n",
    "   â€¢ Final Training        : {training_time/60:.2f} minutes\n",
    "   â€¢ Toplam                : {total_time_4/60:.2f} minutes\n",
    "   â€¢ Epoch Count           : {total_epochs_4}\n",
    "   â€¢ Ortalama Epoch        : {avg_epoch_time_4:.2f} saniye\n",
    "\n",
    "SAVED FILES\n",
    "{'â”€' * 60}\n",
    "   â€¢ {MODEL_SAVE_PATH}/{MODEL_NAME}_final.keras\n",
    "   â€¢ {RESULTS_PATH}/{MODEL_NAME}_history.csv\n",
    "   â€¢ {RESULTS_PATH}/{MODEL_NAME}_history.png\n",
    "   â€¢ {RESULTS_PATH}/{MODEL_NAME}_confusion_matrix.png\n",
    "   â€¢ {RESULTS_PATH}/{MODEL_NAME}_roc_curves.png\n",
    "\"\"\")\n",
    "\n",
    "gc.collect()\n",
    "print(\"=\" * 70)\n",
    "print(\"MODEL 4 (DenseNet121 + InceptionV3 Hybrid) TAMAMLANDI!\")\n",
    "print(\"=\" * 70)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "UAB0qAArv_Cd",
    "outputId": "0740b749-de61-47aa-fefa-4f50df792632"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Cell 17: MODEL 5 - ResNet50 + VGG16 (HYBRID)"
   ],
   "metadata": {
    "id": "xZJPDPnLAJe5"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# =============================================================================\n",
    "# CELL 17: MODEL 5 - ResNet50 + VGG16 (HYBRID MODEL)\n",
    "# =============================================================================\n",
    "# This model combines two classical and powerful CNN architectures:\n",
    "# - ResNet50: Deep network with residual connections\n",
    "# - VGG16: Simple yet effective architecture\n",
    "#\n",
    "# Features:\n",
    "# - No fine-tuning (base models FROZEN)\n",
    "# - Hyperparameter optimization via Keras Tuner\n",
    "# =============================================================================\n",
    "\n",
    "import time\n",
    "import keras_tuner as kt\n",
    "from tensorflow.keras.applications import ResNet50, VGG16\n",
    "from tensorflow.keras.layers import (Input, GlobalAveragePooling2D, Dense,\n",
    "                                      Dropout, Concatenate, BatchNormalization)\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam, SGD, RMSprop\n",
    "\n",
    "MODEL_NAME = \"Model_05_ResNet50_VGG16_Hybrid\"\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"MODEL 5: ResNet50 + VGG16 (HYBRID)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\"\"\n",
    "Model Mimarisi:\n",
    "\n",
    "    Input Image (224x224x3)\n",
    "           |\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”\n",
    "    â–¼             â–¼\n",
    "  ResNet50      VGG16\n",
    "  (FROZEN)     (FROZEN)\n",
    "    |             |\n",
    "   GAP           GAP\n",
    "    |             |\n",
    "Dense(256)    Dense(256)\n",
    "Dropout       Dropout\n",
    "Dense(128)    Dense(128)\n",
    "Dropout       Dropout\n",
    "    |             |\n",
    "    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜\n",
    "           |\n",
    "      Concatenate\n",
    "           |\n",
    "      Dense(256)\n",
    "      Dropout\n",
    "      Dense(128)\n",
    "      Dropout\n",
    "           |\n",
    "      Dense(5) + Softmax\n",
    "\"\"\")\n",
    "\n",
    "# =============================================================================\n",
    "# HYBRID MODEL BUILDER (FOR KERAS TUNER)\n",
    "# =============================================================================\n",
    "\n",
    "def build_resnet_vgg_hybrid(hp):\n",
    "    \"\"\"\n",
    "    ResNet50 + VGG16 Hybrid model builder.\n",
    "    Hyperparameter-tunable for Keras Tuner.\n",
    "    \"\"\"\n",
    "\n",
    "    # Hyperparameters\n",
    "    learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
    "    dropout_1 = hp.Choice('dropout_1', values=[0.2, 0.3, 0.4])\n",
    "    dropout_2 = hp.Choice('dropout_2', values=[0.4, 0.5, 0.6])\n",
    "    dense_units_1 = hp.Choice('dense_units_1', values=[256, 512])\n",
    "    dense_units_2 = hp.Choice('dense_units_2', values=[128, 256])\n",
    "    optimizer_name = hp.Choice('optimizer', values=['adam', 'sgd', 'rmsprop'])\n",
    "\n",
    "    # Input\n",
    "    inputs = Input(shape=IMG_SHAPE, name='input_layer')\n",
    "\n",
    "    # =========================================================================\n",
    "    # BACKBONE 1: ResNet50\n",
    "    # =========================================================================\n",
    "    resnet = ResNet50(\n",
    "        include_top=False,\n",
    "        weights='imagenet',\n",
    "        input_shape=IMG_SHAPE\n",
    "    )\n",
    "    resnet.trainable = False  # FROZEN\n",
    "\n",
    "    # ResNet preprocessing ve feature extraction\n",
    "    x1 = tf.keras.applications.resnet50.preprocess_input(inputs)\n",
    "    x1 = resnet(x1, training=False)\n",
    "    x1 = GlobalAveragePooling2D(name='gap_resnet')(x1)\n",
    "\n",
    "    # ResNet classification head\n",
    "    x1 = Dense(dense_units_1, activation='relu', name='dense1_resnet')(x1)\n",
    "    x1 = Dropout(dropout_1, name='drop1_resnet')(x1)\n",
    "    x1 = Dense(dense_units_2, activation='relu', name='dense2_resnet')(x1)\n",
    "    x1 = Dropout(dropout_2, name='drop2_resnet')(x1)\n",
    "\n",
    "    # =========================================================================\n",
    "    # BACKBONE 2: VGG16\n",
    "    # =========================================================================\n",
    "    vgg = VGG16(\n",
    "        include_top=False,\n",
    "        weights='imagenet',\n",
    "        input_shape=IMG_SHAPE\n",
    "    )\n",
    "    vgg.trainable = False  # FROZEN\n",
    "\n",
    "    # VGG preprocessing ve feature extraction\n",
    "    x2 = tf.keras.applications.vgg16.preprocess_input(inputs)\n",
    "    x2 = vgg(x2, training=False)\n",
    "    x2 = GlobalAveragePooling2D(name='gap_vgg')(x2)\n",
    "\n",
    "    # VGG classification head\n",
    "    x2 = Dense(dense_units_1, activation='relu', name='dense1_vgg')(x2)\n",
    "    x2 = Dropout(dropout_1, name='drop1_vgg')(x2)\n",
    "    x2 = Dense(dense_units_2, activation='relu', name='dense2_vgg')(x2)\n",
    "    x2 = Dropout(dropout_2, name='drop2_vgg')(x2)\n",
    "\n",
    "    # =========================================================================\n",
    "    # CONCATENATE & FINAL CLASSIFICATION\n",
    "    # =========================================================================\n",
    "    merged = Concatenate(name='concat')([x1, x2])\n",
    "\n",
    "    # Final classification head\n",
    "    x = Dense(dense_units_1, activation='relu', name='dense1_final')(merged)\n",
    "    x = Dropout(dropout_1, name='drop1_final')(x)\n",
    "    x = Dense(dense_units_2, activation='relu', name='dense2_final')(x)\n",
    "    x = Dropout(dropout_2, name='drop2_final')(x)\n",
    "\n",
    "    # Output\n",
    "    outputs = Dense(NUM_CLASSES, activation='softmax', name='output')(x)\n",
    "\n",
    "    # Model\n",
    "    model = Model(inputs=inputs, outputs=outputs, name='ResNet50_VGG16_Hybrid')\n",
    "\n",
    "    # Optimizer\n",
    "    if optimizer_name == 'adam':\n",
    "        optimizer = Adam(learning_rate=learning_rate)\n",
    "    elif optimizer_name == 'sgd':\n",
    "        optimizer = SGD(learning_rate=learning_rate, momentum=0.9)\n",
    "    else:\n",
    "        optimizer = RMSprop(learning_rate=learning_rate)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "# =============================================================================\n",
    "# KERAS TUNER - HYPERPARAMETER SEARCH\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"HYPERPARAMETER OPTIMIZATION (KERAS TUNER)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "tuner_dir = f'{MODEL_SAVE_PATH}/tuner_{MODEL_NAME}'\n",
    "\n",
    "tuner = kt.BayesianOptimization(\n",
    "    build_resnet_vgg_hybrid,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=15,\n",
    "    num_initial_points=5,\n",
    "    directory=tuner_dir,\n",
    "    project_name='resnet_vgg_tuning',\n",
    "    overwrite=True\n",
    ")\n",
    "\n",
    "print(\"\"\"\n",
    "Keras Tuner Ayarlari:\n",
    "   - Algoritma: Bayesian Optimization\n",
    "   - Max Trials: 15\n",
    "   - Objective: val_accuracy\n",
    "\n",
    "Aranan Hyperparameter'lar:\n",
    "   - Learning Rate: [0.01, 0.001, 0.0001]\n",
    "   - Dropout 1: [0.2, 0.3, 0.4]\n",
    "   - Dropout 2: [0.4, 0.5, 0.6]\n",
    "   - Dense Units 1: [256, 512]\n",
    "   - Dense Units 2: [128, 256]\n",
    "   - Optimizer: [Adam, SGD, RMSprop]\n",
    "\"\"\")\n",
    "\n",
    "early_stop_tuner = EarlyStopping(\n",
    "    monitor='val_accuracy',\n",
    "    patience=5,\n",
    "    restore_best_weights=True,\n",
    "    mode='max'\n",
    ")\n",
    "\n",
    "print(\"Hyperparameter aramasi basliyor...\")\n",
    "print(\"(This process may take 30-40 minutes)\\n\")\n",
    "\n",
    "start_tuning = time.time()\n",
    "\n",
    "tuner.search(\n",
    "    train_dataset_aug,\n",
    "    epochs=20,\n",
    "    validation_data=val_dataset_aug,\n",
    "    callbacks=[early_stop_tuner],\n",
    "    class_weight=class_weights,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "tuning_time = time.time() - start_tuning\n",
    "print(f\"\\nTuning duration: {tuning_time/60:.2f} minutes\")\n",
    "\n",
    "# =============================================================================\n",
    "# EN IYI HYPERPARAMETER'LAR\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"EN IYI HYPERPARAMETER'LAR\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "print(f\"\"\"\n",
    "Optimal Hyperparameter'lar:\n",
    "   - Learning Rate: {best_hps.get('learning_rate')}\n",
    "   - Dropout 1: {best_hps.get('dropout_1')}\n",
    "   - Dropout 2: {best_hps.get('dropout_2')}\n",
    "   - Dense Units 1: {best_hps.get('dense_units_1')}\n",
    "   - Dense Units 2: {best_hps.get('dense_units_2')}\n",
    "   - Optimizer: {best_hps.get('optimizer')}\n",
    "\"\"\")\n",
    "\n",
    "# =============================================================================\n",
    "# FINAL MODEL TRAINING\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"FINAL MODEL EGITIMI\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "model_5 = tuner.hypermodel.build(best_hps)\n",
    "\n",
    "total_params_5 = model_5.count_params()\n",
    "trainable_params_5 = sum([tf.keras.backend.count_params(w) for w in model_5.trainable_weights])\n",
    "\n",
    "print(f\"\\nModel Bilgisi:\")\n",
    "print(f\"   - Total parameters: {total_params_5:,}\")\n",
    "print(f\"   - Egitilebilir: {trainable_params_5:,}\")\n",
    "print(f\"   - Frozen (backbone): {total_params_5 - trainable_params_5:,}\")\n",
    "\n",
    "callbacks_final = [\n",
    "    EarlyStopping(\n",
    "        monitor='val_accuracy',\n",
    "        patience=15,\n",
    "        restore_best_weights=True,\n",
    "        mode='max',\n",
    "        verbose=1\n",
    "    ),\n",
    "    ModelCheckpoint(\n",
    "        filepath=f'{MODEL_SAVE_PATH}/{MODEL_NAME}_best.keras',\n",
    "        monitor='val_accuracy',\n",
    "        save_best_only=True,\n",
    "        mode='max',\n",
    "        verbose=1\n",
    "    ),\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=5,\n",
    "        min_lr=1e-7,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "print(\"\\nFinal egitim basliyor (50 epoch)...\")\n",
    "start_training = time.time()\n",
    "\n",
    "history_5 = model_5.fit(\n",
    "    train_dataset_aug,\n",
    "    epochs=50,\n",
    "    validation_data=val_dataset_aug,\n",
    "    callbacks=callbacks_final,\n",
    "    class_weight=class_weights,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "training_time = time.time() - start_training\n",
    "total_time_5 = tuning_time + training_time\n",
    "total_epochs_5 = len(history_5.history['accuracy'])\n",
    "avg_epoch_time_5 = training_time / total_epochs_5\n",
    "\n",
    "print(f\"\\nFinal training: {training_time/60:.2f} minutes ({total_epochs_5} epochs)\")\n",
    "print(f\"Total duration: {total_time_5/60:.2f} minutes\")\n",
    "\n",
    "# =============================================================================\n",
    "# EGITIM GRAFIKLERI\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"EGITIM GRAFIKLERI\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "epochs_range = range(1, len(history_5.history['accuracy']) + 1)\n",
    "\n",
    "# Accuracy - Train/Test formatinda\n",
    "axes[0].plot(epochs_range, history_5.history['accuracy'], 'b-', label='Train', linewidth=2)\n",
    "axes[0].plot(epochs_range, history_5.history['val_accuracy'], 'r-', label='Test', linewidth=2)\n",
    "axes[0].set_title(f'{MODEL_NAME}\\nModel Accuracy', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Accuracy')\n",
    "axes[0].legend(loc='lower right')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].set_ylim([0, 1])\n",
    "\n",
    "# Loss - Train/Test formatinda\n",
    "axes[1].plot(epochs_range, history_5.history['loss'], 'b-', label='Train', linewidth=2)\n",
    "axes[1].plot(epochs_range, history_5.history['val_loss'], 'r-', label='Test', linewidth=2)\n",
    "axes[1].set_title(f'{MODEL_NAME}\\nModel Loss', fontsize=12, fontweight='bold')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Loss')\n",
    "axes[1].legend(loc='upper right')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{RESULTS_PATH}/{MODEL_NAME}_history.png', dpi=300, bbox_inches='tight', facecolor='white')\n",
    "plt.show()\n",
    "\n",
    "# =============================================================================\n",
    "# MODEL EVALUATION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"MODEL DEGERLENDIRME (TEST)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "y_pred_proba_5 = model_5.predict(test_dataset_aug, verbose=1)\n",
    "y_pred_5 = np.argmax(y_pred_proba_5, axis=1)\n",
    "\n",
    "y_true_5 = []\n",
    "for _, labels in test_dataset_aug:\n",
    "    y_true_5.extend(np.argmax(labels.numpy(), axis=1))\n",
    "y_true_5 = np.array(y_true_5)\n",
    "\n",
    "# Metrikler\n",
    "accuracy_5 = accuracy_score(y_true_5, y_pred_5)\n",
    "precision_5 = precision_score(y_true_5, y_pred_5, average='weighted', zero_division=0)\n",
    "recall_5 = recall_score(y_true_5, y_pred_5, average='weighted', zero_division=0)\n",
    "f1_5 = f1_score(y_true_5, y_pred_5, average='weighted', zero_division=0)\n",
    "kappa_5 = cohen_kappa_score(y_true_5, y_pred_5)\n",
    "\n",
    "print(f\"\\nPerformance Metrics:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"   Accuracy  : {accuracy_5:.4f} ({accuracy_5*100:.2f}%)\")\n",
    "print(f\"   Precision : {precision_5:.4f}\")\n",
    "print(f\"   Recall    : {recall_5:.4f}\")\n",
    "print(f\"   F1-Score  : {f1_5:.4f}\")\n",
    "print(f\"   Kappa     : {kappa_5:.4f}\")\n",
    "\n",
    "print(f\"\\nClassification Report:\")\n",
    "print(classification_report(y_true_5, y_pred_5, target_names=class_names_ordered, zero_division=0))\n",
    "\n",
    "# =============================================================================\n",
    "# CONFUSION MATRIX\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"CONFUSION MATRIX\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "cm_5 = confusion_matrix(y_true_5, y_pred_5)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm_5, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=class_names_ordered, yticklabels=class_names_ordered,\n",
    "            annot_kws={'size': 14, 'fontweight': 'bold'})\n",
    "plt.title(f'{MODEL_NAME}\\nConfusion Matrix', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Tahmin')\n",
    "plt.ylabel('Gercek')\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{RESULTS_PATH}/{MODEL_NAME}_confusion_matrix.png', dpi=300, bbox_inches='tight', facecolor='white')\n",
    "plt.show()\n",
    "\n",
    "# =============================================================================\n",
    "# ROC CURVES\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"ROC EGRILERI\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "y_true_bin_5 = label_binarize(y_true_5, classes=range(NUM_CLASSES))\n",
    "fpr_5, tpr_5, roc_auc_5 = {}, {}, {}\n",
    "\n",
    "for i in range(NUM_CLASSES):\n",
    "    fpr_5[i], tpr_5[i], _ = roc_curve(y_true_bin_5[:, i], y_pred_proba_5[:, i])\n",
    "    roc_auc_5[i] = auc(fpr_5[i], tpr_5[i])\n",
    "\n",
    "fpr_5[\"micro\"], tpr_5[\"micro\"], _ = roc_curve(y_true_bin_5.ravel(), y_pred_proba_5.ravel())\n",
    "roc_auc_5[\"micro\"] = auc(fpr_5[\"micro\"], tpr_5[\"micro\"])\n",
    "\n",
    "all_fpr_5 = np.unique(np.concatenate([fpr_5[i] for i in range(NUM_CLASSES)]))\n",
    "mean_tpr_5 = np.zeros_like(all_fpr_5)\n",
    "for i in range(NUM_CLASSES):\n",
    "    mean_tpr_5 += np.interp(all_fpr_5, fpr_5[i], tpr_5[i])\n",
    "mean_tpr_5 /= NUM_CLASSES\n",
    "fpr_5[\"macro\"], tpr_5[\"macro\"] = all_fpr_5, mean_tpr_5\n",
    "roc_auc_5[\"macro\"] = auc(fpr_5[\"macro\"], tpr_5[\"macro\"])\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "colors = ['#e41a1c', '#377eb8', '#4daf4a', '#984ea3', '#ff7f00']\n",
    "for i, (name, color) in enumerate(zip(class_names_ordered, colors)):\n",
    "    plt.plot(fpr_5[i], tpr_5[i], color=color, lw=2, label=f'{name} (AUC={roc_auc_5[i]:.4f})')\n",
    "plt.plot(fpr_5[\"micro\"], tpr_5[\"micro\"], 'deeppink', ls=':', lw=3, label=f'Micro-avg (AUC={roc_auc_5[\"micro\"]:.4f})')\n",
    "plt.plot(fpr_5[\"macro\"], tpr_5[\"macro\"], 'navy', ls=':', lw=3, label=f'Macro-avg (AUC={roc_auc_5[\"macro\"]:.4f})')\n",
    "plt.plot([0,1], [0,1], 'k--', lw=2)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title(f'{MODEL_NAME}\\nROC Curves', fontsize=14, fontweight='bold')\n",
    "plt.legend(loc='lower right', fontsize=9)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{RESULTS_PATH}/{MODEL_NAME}_roc_curves.png', dpi=300, bbox_inches='tight', facecolor='white')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nROC AUC Degerleri:\")\n",
    "for i, name in enumerate(class_names_ordered):\n",
    "    print(f\"   {name}: {roc_auc_5[i]:.4f}\")\n",
    "print(f\"   Macro-avg: {roc_auc_5['macro']:.4f}\")\n",
    "print(f\"   Micro-avg: {roc_auc_5['micro']:.4f}\")\n",
    "\n",
    "# =============================================================================\n",
    "# SONUCLARI KAYDET\n",
    "# =============================================================================\n",
    "\n",
    "results_5 = {\n",
    "    'model_name': MODEL_NAME,\n",
    "    'accuracy': accuracy_5,\n",
    "    'precision': precision_5,\n",
    "    'recall': recall_5,\n",
    "    'f1_score': f1_5,\n",
    "    'kappa': kappa_5,\n",
    "    'roc_auc_macro': roc_auc_5['macro'],\n",
    "    'roc_auc_micro': roc_auc_5['micro'],\n",
    "    'training_time_min': total_time_5 / 60,\n",
    "    'avg_epoch_time_sec': avg_epoch_time_5,\n",
    "    'total_epochs': total_epochs_5,\n",
    "    'total_params': total_params_5,\n",
    "    'best_lr': best_hps.get('learning_rate'),\n",
    "    'best_dropout1': best_hps.get('dropout_1'),\n",
    "    'best_dropout2': best_hps.get('dropout_2'),\n",
    "    'best_optimizer': best_hps.get('optimizer')\n",
    "}\n",
    "\n",
    "for i, name in enumerate(class_names_ordered):\n",
    "    results_5[f'auc_{name}'] = roc_auc_5[i]\n",
    "\n",
    "all_results.append(results_5)\n",
    "\n",
    "# Save model\n",
    "model_5.save(f'{MODEL_SAVE_PATH}/{MODEL_NAME}_final.keras')\n",
    "pd.DataFrame(history_5.history).to_csv(f'{RESULTS_PATH}/{MODEL_NAME}_history.csv', index=False)\n",
    "\n",
    "print(f\"\\nModel kaydedildi: {MODEL_SAVE_PATH}/{MODEL_NAME}_final.keras\")\n",
    "\n",
    "# =============================================================================\n",
    "# MODEL COMPARISON TABLE\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"MODEL COMPARISON TABLE\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\n{'Method':<45} {'Precision':>10} {'Recall':>10} {'F1-Score':>10} {'Accuracy':>10} {'Avg Epoch(s)':>12}\")\n",
    "print(\"-\" * 100)\n",
    "\n",
    "for r in all_results:\n",
    "    avg_epoch = r.get('avg_epoch_time_sec', (r['training_time_min'] * 60) / max(r['total_epochs'], 1))\n",
    "    print(f\"{r['model_name']:<45} {r['precision']:>10.4f} {r['recall']:>10.4f} {r['f1_score']:>10.4f} {r['accuracy']:>10.4f} {avg_epoch:>12.2f}\")\n",
    "\n",
    "print(\"-\" * 100)\n",
    "best = max(all_results, key=lambda x: x['accuracy'])\n",
    "print(f\"\\nEn iyi model: {best['model_name']} (Accuracy: {best['accuracy']*100:.2f}%)\")\n",
    "\n",
    "# =============================================================================\n",
    "# OZET RAPOR\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"MODEL 5 - OZET RAPOR\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\"\"\n",
    "Model Bilgileri:\n",
    "   Model Adi         : {MODEL_NAME}\n",
    "   Model Tipi        : Hybrid (Dual-Backbone)\n",
    "   Backbone 1        : ResNet50 (FROZEN)\n",
    "   Backbone 2        : VGG16 (FROZEN)\n",
    "   Toplam Parametre  : {total_params_5:,}\n",
    "   Egitilebilir      : {trainable_params_5:,}\n",
    "\n",
    "Optimal Hyperparameter'lar (Keras Tuner):\n",
    "   Learning Rate   : {best_hps.get('learning_rate')}\n",
    "   Dropout 1       : {best_hps.get('dropout_1')}\n",
    "   Dropout 2       : {best_hps.get('dropout_2')}\n",
    "   Dense Units 1   : {best_hps.get('dense_units_1')}\n",
    "   Dense Units 2   : {best_hps.get('dense_units_2')}\n",
    "   Optimizer       : {best_hps.get('optimizer')}\n",
    "\n",
    "Performans Metrikleri:\n",
    "   Accuracy        : {accuracy_5*100:.2f}%\n",
    "   Precision       : {precision_5:.4f}\n",
    "   Recall          : {recall_5:.4f}\n",
    "   F1-Score        : {f1_5:.4f}\n",
    "   Cohen's Kappa   : {kappa_5:.4f}\n",
    "   ROC AUC (Macro) : {roc_auc_5['macro']:.4f}\n",
    "\n",
    "Egitim Suresi:\n",
    "   Tuning          : {tuning_time/60:.2f} minutes\n",
    "   Final Egitim    : {training_time/60:.2f} minutes\n",
    "   Toplam          : {total_time_5/60:.2f} minutes\n",
    "\"\"\")\n",
    "\n",
    "gc.collect()\n",
    "print(\"=\" * 70)\n",
    "print(\"MODEL 5 (ResNet50 + VGG16 Hybrid) TAMAMLANDI!\")\n",
    "print(\"=\" * 70)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "1OK-gOwZAOIw",
    "outputId": "570b295c-ccca-4180-84e4-086d29e31db5"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Cell 18: MODEL 6 - EfficientNetB0 + MobileNetV2 (HYBRID)"
   ],
   "metadata": {
    "id": "pLaiPliMI8TV"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# =============================================================================\n",
    "# CELL 18: MODEL 6 - EfficientNetB0 + MobileNetV2 (HYBRID MODEL)\n",
    "# =============================================================================\n",
    "# This model combines two lightweight and efficient CNN architectures:\n",
    "# - EfficientNetB0: Architecture optimized via compound scaling\n",
    "# - MobileNetV2: Lightweight architecture with depthwise separable convolution\n",
    "#\n",
    "# Ozellikler:\n",
    "# - Base models FROZEN (transfer learning)\n",
    "# - Hyperparameter optimization via Keras Tuner\n",
    "# - Dual-backbone hybrid architecture\n",
    "# =============================================================================\n",
    "\n",
    "import time\n",
    "import keras_tuner as kt\n",
    "from tensorflow.keras.applications import EfficientNetB0, MobileNetV2\n",
    "from tensorflow.keras.layers import (Input, GlobalAveragePooling2D, Dense,\n",
    "                                      Dropout, Concatenate, BatchNormalization)\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam, SGD, RMSprop\n",
    "\n",
    "MODEL_NAME = \"Model_06_EfficientNetB0_MobileNetV2_Hybrid\"\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"MODEL 6: EfficientNetB0 + MobileNetV2 (HYBRID)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\"\"\n",
    "Model Mimarisi:\n",
    "\n",
    "    Input Image (224x224x3)\n",
    "           |\n",
    "    +------+------+\n",
    "    |             |\n",
    "EfficientNetB0  MobileNetV2\n",
    "  (FROZEN)       (FROZEN)\n",
    "    |             |\n",
    "   GAP           GAP\n",
    "    |             |\n",
    "Dense(256)    Dense(256)\n",
    "Dropout       Dropout\n",
    "Dense(128)    Dense(128)\n",
    "Dropout       Dropout\n",
    "    |             |\n",
    "    +------+------+\n",
    "           |\n",
    "      Concatenate\n",
    "           |\n",
    "      Dense(256)\n",
    "      Dropout\n",
    "      Dense(128)\n",
    "      Dropout\n",
    "           |\n",
    "      Dense(5) + Softmax\n",
    "           |\n",
    "        Output\n",
    "\n",
    "Backbone Ozellikleri:\n",
    "- EfficientNetB0: 5.3M parametre, ImageNet top-1 accuracy 77.1%\n",
    "- MobileNetV2: 3.4M parametre, ImageNet top-1 accuracy 71.8%\n",
    "- Toplam: Hafif ve hizli hybrid model\n",
    "\"\"\")\n",
    "\n",
    "# =============================================================================\n",
    "# HYBRID MODEL BUILDER (FOR KERAS TUNER)\n",
    "# =============================================================================\n",
    "\n",
    "def build_efficientnet_mobilenet_hybrid(hp):\n",
    "    \"\"\"\n",
    "    EfficientNetB0 + MobileNetV2 Hybrid model builder.\n",
    "    Keras Tuner ile hyperparameter optimization.\n",
    "    \"\"\"\n",
    "\n",
    "    # Hyperparameters\n",
    "    learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
    "    dropout_1 = hp.Choice('dropout_1', values=[0.2, 0.3, 0.4])\n",
    "    dropout_2 = hp.Choice('dropout_2', values=[0.4, 0.5, 0.6])\n",
    "    dense_units_1 = hp.Choice('dense_units_1', values=[256, 512])\n",
    "    dense_units_2 = hp.Choice('dense_units_2', values=[128, 256])\n",
    "    optimizer_name = hp.Choice('optimizer', values=['adam', 'sgd', 'rmsprop'])\n",
    "\n",
    "    # Input\n",
    "    inputs = Input(shape=IMG_SHAPE, name='input_layer')\n",
    "\n",
    "    # =========================================================================\n",
    "    # BACKBONE 1: EfficientNetB0\n",
    "    # =========================================================================\n",
    "    efficientnet = EfficientNetB0(\n",
    "        include_top=False,\n",
    "        weights='imagenet',\n",
    "        input_shape=IMG_SHAPE\n",
    "    )\n",
    "    efficientnet.trainable = False  # FROZEN\n",
    "\n",
    "    # EfficientNet branch\n",
    "    x1 = efficientnet(inputs, training=False)\n",
    "    x1 = GlobalAveragePooling2D(name='gap_efficientnet')(x1)\n",
    "    x1 = Dense(dense_units_1, activation='relu', name='dense1_eff')(x1)\n",
    "    x1 = Dropout(dropout_1, name='drop1_eff')(x1)\n",
    "    x1 = Dense(dense_units_2, activation='relu', name='dense2_eff')(x1)\n",
    "    x1 = Dropout(dropout_2, name='drop2_eff')(x1)\n",
    "\n",
    "    # =========================================================================\n",
    "    # BACKBONE 2: MobileNetV2\n",
    "    # =========================================================================\n",
    "    mobilenet = MobileNetV2(\n",
    "        include_top=False,\n",
    "        weights='imagenet',\n",
    "        input_shape=IMG_SHAPE\n",
    "    )\n",
    "    mobilenet.trainable = False  # FROZEN\n",
    "\n",
    "    # MobileNet branch\n",
    "    x2 = tf.keras.applications.mobilenet_v2.preprocess_input(inputs)\n",
    "    x2 = mobilenet(x2, training=False)\n",
    "    x2 = GlobalAveragePooling2D(name='gap_mobilenet')(x2)\n",
    "    x2 = Dense(dense_units_1, activation='relu', name='dense1_mob')(x2)\n",
    "    x2 = Dropout(dropout_1, name='drop1_mob')(x2)\n",
    "    x2 = Dense(dense_units_2, activation='relu', name='dense2_mob')(x2)\n",
    "    x2 = Dropout(dropout_2, name='drop2_mob')(x2)\n",
    "\n",
    "    # =========================================================================\n",
    "    # CONCATENATE & FINAL CLASSIFICATION\n",
    "    # =========================================================================\n",
    "    merged = Concatenate(name='concat')([x1, x2])\n",
    "\n",
    "    # Final classification head\n",
    "    x = Dense(dense_units_1, activation='relu', name='dense1_final')(merged)\n",
    "    x = Dropout(dropout_1, name='drop1_final')(x)\n",
    "    x = Dense(dense_units_2, activation='relu', name='dense2_final')(x)\n",
    "    x = Dropout(dropout_2, name='drop2_final')(x)\n",
    "\n",
    "    # Output\n",
    "    outputs = Dense(NUM_CLASSES, activation='softmax', name='output')(x)\n",
    "\n",
    "    # Model\n",
    "    model = Model(inputs=inputs, outputs=outputs, name='EfficientNetB0_MobileNetV2_Hybrid')\n",
    "\n",
    "    # Optimizer\n",
    "    if optimizer_name == 'adam':\n",
    "        optimizer = Adam(learning_rate=learning_rate)\n",
    "    elif optimizer_name == 'sgd':\n",
    "        optimizer = SGD(learning_rate=learning_rate, momentum=0.9)\n",
    "    else:\n",
    "        optimizer = RMSprop(learning_rate=learning_rate)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "# =============================================================================\n",
    "# KERAS TUNER - HYPERPARAMETER OPTIMIZATION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"HYPERPARAMETER OPTIMIZATION (KERAS TUNER)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "tuner_dir = f'{MODEL_SAVE_PATH}/tuner_{MODEL_NAME}'\n",
    "\n",
    "tuner = kt.BayesianOptimization(\n",
    "    build_efficientnet_mobilenet_hybrid,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=15,\n",
    "    num_initial_points=5,\n",
    "    directory=tuner_dir,\n",
    "    project_name='effnet_mobilenet_tuning',\n",
    "    overwrite=True\n",
    ")\n",
    "\n",
    "print(\"\"\"\n",
    "Tuner Konfigurasyonu:\n",
    "   - Algoritma      : Bayesian Optimization\n",
    "   - Max Trials     : 15\n",
    "   - Objective      : val_accuracy (maximize)\n",
    "\n",
    "Hyperparameter Arama Uzayi:\n",
    "   - Learning Rate  : [0.01, 0.001, 0.0001]\n",
    "   - Dropout 1      : [0.2, 0.3, 0.4]\n",
    "   - Dropout 2      : [0.4, 0.5, 0.6]\n",
    "   - Dense Units 1  : [256, 512]\n",
    "   - Dense Units 2  : [128, 256]\n",
    "   - Optimizer      : [Adam, SGD, RMSprop]\n",
    "\"\"\")\n",
    "\n",
    "early_stop_tuner = EarlyStopping(\n",
    "    monitor='val_accuracy',\n",
    "    patience=5,\n",
    "    restore_best_weights=True,\n",
    "    mode='max'\n",
    ")\n",
    "\n",
    "print(\"Hyperparameter aramasi baslatiliyor...\")\n",
    "start_tuning = time.time()\n",
    "\n",
    "tuner.search(\n",
    "    train_dataset_aug,\n",
    "    epochs=20,\n",
    "    validation_data=val_dataset_aug,\n",
    "    callbacks=[early_stop_tuner],\n",
    "    class_weight=class_weights,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "tuning_time = time.time() - start_tuning\n",
    "print(f\"\\nTuning complete: {tuning_time/60:.2f} minutes\")\n",
    "\n",
    "# =============================================================================\n",
    "# OPTIMAL HYPERPARAMETERS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"OPTIMAL HYPERPARAMETERS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "print(f\"\"\"\n",
    "Bulunan Optimal Degerler:\n",
    "   - Learning Rate  : {best_hps.get('learning_rate')}\n",
    "   - Dropout 1      : {best_hps.get('dropout_1')}\n",
    "   - Dropout 2      : {best_hps.get('dropout_2')}\n",
    "   - Dense Units 1  : {best_hps.get('dense_units_1')}\n",
    "   - Dense Units 2  : {best_hps.get('dense_units_2')}\n",
    "   - Optimizer      : {best_hps.get('optimizer')}\n",
    "\"\"\")\n",
    "\n",
    "# =============================================================================\n",
    "# FINAL MODEL TRAINING\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"FINAL MODEL TRAINING\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "model_6 = tuner.hypermodel.build(best_hps)\n",
    "\n",
    "total_params_6 = model_6.count_params()\n",
    "trainable_params_6 = sum([tf.keras.backend.count_params(w) for w in model_6.trainable_weights])\n",
    "\n",
    "print(f\"\"\"\n",
    "Model Parametreleri:\n",
    "   - Toplam         : {total_params_6:,}\n",
    "   - Egitilebilir   : {trainable_params_6:,}\n",
    "   - Frozen         : {total_params_6 - trainable_params_6:,}\n",
    "\"\"\")\n",
    "\n",
    "callbacks_final = [\n",
    "    EarlyStopping(\n",
    "        monitor='val_accuracy',\n",
    "        patience=15,\n",
    "        restore_best_weights=True,\n",
    "        mode='max',\n",
    "        verbose=1\n",
    "    ),\n",
    "    ModelCheckpoint(\n",
    "        filepath=f'{MODEL_SAVE_PATH}/{MODEL_NAME}_best.keras',\n",
    "        monitor='val_accuracy',\n",
    "        save_best_only=True,\n",
    "        mode='max',\n",
    "        verbose=1\n",
    "    ),\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=5,\n",
    "        min_lr=1e-7,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "print(\"Final model egitimi baslatiliyor (50 epoch)...\")\n",
    "start_training = time.time()\n",
    "\n",
    "history_6 = model_6.fit(\n",
    "    train_dataset_aug,\n",
    "    epochs=50,\n",
    "    validation_data=val_dataset_aug,\n",
    "    callbacks=callbacks_final,\n",
    "    class_weight=class_weights,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "training_time = time.time() - start_training\n",
    "total_time_6 = tuning_time + training_time\n",
    "total_epochs_6 = len(history_6.history['accuracy'])\n",
    "avg_epoch_time_6 = training_time / total_epochs_6\n",
    "\n",
    "print(f\"\\nFinal training complete: {training_time/60:.2f} minutes ({total_epochs_6} epochs)\")\n",
    "print(f\"Total duration: {total_time_6/60:.2f} minutes\")\n",
    "\n",
    "# =============================================================================\n",
    "# TRAINING HISTORY VISUALIZATION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"TRAINING HISTORY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "epochs_range = range(1, len(history_6.history['accuracy']) + 1)\n",
    "\n",
    "# Accuracy Plot\n",
    "axes[0].plot(epochs_range, history_6.history['accuracy'], 'b-', label='Train', linewidth=2)\n",
    "axes[0].plot(epochs_range, history_6.history['val_accuracy'], 'r-', label='Test', linewidth=2)\n",
    "axes[0].set_title(f'{MODEL_NAME}\\nModel Accuracy', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Accuracy')\n",
    "axes[0].legend(loc='lower right')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].set_ylim([0, 1])\n",
    "\n",
    "# Loss Plot\n",
    "axes[1].plot(epochs_range, history_6.history['loss'], 'b-', label='Train', linewidth=2)\n",
    "axes[1].plot(epochs_range, history_6.history['val_loss'], 'r-', label='Test', linewidth=2)\n",
    "axes[1].set_title(f'{MODEL_NAME}\\nModel Loss', fontsize=12, fontweight='bold')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Loss')\n",
    "axes[1].legend(loc='upper right')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{RESULTS_PATH}/{MODEL_NAME}_history.png', dpi=300, bbox_inches='tight', facecolor='white')\n",
    "plt.show()\n",
    "\n",
    "# =============================================================================\n",
    "# MODEL EVALUATION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"MODEL EVALUATION (TEST SET)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "y_pred_proba_6 = model_6.predict(test_dataset_aug, verbose=1)\n",
    "y_pred_6 = np.argmax(y_pred_proba_6, axis=1)\n",
    "\n",
    "y_true_6 = []\n",
    "for _, labels in test_dataset_aug:\n",
    "    y_true_6.extend(np.argmax(labels.numpy(), axis=1))\n",
    "y_true_6 = np.array(y_true_6)\n",
    "\n",
    "# Metrics\n",
    "accuracy_6 = accuracy_score(y_true_6, y_pred_6)\n",
    "precision_6 = precision_score(y_true_6, y_pred_6, average='weighted', zero_division=0)\n",
    "recall_6 = recall_score(y_true_6, y_pred_6, average='weighted', zero_division=0)\n",
    "f1_6 = f1_score(y_true_6, y_pred_6, average='weighted', zero_division=0)\n",
    "kappa_6 = cohen_kappa_score(y_true_6, y_pred_6)\n",
    "\n",
    "print(f\"\"\"\n",
    "Performance Metrics:\n",
    "{'-' * 50}\n",
    "   Accuracy     : {accuracy_6:.4f} ({accuracy_6*100:.2f}%)\n",
    "   Precision    : {precision_6:.4f}\n",
    "   Recall       : {recall_6:.4f}\n",
    "   F1-Score     : {f1_6:.4f}\n",
    "   Cohen Kappa  : {kappa_6:.4f}\n",
    "{'-' * 50}\n",
    "\"\"\")\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_true_6, y_pred_6, target_names=class_names_ordered, zero_division=0))\n",
    "\n",
    "# =============================================================================\n",
    "# CONFUSION MATRIX\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"CONFUSION MATRIX\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "cm_6 = confusion_matrix(y_true_6, y_pred_6)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm_6, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=class_names_ordered, yticklabels=class_names_ordered,\n",
    "            annot_kws={'size': 14, 'fontweight': 'bold'})\n",
    "plt.title(f'{MODEL_NAME}\\nConfusion Matrix', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{RESULTS_PATH}/{MODEL_NAME}_confusion_matrix.png', dpi=300, bbox_inches='tight', facecolor='white')\n",
    "plt.show()\n",
    "\n",
    "# =============================================================================\n",
    "# ROC CURVES\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"ROC CURVES\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "y_true_bin_6 = label_binarize(y_true_6, classes=range(NUM_CLASSES))\n",
    "fpr_6, tpr_6, roc_auc_6 = {}, {}, {}\n",
    "\n",
    "for i in range(NUM_CLASSES):\n",
    "    fpr_6[i], tpr_6[i], _ = roc_curve(y_true_bin_6[:, i], y_pred_proba_6[:, i])\n",
    "    roc_auc_6[i] = auc(fpr_6[i], tpr_6[i])\n",
    "\n",
    "fpr_6[\"micro\"], tpr_6[\"micro\"], _ = roc_curve(y_true_bin_6.ravel(), y_pred_proba_6.ravel())\n",
    "roc_auc_6[\"micro\"] = auc(fpr_6[\"micro\"], tpr_6[\"micro\"])\n",
    "\n",
    "all_fpr_6 = np.unique(np.concatenate([fpr_6[i] for i in range(NUM_CLASSES)]))\n",
    "mean_tpr_6 = np.zeros_like(all_fpr_6)\n",
    "for i in range(NUM_CLASSES):\n",
    "    mean_tpr_6 += np.interp(all_fpr_6, fpr_6[i], tpr_6[i])\n",
    "mean_tpr_6 /= NUM_CLASSES\n",
    "fpr_6[\"macro\"], tpr_6[\"macro\"] = all_fpr_6, mean_tpr_6\n",
    "roc_auc_6[\"macro\"] = auc(fpr_6[\"macro\"], tpr_6[\"macro\"])\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "colors = ['#e41a1c', '#377eb8', '#4daf4a', '#984ea3', '#ff7f00']\n",
    "for i, (name, color) in enumerate(zip(class_names_ordered, colors)):\n",
    "    plt.plot(fpr_6[i], tpr_6[i], color=color, lw=2, label=f'{name} (AUC={roc_auc_6[i]:.4f})')\n",
    "plt.plot(fpr_6[\"micro\"], tpr_6[\"micro\"], 'deeppink', ls=':', lw=3, label=f'Micro-avg (AUC={roc_auc_6[\"micro\"]:.4f})')\n",
    "plt.plot(fpr_6[\"macro\"], tpr_6[\"macro\"], 'navy', ls=':', lw=3, label=f'Macro-avg (AUC={roc_auc_6[\"macro\"]:.4f})')\n",
    "plt.plot([0,1], [0,1], 'k--', lw=2)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title(f'{MODEL_NAME}\\nROC Curves', fontsize=14, fontweight='bold')\n",
    "plt.legend(loc='lower right', fontsize=9)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{RESULTS_PATH}/{MODEL_NAME}_roc_curves.png', dpi=300, bbox_inches='tight', facecolor='white')\n",
    "plt.show()\n",
    "\n",
    "print(\"ROC AUC Scores:\")\n",
    "for i, name in enumerate(class_names_ordered):\n",
    "    print(f\"   {name}: {roc_auc_6[i]:.4f}\")\n",
    "print(f\"   Macro-average: {roc_auc_6['macro']:.4f}\")\n",
    "print(f\"   Micro-average: {roc_auc_6['micro']:.4f}\")\n",
    "\n",
    "# =============================================================================\n",
    "# SAVE RESULTS\n",
    "# =============================================================================\n",
    "\n",
    "results_6 = {\n",
    "    'model_name': MODEL_NAME,\n",
    "    'accuracy': accuracy_6,\n",
    "    'precision': precision_6,\n",
    "    'recall': recall_6,\n",
    "    'f1_score': f1_6,\n",
    "    'kappa': kappa_6,\n",
    "    'roc_auc_macro': roc_auc_6['macro'],\n",
    "    'roc_auc_micro': roc_auc_6['micro'],\n",
    "    'training_time_min': total_time_6 / 60,\n",
    "    'avg_epoch_time_sec': avg_epoch_time_6,\n",
    "    'total_epochs': total_epochs_6,\n",
    "    'total_params': total_params_6,\n",
    "    'best_lr': best_hps.get('learning_rate'),\n",
    "    'best_dropout1': best_hps.get('dropout_1'),\n",
    "    'best_dropout2': best_hps.get('dropout_2'),\n",
    "    'best_optimizer': best_hps.get('optimizer')\n",
    "}\n",
    "\n",
    "for i, name in enumerate(class_names_ordered):\n",
    "    results_6[f'auc_{name}'] = roc_auc_6[i]\n",
    "\n",
    "all_results.append(results_6)\n",
    "\n",
    "# Save model\n",
    "model_6.save(f'{MODEL_SAVE_PATH}/{MODEL_NAME}_final.keras')\n",
    "pd.DataFrame(history_6.history).to_csv(f'{RESULTS_PATH}/{MODEL_NAME}_history.csv', index=False)\n",
    "\n",
    "print(f\"\\nModel saved: {MODEL_SAVE_PATH}/{MODEL_NAME}_final.keras\")\n",
    "\n",
    "# =============================================================================\n",
    "# MODEL COMPARISON TABLE\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"MODEL COMPARISON TABLE\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "print(f\"\\n{'Method':<50} {'Precision':>10} {'Recall':>10} {'F1-Score':>10} {'Accuracy':>10} {'Avg Epoch(s)':>12}\")\n",
    "print(\"-\" * 105)\n",
    "\n",
    "for r in all_results:\n",
    "    avg_epoch = r.get('avg_epoch_time_sec', 0)\n",
    "    print(f\"{r['model_name']:<50} {r['precision']:>10.4f} {r['recall']:>10.4f} {r['f1_score']:>10.4f} {r['accuracy']:>10.4f} {avg_epoch:>12.2f}\")\n",
    "\n",
    "print(\"-\" * 105)\n",
    "\n",
    "best = max(all_results, key=lambda x: x['accuracy'])\n",
    "print(f\"\\nBest Model: {best['model_name']} (Accuracy: {best['accuracy']*100:.2f}%)\")\n",
    "\n",
    "# =============================================================================\n",
    "# MODEL SUMMARY REPORT\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"MODEL 6 - SUMMARY REPORT\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\"\"\n",
    "Model Information:\n",
    "   Name            : {MODEL_NAME}\n",
    "   Type            : Hybrid (Dual-Backbone)\n",
    "   Backbone 1      : EfficientNetB0 (FROZEN)\n",
    "   Backbone 2      : MobileNetV2 (FROZEN)\n",
    "   Total Params    : {total_params_6:,}\n",
    "   Trainable       : {trainable_params_6:,}\n",
    "\n",
    "Optimal Hyperparameters:\n",
    "   Learning Rate   : {best_hps.get('learning_rate')}\n",
    "   Dropout 1       : {best_hps.get('dropout_1')}\n",
    "   Dropout 2       : {best_hps.get('dropout_2')}\n",
    "   Dense Units 1   : {best_hps.get('dense_units_1')}\n",
    "   Dense Units 2   : {best_hps.get('dense_units_2')}\n",
    "   Optimizer       : {best_hps.get('optimizer')}\n",
    "\n",
    "Performance Metrics:\n",
    "   Accuracy        : {accuracy_6*100:.2f}%\n",
    "   Precision       : {precision_6:.4f}\n",
    "   Recall          : {recall_6:.4f}\n",
    "   F1-Score        : {f1_6:.4f}\n",
    "   Cohen Kappa     : {kappa_6:.4f}\n",
    "   ROC AUC (Macro) : {roc_auc_6['macro']:.4f}\n",
    "\n",
    "Training Time:\n",
    "   Tuning          : {tuning_time/60:.2f} minutes\n",
    "   Final Training  : {training_time/60:.2f} minutes\n",
    "   Total           : {total_time_6/60:.2f} minutes\n",
    "\n",
    "Saved Files:\n",
    "   - {MODEL_SAVE_PATH}/{MODEL_NAME}_final.keras\n",
    "   - {MODEL_SAVE_PATH}/{MODEL_NAME}_best.keras\n",
    "   - {RESULTS_PATH}/{MODEL_NAME}_history.csv\n",
    "   - {RESULTS_PATH}/{MODEL_NAME}_history.png\n",
    "   - {RESULTS_PATH}/{MODEL_NAME}_confusion_matrix.png\n",
    "   - {RESULTS_PATH}/{MODEL_NAME}_roc_curves.png\n",
    "\"\"\")\n",
    "\n",
    "gc.collect()\n",
    "print(\"=\" * 70)\n",
    "print(\"MODEL 6 COMPLETED SUCCESSFULLY\")\n",
    "print(\"=\" * 70)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "_NukSv0VI9N3",
    "outputId": "7d46b1cb-1c35-4d0e-c456-0cba77563613"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Cell 19: MODEL 7 - Xception + InceptionResNetV2 (HYBRID)"
   ],
   "metadata": {
    "id": "f3HynZSAx-NF"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# =============================================================================\n",
    "# CELL 19: MODEL 7 - Xception + InceptionResNetV2 (HYBRID MODEL)\n",
    "# =============================================================================\n",
    "# This model combines two powerful and modern CNN architectures:\n",
    "# - Xception: Depthwise separable convolutions, efficient architecture\n",
    "# - InceptionResNetV2: Inception modules + Residual connections\n",
    "#\n",
    "# NOTE: These models require 299x299 input, so the dataset is re-created\n",
    "# olusturulacak. Mevcut train/val/test split'leri korunacak.\n",
    "# =============================================================================\n",
    "\n",
    "import time\n",
    "import keras_tuner as kt\n",
    "from tensorflow.keras.applications import Xception, InceptionResNetV2\n",
    "from tensorflow.keras.layers import (Input, GlobalAveragePooling2D, Dense,\n",
    "                                      Dropout, Concatenate)\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam, SGD, RMSprop\n",
    "\n",
    "MODEL_NAME = \"Model_07_Xception_InceptionResNetV2_Hybrid\"\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"MODEL 7: Xception + InceptionResNetV2 (HYBRID)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\"\"\n",
    "Model Mimarisi:\n",
    "\n",
    "    Input Image (299x299x3)\n",
    "           |\n",
    "    +------+------+\n",
    "    |             |\n",
    "  Xception   InceptionResNetV2\n",
    "  (FROZEN)      (FROZEN)\n",
    "    |             |\n",
    "   GAP           GAP\n",
    "    |             |\n",
    "Dense(256)    Dense(256)\n",
    "Dropout       Dropout\n",
    "Dense(128)    Dense(128)\n",
    "Dropout       Dropout\n",
    "    |             |\n",
    "    +------+------+\n",
    "           |\n",
    "      Concatenate\n",
    "           |\n",
    "      Dense(256)\n",
    "      Dropout\n",
    "      Dense(128)\n",
    "      Dropout\n",
    "           |\n",
    "      Dense(5) + Softmax\n",
    "           |\n",
    "        Output\n",
    "\n",
    "Backbone Ozellikleri:\n",
    "- Xception: 22.9M parametre, depthwise separable convolutions\n",
    "- InceptionResNetV2: 55.9M parametre, inception + residual\n",
    "- Input Size: 299x299 (native resolution for both models)\n",
    "\"\"\")\n",
    "\n",
    "# =============================================================================\n",
    "# 299x299 DATASET OLUSTURMA\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n299x299 Dataset olusturuluyor...\")\n",
    "\n",
    "IMG_SIZE_299 = 299\n",
    "IMG_SHAPE_299 = (IMG_SIZE_299, IMG_SIZE_299, 3)\n",
    "\n",
    "# Augmentation layer for 299x299\n",
    "augmentation_layer_299 = tf.keras.Sequential([\n",
    "    tf.keras.layers.RandomFlip(\"horizontal\"),\n",
    "    tf.keras.layers.RandomRotation(0.15),\n",
    "    tf.keras.layers.RandomZoom(0.15),\n",
    "    tf.keras.layers.RandomContrast(0.1),\n",
    "    tf.keras.layers.RandomBrightness(0.1),\n",
    "], name='augmentation_299')\n",
    "\n",
    "# Resize function\n",
    "def resize_to_299(image, label):\n",
    "    image = tf.image.resize(image, [IMG_SIZE_299, IMG_SIZE_299])\n",
    "    return image, label\n",
    "\n",
    "# Train dataset 299x299 (augmented)\n",
    "train_dataset_299 = train_dataset.map(resize_to_299, num_parallel_calls=AUTOTUNE)\n",
    "train_dataset_299 = train_dataset_299.map(\n",
    "    lambda x, y: (augmentation_layer_299(x, training=True), y),\n",
    "    num_parallel_calls=AUTOTUNE\n",
    ")\n",
    "train_dataset_299 = train_dataset_299.prefetch(AUTOTUNE)\n",
    "\n",
    "# Validation dataset 299x299\n",
    "val_dataset_299 = val_dataset.map(resize_to_299, num_parallel_calls=AUTOTUNE)\n",
    "val_dataset_299 = val_dataset_299.prefetch(AUTOTUNE)\n",
    "\n",
    "# Test dataset 299x299\n",
    "test_dataset_299 = test_dataset.map(resize_to_299, num_parallel_calls=AUTOTUNE)\n",
    "test_dataset_299 = test_dataset_299.prefetch(AUTOTUNE)\n",
    "\n",
    "print(\"299x299 Dataset hazir!\")\n",
    "\n",
    "# =============================================================================\n",
    "# HYBRID MODEL BUILDER (FOR KERAS TUNER)\n",
    "# =============================================================================\n",
    "\n",
    "def build_xception_inceptionresnet_hybrid(hp):\n",
    "    \"\"\"\n",
    "    Xception + InceptionResNetV2 Hybrid model builder.\n",
    "    Keras Tuner ile hyperparameter optimization.\n",
    "    \"\"\"\n",
    "\n",
    "    # Hyperparameters\n",
    "    learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
    "    dropout_1 = hp.Choice('dropout_1', values=[0.2, 0.3, 0.4])\n",
    "    dropout_2 = hp.Choice('dropout_2', values=[0.4, 0.5, 0.6])\n",
    "    dense_units_1 = hp.Choice('dense_units_1', values=[256, 512])\n",
    "    dense_units_2 = hp.Choice('dense_units_2', values=[128, 256])\n",
    "    optimizer_name = hp.Choice('optimizer', values=['adam', 'sgd', 'rmsprop'])\n",
    "\n",
    "    # Input\n",
    "    inputs = Input(shape=IMG_SHAPE_299, name='input_layer')\n",
    "\n",
    "    # =========================================================================\n",
    "    # BACKBONE 1: Xception\n",
    "    # =========================================================================\n",
    "    xception = Xception(\n",
    "        include_top=False,\n",
    "        weights='imagenet',\n",
    "        input_shape=IMG_SHAPE_299\n",
    "    )\n",
    "    xception.trainable = False  # FROZEN\n",
    "\n",
    "    # Xception branch\n",
    "    x1 = tf.keras.applications.xception.preprocess_input(inputs)\n",
    "    x1 = xception(x1, training=False)\n",
    "    x1 = GlobalAveragePooling2D(name='gap_xception')(x1)\n",
    "    x1 = Dense(dense_units_1, activation='relu', name='dense1_xception')(x1)\n",
    "    x1 = Dropout(dropout_1, name='drop1_xception')(x1)\n",
    "    x1 = Dense(dense_units_2, activation='relu', name='dense2_xception')(x1)\n",
    "    x1 = Dropout(dropout_2, name='drop2_xception')(x1)\n",
    "\n",
    "    # =========================================================================\n",
    "    # BACKBONE 2: InceptionResNetV2\n",
    "    # =========================================================================\n",
    "    inception_resnet = InceptionResNetV2(\n",
    "        include_top=False,\n",
    "        weights='imagenet',\n",
    "        input_shape=IMG_SHAPE_299\n",
    "    )\n",
    "    inception_resnet.trainable = False  # FROZEN\n",
    "\n",
    "    # InceptionResNetV2 branch\n",
    "    x2 = tf.keras.applications.inception_resnet_v2.preprocess_input(inputs)\n",
    "    x2 = inception_resnet(x2, training=False)\n",
    "    x2 = GlobalAveragePooling2D(name='gap_inception_resnet')(x2)\n",
    "    x2 = Dense(dense_units_1, activation='relu', name='dense1_incres')(x2)\n",
    "    x2 = Dropout(dropout_1, name='drop1_incres')(x2)\n",
    "    x2 = Dense(dense_units_2, activation='relu', name='dense2_incres')(x2)\n",
    "    x2 = Dropout(dropout_2, name='drop2_incres')(x2)\n",
    "\n",
    "    # =========================================================================\n",
    "    # CONCATENATE & FINAL CLASSIFICATION\n",
    "    # =========================================================================\n",
    "    merged = Concatenate(name='concat')([x1, x2])\n",
    "\n",
    "    # Final classification head\n",
    "    x = Dense(dense_units_1, activation='relu', name='dense1_final')(merged)\n",
    "    x = Dropout(dropout_1, name='drop1_final')(x)\n",
    "    x = Dense(dense_units_2, activation='relu', name='dense2_final')(x)\n",
    "    x = Dropout(dropout_2, name='drop2_final')(x)\n",
    "\n",
    "    # Output\n",
    "    outputs = Dense(NUM_CLASSES, activation='softmax', name='output')(x)\n",
    "\n",
    "    # Model\n",
    "    model = Model(inputs=inputs, outputs=outputs, name='Xception_InceptionResNetV2_Hybrid')\n",
    "\n",
    "    # Optimizer\n",
    "    if optimizer_name == 'adam':\n",
    "        optimizer = Adam(learning_rate=learning_rate)\n",
    "    elif optimizer_name == 'sgd':\n",
    "        optimizer = SGD(learning_rate=learning_rate, momentum=0.9)\n",
    "    else:\n",
    "        optimizer = RMSprop(learning_rate=learning_rate)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "# =============================================================================\n",
    "# KERAS TUNER - HYPERPARAMETER OPTIMIZATION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"HYPERPARAMETER OPTIMIZATION (KERAS TUNER)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "tuner_dir = f'{MODEL_SAVE_PATH}/tuner_{MODEL_NAME}'\n",
    "\n",
    "tuner = kt.BayesianOptimization(\n",
    "    build_xception_inceptionresnet_hybrid,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=15,\n",
    "    num_initial_points=5,\n",
    "    directory=tuner_dir,\n",
    "    project_name='xception_incresnet_tuning',\n",
    "    overwrite=True\n",
    ")\n",
    "\n",
    "print(\"\"\"\n",
    "Tuner Configuration:\n",
    "   - Algorithm      : Bayesian Optimization\n",
    "   - Max Trials     : 15\n",
    "   - Objective      : val_accuracy (maximize)\n",
    "\n",
    "Hyperparameter Search Space:\n",
    "   - Learning Rate  : [0.01, 0.001, 0.0001]\n",
    "   - Dropout 1      : [0.2, 0.3, 0.4]\n",
    "   - Dropout 2      : [0.4, 0.5, 0.6]\n",
    "   - Dense Units 1  : [256, 512]\n",
    "   - Dense Units 2  : [128, 256]\n",
    "   - Optimizer      : [Adam, SGD, RMSprop]\n",
    "\"\"\")\n",
    "\n",
    "early_stop_tuner = EarlyStopping(\n",
    "    monitor='val_accuracy',\n",
    "    patience=5,\n",
    "    restore_best_weights=True,\n",
    "    mode='max'\n",
    ")\n",
    "\n",
    "print(\"Hyperparameter search starting...\")\n",
    "start_tuning = time.time()\n",
    "\n",
    "tuner.search(\n",
    "    train_dataset_299,\n",
    "    epochs=20,\n",
    "    validation_data=val_dataset_299,\n",
    "    callbacks=[early_stop_tuner],\n",
    "    class_weight=class_weights,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "tuning_time = time.time() - start_tuning\n",
    "print(f\"\\nTuning completed: {tuning_time/60:.2f} minutes\")\n",
    "\n",
    "# =============================================================================\n",
    "# OPTIMAL HYPERPARAMETERS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"OPTIMAL HYPERPARAMETERS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "print(f\"\"\"\n",
    "Optimal Values Found:\n",
    "   - Learning Rate  : {best_hps.get('learning_rate')}\n",
    "   - Dropout 1      : {best_hps.get('dropout_1')}\n",
    "   - Dropout 2      : {best_hps.get('dropout_2')}\n",
    "   - Dense Units 1  : {best_hps.get('dense_units_1')}\n",
    "   - Dense Units 2  : {best_hps.get('dense_units_2')}\n",
    "   - Optimizer      : {best_hps.get('optimizer')}\n",
    "\"\"\")\n",
    "\n",
    "# =============================================================================\n",
    "# FINAL MODEL TRAINING\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"FINAL MODEL TRAINING\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "model_7 = tuner.hypermodel.build(best_hps)\n",
    "\n",
    "total_params_7 = model_7.count_params()\n",
    "trainable_params_7 = sum([tf.keras.backend.count_params(w) for w in model_7.trainable_weights])\n",
    "\n",
    "print(f\"\"\"\n",
    "Model Parameters:\n",
    "   - Total          : {total_params_7:,}\n",
    "   - Trainable      : {trainable_params_7:,}\n",
    "   - Frozen         : {total_params_7 - trainable_params_7:,}\n",
    "\"\"\")\n",
    "\n",
    "callbacks_final = [\n",
    "    EarlyStopping(\n",
    "        monitor='val_accuracy',\n",
    "        patience=15,\n",
    "        restore_best_weights=True,\n",
    "        mode='max',\n",
    "        verbose=1\n",
    "    ),\n",
    "    ModelCheckpoint(\n",
    "        filepath=f'{MODEL_SAVE_PATH}/{MODEL_NAME}_best.keras',\n",
    "        monitor='val_accuracy',\n",
    "        save_best_only=True,\n",
    "        mode='max',\n",
    "        verbose=1\n",
    "    ),\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=5,\n",
    "        min_lr=1e-7,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "print(\"Final model training starting (50 epochs)...\")\n",
    "start_training = time.time()\n",
    "\n",
    "history_7 = model_7.fit(\n",
    "    train_dataset_299,\n",
    "    epochs=50,\n",
    "    validation_data=val_dataset_299,\n",
    "    callbacks=callbacks_final,\n",
    "    class_weight=class_weights,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "training_time = time.time() - start_training\n",
    "total_time_7 = tuning_time + training_time\n",
    "total_epochs_7 = len(history_7.history['accuracy'])\n",
    "avg_epoch_time_7 = training_time / total_epochs_7\n",
    "\n",
    "print(f\"\\nFinal training completed: {training_time/60:.2f} minutes ({total_epochs_7} epochs)\")\n",
    "print(f\"Total time: {total_time_7/60:.2f} minutes\")\n",
    "\n",
    "# =============================================================================\n",
    "# TRAINING HISTORY VISUALIZATION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"TRAINING HISTORY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "epochs_range = range(1, len(history_7.history['accuracy']) + 1)\n",
    "\n",
    "# Accuracy Plot\n",
    "axes[0].plot(epochs_range, history_7.history['accuracy'], 'b-', label='Train', linewidth=2)\n",
    "axes[0].plot(epochs_range, history_7.history['val_accuracy'], 'r-', label='Test', linewidth=2)\n",
    "axes[0].set_title(f'{MODEL_NAME}\\nModel Accuracy', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Accuracy')\n",
    "axes[0].legend(loc='lower right')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].set_ylim([0, 1])\n",
    "\n",
    "# Loss Plot\n",
    "axes[1].plot(epochs_range, history_7.history['loss'], 'b-', label='Train', linewidth=2)\n",
    "axes[1].plot(epochs_range, history_7.history['val_loss'], 'r-', label='Test', linewidth=2)\n",
    "axes[1].set_title(f'{MODEL_NAME}\\nModel Loss', fontsize=12, fontweight='bold')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Loss')\n",
    "axes[1].legend(loc='upper right')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{RESULTS_PATH}/{MODEL_NAME}_history.png', dpi=300, bbox_inches='tight', facecolor='white')\n",
    "plt.show()\n",
    "\n",
    "# =============================================================================\n",
    "# MODEL EVALUATION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"MODEL EVALUATION (TEST SET)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "y_pred_proba_7 = model_7.predict(test_dataset_299, verbose=1)\n",
    "y_pred_7 = np.argmax(y_pred_proba_7, axis=1)\n",
    "\n",
    "y_true_7 = []\n",
    "for _, labels in test_dataset_299:\n",
    "    y_true_7.extend(np.argmax(labels.numpy(), axis=1))\n",
    "y_true_7 = np.array(y_true_7)\n",
    "\n",
    "# Metrics\n",
    "accuracy_7 = accuracy_score(y_true_7, y_pred_7)\n",
    "precision_7 = precision_score(y_true_7, y_pred_7, average='weighted', zero_division=0)\n",
    "recall_7 = recall_score(y_true_7, y_pred_7, average='weighted', zero_division=0)\n",
    "f1_7 = f1_score(y_true_7, y_pred_7, average='weighted', zero_division=0)\n",
    "kappa_7 = cohen_kappa_score(y_true_7, y_pred_7)\n",
    "\n",
    "print(f\"\"\"\n",
    "Performance Metrics:\n",
    "{'-' * 50}\n",
    "   Accuracy     : {accuracy_7:.4f} ({accuracy_7*100:.2f}%)\n",
    "   Precision    : {precision_7:.4f}\n",
    "   Recall       : {recall_7:.4f}\n",
    "   F1-Score     : {f1_7:.4f}\n",
    "   Cohen Kappa  : {kappa_7:.4f}\n",
    "{'-' * 50}\n",
    "\"\"\")\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_true_7, y_pred_7, target_names=class_names_ordered, zero_division=0))\n",
    "\n",
    "# =============================================================================\n",
    "# CONFUSION MATRIX\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"CONFUSION MATRIX\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "cm_7 = confusion_matrix(y_true_7, y_pred_7)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm_7, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=class_names_ordered, yticklabels=class_names_ordered,\n",
    "            annot_kws={'size': 14, 'fontweight': 'bold'})\n",
    "plt.title(f'{MODEL_NAME}\\nConfusion Matrix', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{RESULTS_PATH}/{MODEL_NAME}_confusion_matrix.png', dpi=300, bbox_inches='tight', facecolor='white')\n",
    "plt.show()\n",
    "\n",
    "# =============================================================================\n",
    "# ROC CURVES\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"ROC CURVES\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "y_true_bin_7 = label_binarize(y_true_7, classes=range(NUM_CLASSES))\n",
    "fpr_7, tpr_7, roc_auc_7 = {}, {}, {}\n",
    "\n",
    "for i in range(NUM_CLASSES):\n",
    "    fpr_7[i], tpr_7[i], _ = roc_curve(y_true_bin_7[:, i], y_pred_proba_7[:, i])\n",
    "    roc_auc_7[i] = auc(fpr_7[i], tpr_7[i])\n",
    "\n",
    "fpr_7[\"micro\"], tpr_7[\"micro\"], _ = roc_curve(y_true_bin_7.ravel(), y_pred_proba_7.ravel())\n",
    "roc_auc_7[\"micro\"] = auc(fpr_7[\"micro\"], tpr_7[\"micro\"])\n",
    "\n",
    "all_fpr_7 = np.unique(np.concatenate([fpr_7[i] for i in range(NUM_CLASSES)]))\n",
    "mean_tpr_7 = np.zeros_like(all_fpr_7)\n",
    "for i in range(NUM_CLASSES):\n",
    "    mean_tpr_7 += np.interp(all_fpr_7, fpr_7[i], tpr_7[i])\n",
    "mean_tpr_7 /= NUM_CLASSES\n",
    "fpr_7[\"macro\"], tpr_7[\"macro\"] = all_fpr_7, mean_tpr_7\n",
    "roc_auc_7[\"macro\"] = auc(fpr_7[\"macro\"], tpr_7[\"macro\"])\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "colors = ['#e41a1c', '#377eb8', '#4daf4a', '#984ea3', '#ff7f00']\n",
    "for i, (name, color) in enumerate(zip(class_names_ordered, colors)):\n",
    "    plt.plot(fpr_7[i], tpr_7[i], color=color, lw=2, label=f'{name} (AUC={roc_auc_7[i]:.4f})')\n",
    "plt.plot(fpr_7[\"micro\"], tpr_7[\"micro\"], 'deeppink', ls=':', lw=3, label=f'Micro-avg (AUC={roc_auc_7[\"micro\"]:.4f})')\n",
    "plt.plot(fpr_7[\"macro\"], tpr_7[\"macro\"], 'navy', ls=':', lw=3, label=f'Macro-avg (AUC={roc_auc_7[\"macro\"]:.4f})')\n",
    "plt.plot([0,1], [0,1], 'k--', lw=2)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title(f'{MODEL_NAME}\\nROC Curves', fontsize=14, fontweight='bold')\n",
    "plt.legend(loc='lower right', fontsize=9)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{RESULTS_PATH}/{MODEL_NAME}_roc_curves.png', dpi=300, bbox_inches='tight', facecolor='white')\n",
    "plt.show()\n",
    "\n",
    "print(\"ROC AUC Scores:\")\n",
    "for i, name in enumerate(class_names_ordered):\n",
    "    print(f\"   {name}: {roc_auc_7[i]:.4f}\")\n",
    "print(f\"   Macro-average: {roc_auc_7['macro']:.4f}\")\n",
    "print(f\"   Micro-average: {roc_auc_7['micro']:.4f}\")\n",
    "\n",
    "# =============================================================================\n",
    "# SAVE RESULTS\n",
    "# =============================================================================\n",
    "\n",
    "results_7 = {\n",
    "    'model_name': MODEL_NAME,\n",
    "    'accuracy': accuracy_7,\n",
    "    'precision': precision_7,\n",
    "    'recall': recall_7,\n",
    "    'f1_score': f1_7,\n",
    "    'kappa': kappa_7,\n",
    "    'roc_auc_macro': roc_auc_7['macro'],\n",
    "    'roc_auc_micro': roc_auc_7['micro'],\n",
    "    'training_time_min': total_time_7 / 60,\n",
    "    'avg_epoch_time_sec': avg_epoch_time_7,\n",
    "    'total_epochs': total_epochs_7,\n",
    "    'total_params': total_params_7,\n",
    "    'best_lr': best_hps.get('learning_rate'),\n",
    "    'best_dropout1': best_hps.get('dropout_1'),\n",
    "    'best_dropout2': best_hps.get('dropout_2'),\n",
    "    'best_optimizer': best_hps.get('optimizer')\n",
    "}\n",
    "\n",
    "for i, name in enumerate(class_names_ordered):\n",
    "    results_7[f'auc_{name}'] = roc_auc_7[i]\n",
    "\n",
    "all_results.append(results_7)\n",
    "\n",
    "# Save model\n",
    "model_7.save(f'{MODEL_SAVE_PATH}/{MODEL_NAME}_final.keras')\n",
    "pd.DataFrame(history_7.history).to_csv(f'{RESULTS_PATH}/{MODEL_NAME}_history.csv', index=False)\n",
    "\n",
    "print(f\"\\nModel saved: {MODEL_SAVE_PATH}/{MODEL_NAME}_final.keras\")\n",
    "\n",
    "# =============================================================================\n",
    "# MODEL COMPARISON TABLE\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"MODEL COMPARISON TABLE\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "print(f\"\\n{'Method':<50} {'Precision':>10} {'Recall':>10} {'F1-Score':>10} {'Accuracy':>10} {'Avg Epoch(s)':>12}\")\n",
    "print(\"-\" * 105)\n",
    "\n",
    "for r in all_results:\n",
    "    avg_epoch = r.get('avg_epoch_time_sec', 0)\n",
    "    print(f\"{r['model_name']:<50} {r['precision']:>10.4f} {r['recall']:>10.4f} {r['f1_score']:>10.4f} {r['accuracy']:>10.4f} {avg_epoch:>12.2f}\")\n",
    "\n",
    "print(\"-\" * 105)\n",
    "\n",
    "best = max(all_results, key=lambda x: x['accuracy'])\n",
    "print(f\"\\nBest Model: {best['model_name']} (Accuracy: {best['accuracy']*100:.2f}%)\")\n",
    "\n",
    "# =============================================================================\n",
    "# MODEL SUMMARY REPORT\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"MODEL 7 - SUMMARY REPORT\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\"\"\n",
    "Model Information:\n",
    "   Name            : {MODEL_NAME}\n",
    "   Type            : Hybrid (Dual-Backbone)\n",
    "   Backbone 1      : Xception (FROZEN)\n",
    "   Backbone 2      : InceptionResNetV2 (FROZEN)\n",
    "   Input Size      : 299x299x3\n",
    "   Total Params    : {total_params_7:,}\n",
    "   Trainable       : {trainable_params_7:,}\n",
    "\n",
    "Optimal Hyperparameters:\n",
    "   Learning Rate   : {best_hps.get('learning_rate')}\n",
    "   Dropout 1       : {best_hps.get('dropout_1')}\n",
    "   Dropout 2       : {best_hps.get('dropout_2')}\n",
    "   Dense Units 1   : {best_hps.get('dense_units_1')}\n",
    "   Dense Units 2   : {best_hps.get('dense_units_2')}\n",
    "   Optimizer       : {best_hps.get('optimizer')}\n",
    "\n",
    "Performance Metrics:\n",
    "   Accuracy        : {accuracy_7*100:.2f}%\n",
    "   Precision       : {precision_7:.4f}\n",
    "   Recall          : {recall_7:.4f}\n",
    "   F1-Score        : {f1_7:.4f}\n",
    "   Cohen Kappa     : {kappa_7:.4f}\n",
    "   ROC AUC (Macro) : {roc_auc_7['macro']:.4f}\n",
    "\n",
    "Training Time:\n",
    "   Tuning          : {tuning_time/60:.2f} minutes\n",
    "   Final Training  : {training_time/60:.2f} minutes\n",
    "   Total           : {total_time_7/60:.2f} minutes\n",
    "\"\"\")\n",
    "\n",
    "gc.collect()\n",
    "print(\"=\" * 70)\n",
    "print(\"MODEL 7 COMPLETED SUCCESSFULLY\")\n",
    "print(\"=\" * 70)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "tdwml-YHyFRG",
    "outputId": "f370536d-6f51-4594-c4bf-d2b6fb65f97c"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Cell 20: MODEL 8 - InceptionResNetV2 + CBAM (ATTENTION)"
   ],
   "metadata": {
    "id": "zzG_ukcWLGi7"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# =============================================================================\n",
    "# CELL 20: MODEL 8 - InceptionResNetV2 + CBAM (ATTENTION MODEL)\n",
    "# =============================================================================\n",
    "# This model combines transfer learning with attention mechanisms:\n",
    "# - InceptionResNetV2: Guclu feature extraction\n",
    "# - CBAM: Convolutional Block Attention Module (Channel + Spatial Attention)\n",
    "#\n",
    "# CBAM enables the model to focus on salient regions.\n",
    "# Augmentation oranlari 0.10'a dusurulmustur (daha stabil egitim icin).\n",
    "# =============================================================================\n",
    "\n",
    "import time\n",
    "import keras_tuner as kt\n",
    "from tensorflow.keras.applications import InceptionResNetV2\n",
    "from tensorflow.keras.layers import (Input, GlobalAveragePooling2D, Dense,\n",
    "                                      Dropout, Multiply, Add, Activation,\n",
    "                                      Reshape, Concatenate, Conv2D,\n",
    "                                      GlobalMaxPooling2D, Lambda)\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam, SGD, RMSprop\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "MODEL_NAME = \"Model_08_InceptionResNetV2_CBAM\"\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"MODEL 8: InceptionResNetV2 + CBAM (ATTENTION)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\"\"\n",
    "Model Mimarisi:\n",
    "\n",
    "    Input Image (299x299x3)\n",
    "           |\n",
    "    InceptionResNetV2\n",
    "        (FROZEN)\n",
    "           |\n",
    "        CBAM\n",
    "    (Attention Module)\n",
    "           |\n",
    "    +------+------+\n",
    "    |             |\n",
    " Channel      Spatial\n",
    " Attention    Attention\n",
    "    |             |\n",
    "    +------+------+\n",
    "           |\n",
    "          GAP\n",
    "           |\n",
    "      Dense(512)\n",
    "      Dropout\n",
    "      Dense(256)\n",
    "      Dropout\n",
    "           |\n",
    "      Dense(5) + Softmax\n",
    "           |\n",
    "        Output\n",
    "\n",
    "CBAM (Convolutional Block Attention Module):\n",
    "- Channel Attention: \"Ne\" onemli (feature maps)\n",
    "- Spatial Attention: \"Nerede\" onemli (lokasyon)\n",
    "- Sirasiyla uygulanir: Channel -> Spatial\n",
    "\"\"\")\n",
    "\n",
    "# =============================================================================\n",
    "# 299x299 DATASET (DUSUK AUGMENTATION)\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n299x299 Dataset olusturuluyor (dusuk augmentation)...\")\n",
    "\n",
    "IMG_SIZE_299 = 299\n",
    "IMG_SHAPE_299 = (IMG_SIZE_299, IMG_SIZE_299, 3)\n",
    "\n",
    "# Dusurulmus augmentation layer (0.15 -> 0.10)\n",
    "augmentation_layer_low = tf.keras.Sequential([\n",
    "    tf.keras.layers.RandomFlip(\"horizontal\"),\n",
    "    tf.keras.layers.RandomRotation(0.10),  # 0.15'ten 0.10'a dusuruldu\n",
    "    tf.keras.layers.RandomZoom(0.10),       # 0.15'ten 0.10'a dusuruldu\n",
    "    tf.keras.layers.RandomContrast(0.05),   # 0.10'dan 0.05'e dusuruldu\n",
    "    tf.keras.layers.RandomBrightness(0.05), # 0.10'dan 0.05'e dusuruldu\n",
    "], name='augmentation_low')\n",
    "\n",
    "# Resize function\n",
    "def resize_to_299(image, label):\n",
    "    image = tf.image.resize(image, [IMG_SIZE_299, IMG_SIZE_299])\n",
    "    return image, label\n",
    "\n",
    "# Train dataset 299x299 (dusuk augmentation)\n",
    "train_dataset_299_low = train_dataset.map(resize_to_299, num_parallel_calls=AUTOTUNE)\n",
    "train_dataset_299_low = train_dataset_299_low.map(\n",
    "    lambda x, y: (augmentation_layer_low(x, training=True), y),\n",
    "    num_parallel_calls=AUTOTUNE\n",
    ")\n",
    "train_dataset_299_low = train_dataset_299_low.prefetch(AUTOTUNE)\n",
    "\n",
    "# Validation dataset 299x299 (augmentation yok)\n",
    "val_dataset_299 = val_dataset.map(resize_to_299, num_parallel_calls=AUTOTUNE)\n",
    "val_dataset_299 = val_dataset_299.prefetch(AUTOTUNE)\n",
    "\n",
    "# Test dataset 299x299 (augmentation yok)\n",
    "test_dataset_299 = test_dataset.map(resize_to_299, num_parallel_calls=AUTOTUNE)\n",
    "test_dataset_299 = test_dataset_299.prefetch(AUTOTUNE)\n",
    "\n",
    "print(\"299x299 Dataset hazir (dusuk augmentation)!\")\n",
    "\n",
    "# =============================================================================\n",
    "# CBAM MODULU\n",
    "# =============================================================================\n",
    "\n",
    "def channel_attention(input_tensor, ratio=8):\n",
    "    \"\"\"\n",
    "    Channel Attention Module.\n",
    "    Her feature channel'in onemini hesaplar.\n",
    "    \"\"\"\n",
    "    channel = input_tensor.shape[-1]\n",
    "\n",
    "    # Global Average Pooling\n",
    "    avg_pool = GlobalAveragePooling2D()(input_tensor)\n",
    "    avg_pool = Reshape((1, 1, channel))(avg_pool)\n",
    "\n",
    "    # Global Max Pooling\n",
    "    max_pool = GlobalMaxPooling2D()(input_tensor)\n",
    "    max_pool = Reshape((1, 1, channel))(max_pool)\n",
    "\n",
    "    # Shared MLP\n",
    "    shared_dense1 = Dense(channel // ratio, activation='relu', name='channel_dense1')\n",
    "    shared_dense2 = Dense(channel, name='channel_dense2')\n",
    "\n",
    "    avg_out = shared_dense2(shared_dense1(avg_pool))\n",
    "    max_out = shared_dense2(shared_dense1(max_pool))\n",
    "\n",
    "    # Combine\n",
    "    attention = Add()([avg_out, max_out])\n",
    "    attention = Activation('sigmoid')(attention)\n",
    "\n",
    "    return Multiply()([input_tensor, attention])\n",
    "\n",
    "\n",
    "def spatial_attention(input_tensor, kernel_size=7):\n",
    "    \"\"\"\n",
    "    Spatial Attention Module.\n",
    "    Goruntunun hangi bolgelerinin onemli oldugunu hesaplar.\n",
    "    \"\"\"\n",
    "    # Channel-wise average ve max\n",
    "    avg_pool = Lambda(lambda x: K.mean(x, axis=-1, keepdims=True))(input_tensor)\n",
    "    max_pool = Lambda(lambda x: K.max(x, axis=-1, keepdims=True))(input_tensor)\n",
    "\n",
    "    # Concatenate\n",
    "    concat = Concatenate(axis=-1)([avg_pool, max_pool])\n",
    "\n",
    "    # Conv layer\n",
    "    attention = Conv2D(1, kernel_size=kernel_size, padding='same', activation='sigmoid', name='spatial_conv')(concat)\n",
    "\n",
    "    return Multiply()([input_tensor, attention])\n",
    "\n",
    "\n",
    "def cbam_block(input_tensor, ratio=8, kernel_size=7):\n",
    "    \"\"\"\n",
    "    CBAM: Channel Attention + Spatial Attention (sirasiyla).\n",
    "    \"\"\"\n",
    "    x = channel_attention(input_tensor, ratio)\n",
    "    x = spatial_attention(x, kernel_size)\n",
    "    return x\n",
    "\n",
    "# =============================================================================\n",
    "# MODEL BUILDER (FOR KERAS TUNER)\n",
    "# =============================================================================\n",
    "\n",
    "def build_inceptionresnet_cbam(hp):\n",
    "    \"\"\"\n",
    "    InceptionResNetV2 + CBAM model builder.\n",
    "    Keras Tuner ile hyperparameter optimization.\n",
    "    \"\"\"\n",
    "\n",
    "    # Hyperparameters\n",
    "    learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
    "    dropout_1 = hp.Choice('dropout_1', values=[0.2, 0.3, 0.4])\n",
    "    dropout_2 = hp.Choice('dropout_2', values=[0.4, 0.5, 0.6])\n",
    "    dense_units_1 = hp.Choice('dense_units_1', values=[256, 512])\n",
    "    dense_units_2 = hp.Choice('dense_units_2', values=[128, 256])\n",
    "    optimizer_name = hp.Choice('optimizer', values=['adam', 'sgd', 'rmsprop'])\n",
    "    cbam_ratio = hp.Choice('cbam_ratio', values=[8, 16])\n",
    "\n",
    "    # Input\n",
    "    inputs = Input(shape=IMG_SHAPE_299, name='input_layer')\n",
    "\n",
    "    # Preprocessing\n",
    "    x = tf.keras.applications.inception_resnet_v2.preprocess_input(inputs)\n",
    "\n",
    "    # Base model: InceptionResNetV2\n",
    "    base_model = InceptionResNetV2(\n",
    "        include_top=False,\n",
    "        weights='imagenet',\n",
    "        input_shape=IMG_SHAPE_299\n",
    "    )\n",
    "    base_model.trainable = False  # FROZEN\n",
    "\n",
    "    # Feature extraction\n",
    "    features = base_model(x, training=False)\n",
    "\n",
    "    # CBAM Attention\n",
    "    attended_features = cbam_block(features, ratio=cbam_ratio)\n",
    "\n",
    "    # Global Average Pooling\n",
    "    x = GlobalAveragePooling2D(name='gap')(attended_features)\n",
    "\n",
    "    # Classification head\n",
    "    x = Dense(dense_units_1, activation='relu', name='dense1')(x)\n",
    "    x = Dropout(dropout_1, name='drop1')(x)\n",
    "    x = Dense(dense_units_2, activation='relu', name='dense2')(x)\n",
    "    x = Dropout(dropout_2, name='drop2')(x)\n",
    "\n",
    "    # Output\n",
    "    outputs = Dense(NUM_CLASSES, activation='softmax', name='output')(x)\n",
    "\n",
    "    # Model\n",
    "    model = Model(inputs=inputs, outputs=outputs, name='InceptionResNetV2_CBAM')\n",
    "\n",
    "    # Optimizer\n",
    "    if optimizer_name == 'adam':\n",
    "        optimizer = Adam(learning_rate=learning_rate)\n",
    "    elif optimizer_name == 'sgd':\n",
    "        optimizer = SGD(learning_rate=learning_rate, momentum=0.9)\n",
    "    else:\n",
    "        optimizer = RMSprop(learning_rate=learning_rate)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "# =============================================================================\n",
    "# KERAS TUNER - HYPERPARAMETER OPTIMIZATION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"HYPERPARAMETER OPTIMIZATION (KERAS TUNER)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "tuner_dir = f'{MODEL_SAVE_PATH}/tuner_{MODEL_NAME}'\n",
    "\n",
    "tuner = kt.BayesianOptimization(\n",
    "    build_inceptionresnet_cbam,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=15,\n",
    "    num_initial_points=5,\n",
    "    directory=tuner_dir,\n",
    "    project_name='inceptionresnet_cbam_tuning',\n",
    "    overwrite=True\n",
    ")\n",
    "\n",
    "print(\"\"\"\n",
    "Tuner Configuration:\n",
    "   - Algorithm      : Bayesian Optimization\n",
    "   - Max Trials     : 15\n",
    "   - Objective      : val_accuracy (maximize)\n",
    "\n",
    "Hyperparameter Search Space:\n",
    "   - Learning Rate  : [0.01, 0.001, 0.0001]\n",
    "   - Dropout 1      : [0.2, 0.3, 0.4]\n",
    "   - Dropout 2      : [0.4, 0.5, 0.6]\n",
    "   - Dense Units 1  : [256, 512]\n",
    "   - Dense Units 2  : [128, 256]\n",
    "   - Optimizer      : [Adam, SGD, RMSprop]\n",
    "   - CBAM Ratio     : [8, 16]\n",
    "\n",
    "Augmentation (Dusurulmus):\n",
    "   - RandomRotation : 0.10 (onceki: 0.15)\n",
    "   - RandomZoom     : 0.10 (onceki: 0.15)\n",
    "   - RandomContrast : 0.05 (onceki: 0.10)\n",
    "   - RandomBrightness: 0.05 (onceki: 0.10)\n",
    "\"\"\")\n",
    "\n",
    "early_stop_tuner = EarlyStopping(\n",
    "    monitor='val_accuracy',\n",
    "    patience=5,\n",
    "    restore_best_weights=True,\n",
    "    mode='max'\n",
    ")\n",
    "\n",
    "print(\"Hyperparameter search starting...\")\n",
    "start_tuning = time.time()\n",
    "\n",
    "tuner.search(\n",
    "    train_dataset_299_low,\n",
    "    epochs=20,\n",
    "    validation_data=val_dataset_299,\n",
    "    callbacks=[early_stop_tuner],\n",
    "    class_weight=class_weights,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "tuning_time = time.time() - start_tuning\n",
    "print(f\"\\nTuning completed: {tuning_time/60:.2f} minutes\")\n",
    "\n",
    "# =============================================================================\n",
    "# OPTIMAL HYPERPARAMETERS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"OPTIMAL HYPERPARAMETERS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "print(f\"\"\"\n",
    "Optimal Values Found:\n",
    "   - Learning Rate  : {best_hps.get('learning_rate')}\n",
    "   - Dropout 1      : {best_hps.get('dropout_1')}\n",
    "   - Dropout 2      : {best_hps.get('dropout_2')}\n",
    "   - Dense Units 1  : {best_hps.get('dense_units_1')}\n",
    "   - Dense Units 2  : {best_hps.get('dense_units_2')}\n",
    "   - Optimizer      : {best_hps.get('optimizer')}\n",
    "   - CBAM Ratio     : {best_hps.get('cbam_ratio')}\n",
    "\"\"\")\n",
    "\n",
    "# =============================================================================\n",
    "# FINAL MODEL TRAINING\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"FINAL MODEL TRAINING\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "model_8 = tuner.hypermodel.build(best_hps)\n",
    "\n",
    "total_params_8 = model_8.count_params()\n",
    "trainable_params_8 = sum([tf.keras.backend.count_params(w) for w in model_8.trainable_weights])\n",
    "\n",
    "print(f\"\"\"\n",
    "Model Parameters:\n",
    "   - Total          : {total_params_8:,}\n",
    "   - Trainable      : {trainable_params_8:,}\n",
    "   - Frozen         : {total_params_8 - trainable_params_8:,}\n",
    "\"\"\")\n",
    "\n",
    "callbacks_final = [\n",
    "    EarlyStopping(\n",
    "        monitor='val_accuracy',\n",
    "        patience=15,\n",
    "        restore_best_weights=True,\n",
    "        mode='max',\n",
    "        verbose=1\n",
    "    ),\n",
    "    ModelCheckpoint(\n",
    "        filepath=f'{MODEL_SAVE_PATH}/{MODEL_NAME}_best.keras',\n",
    "        monitor='val_accuracy',\n",
    "        save_best_only=True,\n",
    "        mode='max',\n",
    "        verbose=1\n",
    "    ),\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=5,\n",
    "        min_lr=1e-7,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "print(\"Final model training starting (50 epochs)...\")\n",
    "start_training = time.time()\n",
    "\n",
    "history_8 = model_8.fit(\n",
    "    train_dataset_299_low,\n",
    "    epochs=50,\n",
    "    validation_data=val_dataset_299,\n",
    "    callbacks=callbacks_final,\n",
    "    class_weight=class_weights,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "training_time = time.time() - start_training\n",
    "total_time_8 = tuning_time + training_time\n",
    "total_epochs_8 = len(history_8.history['accuracy'])\n",
    "avg_epoch_time_8 = training_time / total_epochs_8\n",
    "\n",
    "print(f\"\\nFinal training completed: {training_time/60:.2f} minutes ({total_epochs_8} epochs)\")\n",
    "print(f\"Total time: {total_time_8/60:.2f} minutes\")\n",
    "\n",
    "# =============================================================================\n",
    "# TRAINING HISTORY VISUALIZATION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"TRAINING HISTORY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "epochs_range = range(1, len(history_8.history['accuracy']) + 1)\n",
    "\n",
    "# Accuracy Plot\n",
    "axes[0].plot(epochs_range, history_8.history['accuracy'], 'b-', label='Train', linewidth=2)\n",
    "axes[0].plot(epochs_range, history_8.history['val_accuracy'], 'r-', label='Test', linewidth=2)\n",
    "axes[0].set_title(f'{MODEL_NAME}\\nModel Accuracy', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Accuracy')\n",
    "axes[0].legend(loc='lower right')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].set_ylim([0, 1])\n",
    "\n",
    "# Loss Plot\n",
    "axes[1].plot(epochs_range, history_8.history['loss'], 'b-', label='Train', linewidth=2)\n",
    "axes[1].plot(epochs_range, history_8.history['val_loss'], 'r-', label='Test', linewidth=2)\n",
    "axes[1].set_title(f'{MODEL_NAME}\\nModel Loss', fontsize=12, fontweight='bold')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Loss')\n",
    "axes[1].legend(loc='upper right')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{RESULTS_PATH}/{MODEL_NAME}_history.png', dpi=300, bbox_inches='tight', facecolor='white')\n",
    "plt.show()\n",
    "\n",
    "# =============================================================================\n",
    "# MODEL EVALUATION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"MODEL EVALUATION (TEST SET)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "y_pred_proba_8 = model_8.predict(test_dataset_299, verbose=1)\n",
    "y_pred_8 = np.argmax(y_pred_proba_8, axis=1)\n",
    "\n",
    "y_true_8 = []\n",
    "for _, labels in test_dataset_299:\n",
    "    y_true_8.extend(np.argmax(labels.numpy(), axis=1))\n",
    "y_true_8 = np.array(y_true_8)\n",
    "\n",
    "# Metrics\n",
    "accuracy_8 = accuracy_score(y_true_8, y_pred_8)\n",
    "precision_8 = precision_score(y_true_8, y_pred_8, average='weighted', zero_division=0)\n",
    "recall_8 = recall_score(y_true_8, y_pred_8, average='weighted', zero_division=0)\n",
    "f1_8 = f1_score(y_true_8, y_pred_8, average='weighted', zero_division=0)\n",
    "kappa_8 = cohen_kappa_score(y_true_8, y_pred_8)\n",
    "\n",
    "print(f\"\"\"\n",
    "Performance Metrics:\n",
    "{'-' * 50}\n",
    "   Accuracy     : {accuracy_8:.4f} ({accuracy_8*100:.2f}%)\n",
    "   Precision    : {precision_8:.4f}\n",
    "   Recall       : {recall_8:.4f}\n",
    "   F1-Score     : {f1_8:.4f}\n",
    "   Cohen Kappa  : {kappa_8:.4f}\n",
    "{'-' * 50}\n",
    "\"\"\")\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_true_8, y_pred_8, target_names=class_names_ordered, zero_division=0))\n",
    "\n",
    "# =============================================================================\n",
    "# CONFUSION MATRIX\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"CONFUSION MATRIX\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "cm_8 = confusion_matrix(y_true_8, y_pred_8)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm_8, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=class_names_ordered, yticklabels=class_names_ordered,\n",
    "            annot_kws={'size': 14, 'fontweight': 'bold'})\n",
    "plt.title(f'{MODEL_NAME}\\nConfusion Matrix', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{RESULTS_PATH}/{MODEL_NAME}_confusion_matrix.png', dpi=300, bbox_inches='tight', facecolor='white')\n",
    "plt.show()\n",
    "\n",
    "# =============================================================================\n",
    "# ROC CURVES\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"ROC CURVES\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "y_true_bin_8 = label_binarize(y_true_8, classes=range(NUM_CLASSES))\n",
    "fpr_8, tpr_8, roc_auc_8 = {}, {}, {}\n",
    "\n",
    "for i in range(NUM_CLASSES):\n",
    "    fpr_8[i], tpr_8[i], _ = roc_curve(y_true_bin_8[:, i], y_pred_proba_8[:, i])\n",
    "    roc_auc_8[i] = auc(fpr_8[i], tpr_8[i])\n",
    "\n",
    "fpr_8[\"micro\"], tpr_8[\"micro\"], _ = roc_curve(y_true_bin_8.ravel(), y_pred_proba_8.ravel())\n",
    "roc_auc_8[\"micro\"] = auc(fpr_8[\"micro\"], tpr_8[\"micro\"])\n",
    "\n",
    "all_fpr_8 = np.unique(np.concatenate([fpr_8[i] for i in range(NUM_CLASSES)]))\n",
    "mean_tpr_8 = np.zeros_like(all_fpr_8)\n",
    "for i in range(NUM_CLASSES):\n",
    "    mean_tpr_8 += np.interp(all_fpr_8, fpr_8[i], tpr_8[i])\n",
    "mean_tpr_8 /= NUM_CLASSES\n",
    "fpr_8[\"macro\"], tpr_8[\"macro\"] = all_fpr_8, mean_tpr_8\n",
    "roc_auc_8[\"macro\"] = auc(fpr_8[\"macro\"], tpr_8[\"macro\"])\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "colors = ['#e41a1c', '#377eb8', '#4daf4a', '#984ea3', '#ff7f00']\n",
    "for i, (name, color) in enumerate(zip(class_names_ordered, colors)):\n",
    "    plt.plot(fpr_8[i], tpr_8[i], color=color, lw=2, label=f'{name} (AUC={roc_auc_8[i]:.4f})')\n",
    "plt.plot(fpr_8[\"micro\"], tpr_8[\"micro\"], 'deeppink', ls=':', lw=3, label=f'Micro-avg (AUC={roc_auc_8[\"micro\"]:.4f})')\n",
    "plt.plot(fpr_8[\"macro\"], tpr_8[\"macro\"], 'navy', ls=':', lw=3, label=f'Macro-avg (AUC={roc_auc_8[\"macro\"]:.4f})')\n",
    "plt.plot([0,1], [0,1], 'k--', lw=2)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title(f'{MODEL_NAME}\\nROC Curves', fontsize=14, fontweight='bold')\n",
    "plt.legend(loc='lower right', fontsize=9)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{RESULTS_PATH}/{MODEL_NAME}_roc_curves.png', dpi=300, bbox_inches='tight', facecolor='white')\n",
    "plt.show()\n",
    "\n",
    "print(\"ROC AUC Scores:\")\n",
    "for i, name in enumerate(class_names_ordered):\n",
    "    print(f\"   {name}: {roc_auc_8[i]:.4f}\")\n",
    "print(f\"   Macro-average: {roc_auc_8['macro']:.4f}\")\n",
    "print(f\"   Micro-average: {roc_auc_8['micro']:.4f}\")\n",
    "\n",
    "# =============================================================================\n",
    "# SAVE RESULTS\n",
    "# =============================================================================\n",
    "\n",
    "results_8 = {\n",
    "    'model_name': MODEL_NAME,\n",
    "    'accuracy': accuracy_8,\n",
    "    'precision': precision_8,\n",
    "    'recall': recall_8,\n",
    "    'f1_score': f1_8,\n",
    "    'kappa': kappa_8,\n",
    "    'roc_auc_macro': roc_auc_8['macro'],\n",
    "    'roc_auc_micro': roc_auc_8['micro'],\n",
    "    'training_time_min': total_time_8 / 60,\n",
    "    'avg_epoch_time_sec': avg_epoch_time_8,\n",
    "    'total_epochs': total_epochs_8,\n",
    "    'total_params': total_params_8,\n",
    "    'best_lr': best_hps.get('learning_rate'),\n",
    "    'best_dropout1': best_hps.get('dropout_1'),\n",
    "    'best_dropout2': best_hps.get('dropout_2'),\n",
    "    'best_optimizer': best_hps.get('optimizer'),\n",
    "    'best_cbam_ratio': best_hps.get('cbam_ratio')\n",
    "}\n",
    "\n",
    "for i, name in enumerate(class_names_ordered):\n",
    "    results_8[f'auc_{name}'] = roc_auc_8[i]\n",
    "\n",
    "all_results.append(results_8)\n",
    "\n",
    "# Save model\n",
    "model_8.save(f'{MODEL_SAVE_PATH}/{MODEL_NAME}_final.keras')\n",
    "pd.DataFrame(history_8.history).to_csv(f'{RESULTS_PATH}/{MODEL_NAME}_history.csv', index=False)\n",
    "\n",
    "print(f\"\\nModel saved: {MODEL_SAVE_PATH}/{MODEL_NAME}_final.keras\")\n",
    "\n",
    "# =============================================================================\n",
    "# MODEL COMPARISON TABLE\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"MODEL COMPARISON TABLE\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "print(f\"\\n{'Method':<50} {'Precision':>10} {'Recall':>10} {'F1-Score':>10} {'Accuracy':>10} {'Avg Epoch(s)':>12}\")\n",
    "print(\"-\" * 105)\n",
    "\n",
    "for r in all_results:\n",
    "    avg_epoch = r.get('avg_epoch_time_sec', 0)\n",
    "    print(f\"{r['model_name']:<50} {r['precision']:>10.4f} {r['recall']:>10.4f} {r['f1_score']:>10.4f} {r['accuracy']:>10.4f} {avg_epoch:>12.2f}\")\n",
    "\n",
    "print(\"-\" * 105)\n",
    "\n",
    "best = max(all_results, key=lambda x: x['accuracy'])\n",
    "print(f\"\\nBest Model: {best['model_name']} (Accuracy: {best['accuracy']*100:.2f}%)\")\n",
    "\n",
    "# =============================================================================\n",
    "# MODEL SUMMARY REPORT\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"MODEL 8 - SUMMARY REPORT\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\"\"\n",
    "Model Information:\n",
    "   Name            : {MODEL_NAME}\n",
    "   Type            : Attention-based (CBAM)\n",
    "   Backbone        : InceptionResNetV2 (FROZEN)\n",
    "   Attention       : CBAM (Channel + Spatial)\n",
    "   Input Size      : 299x299x3\n",
    "   Total Params    : {total_params_8:,}\n",
    "   Trainable       : {trainable_params_8:,}\n",
    "\n",
    "CBAM Details:\n",
    "   - Channel Attention: Global Avg/Max Pool + Shared MLP\n",
    "   - Spatial Attention: Channel Avg/Max + Conv2D\n",
    "   - Ratio: {best_hps.get('cbam_ratio')}\n",
    "\n",
    "Augmentation (Reduced):\n",
    "   - RandomRotation  : 0.10\n",
    "   - RandomZoom      : 0.10\n",
    "   - RandomContrast  : 0.05\n",
    "   - RandomBrightness: 0.05\n",
    "\n",
    "Optimal Hyperparameters:\n",
    "   Learning Rate   : {best_hps.get('learning_rate')}\n",
    "   Dropout 1       : {best_hps.get('dropout_1')}\n",
    "   Dropout 2       : {best_hps.get('dropout_2')}\n",
    "   Dense Units 1   : {best_hps.get('dense_units_1')}\n",
    "   Dense Units 2   : {best_hps.get('dense_units_2')}\n",
    "   Optimizer       : {best_hps.get('optimizer')}\n",
    "\n",
    "Performance Metrics:\n",
    "   Accuracy        : {accuracy_8*100:.2f}%\n",
    "   Precision       : {precision_8:.4f}\n",
    "   Recall          : {recall_8:.4f}\n",
    "   F1-Score        : {f1_8:.4f}\n",
    "   Cohen Kappa     : {kappa_8:.4f}\n",
    "   ROC AUC (Macro) : {roc_auc_8['macro']:.4f}\n",
    "\n",
    "Training Time:\n",
    "   Tuning          : {tuning_time/60:.2f} minutes\n",
    "   Final Training  : {training_time/60:.2f} minutes\n",
    "   Total           : {total_time_8/60:.2f} minutes\n",
    "\"\"\")\n",
    "\n",
    "gc.collect()\n",
    "print(\"=\" * 70)\n",
    "print(\"MODEL 8 COMPLETED SUCCESSFULLY\")\n",
    "print(\"=\" * 70)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Y2XvEUk2LJL8",
    "outputId": "226f4569-7456-470e-8b14-b9d6bd370e54"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Cell 21: MODEL 9 - Xception + FPN (MULTI-SCALE CLASSIFICATION)"
   ],
   "metadata": {
    "id": "3TH0KFrlhojT"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# =============================================================================\n",
    "# Cell 21: MODEL 9 - Xception + FPN (MULTI-SCALE CLASSIFICATION)\n",
    "# =============================================================================\n",
    "# TECHNICAL FIX REPORT:\n",
    "# 1. \"ResizeNearestNeighborGrad\" error resolved via Lambda layers.\n",
    "# 2. All required plots (ROC, Confusion Matrix, History) are included.\n",
    "# 3. Memory is conserved by reusing 'train_dataset_299' from Model 7.\n",
    "# =============================================================================\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, cohen_kappa_score, classification_report, confusion_matrix, roc_curve, auc\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from tensorflow.keras.applications import Xception\n",
    "from tensorflow.keras.layers import (Input, GlobalAveragePooling2D, Dense,\n",
    "                                     Dropout, Concatenate, Conv2D, UpSampling2D,\n",
    "                                     Add, BatchNormalization, Activation, Lambda)\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "MODEL_NAME = \"Model_09_Xception_FPN\"\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"MODEL 9: Xception + FPN (MULTI-SCALE CLASSIFICATION)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\"\"\n",
    "ARCHITECTURE SUMMARY:\n",
    "   Input (299x299) -> Xception (Frozen) -> FPN (P3, P4, P5) -> Multi-Scale Head -> Output\n",
    "\n",
    "   * FPN (Feature Pyramid Network) enables the model to detect both small and large lesions.\n",
    "   * Lambda layers make the Resize operation GPU-compatible.\n",
    "\"\"\")\n",
    "\n",
    "# =============================================================================\n",
    "# FPN MODEL BUILDER (GPU-COMPATIBLE FIX - VIA LAMBDA)\n",
    "# =============================================================================\n",
    "\n",
    "def create_xception_fpn_model(input_shape, num_classes):\n",
    "    \"\"\"\n",
    "    Xception + FPN.\n",
    "    FIX: tf.image.resize with method='bilinear' and Lambda layer applied.\n",
    "    \"\"\"\n",
    "\n",
    "    inputs = Input(shape=input_shape, name='input_layer')\n",
    "\n",
    "    # Preprocessing\n",
    "    x = tf.keras.applications.xception.preprocess_input(inputs)\n",
    "\n",
    "    # Backbone: Xception\n",
    "    backbone = Xception(include_top=False, weights='imagenet', input_tensor=x)\n",
    "    backbone.trainable = False  # FROZEN\n",
    "\n",
    "    # Feature maps\n",
    "    c3 = backbone.get_layer('block4_sepconv2_bn').output\n",
    "    c4 = backbone.get_layer('block13_sepconv2_bn').output\n",
    "    c5 = backbone.get_layer('block14_sepconv2_act').output\n",
    "\n",
    "    # Lateral connections (1x1 Conv)\n",
    "    p5_lateral = Conv2D(256, (1, 1), padding='same', name='fpn_c5_lateral')(c5)\n",
    "    p4_lateral = Conv2D(256, (1, 1), padding='same', name='fpn_c4_lateral')(c4)\n",
    "    p3_lateral = Conv2D(256, (1, 1), padding='same', name='fpn_c3_lateral')(c3)\n",
    "\n",
    "    # P5\n",
    "    p5 = p5_lateral\n",
    "\n",
    "    # --- P4 CONSTRUCTION (BUG-FIXED: Lambda + Bilinear) ---\n",
    "    p5_resized = Lambda(\n",
    "        lambda x: tf.image.resize(x[0], tf.shape(x[1])[1:3], method='bilinear'),\n",
    "        name='fpn_p5_resize'\n",
    "    )([p5, p4_lateral])\n",
    "\n",
    "    p4 = Add(name='fpn_p4_add')([p4_lateral, p5_resized])\n",
    "\n",
    "    # --- P3 CONSTRUCTION (BUG-FIXED: Lambda + Bilinear) ---\n",
    "    p4_resized = Lambda(\n",
    "        lambda x: tf.image.resize(x[0], tf.shape(x[1])[1:3], method='bilinear'),\n",
    "        name='fpn_p4_resize'\n",
    "    )([p4, p3_lateral])\n",
    "\n",
    "    p3 = Add(name='fpn_p3_add')([p3_lateral, p4_resized])\n",
    "\n",
    "    # Smooth layers\n",
    "    p5 = Conv2D(256, (3, 3), padding='same', activation='relu', name='fpn_p5_smooth')(p5)\n",
    "    p4 = Conv2D(256, (3, 3), padding='same', activation='relu', name='fpn_p4_smooth')(p4)\n",
    "    p3 = Conv2D(256, (3, 3), padding='same', activation='relu', name='fpn_p3_smooth')(p3)\n",
    "\n",
    "    # Head\n",
    "    gap_p3 = GlobalAveragePooling2D(name='gap_p3')(p3)\n",
    "    gap_p4 = GlobalAveragePooling2D(name='gap_p4')(p4)\n",
    "    gap_p5 = GlobalAveragePooling2D(name='gap_p5')(p5)\n",
    "\n",
    "    merged = Concatenate(name='fpn_concat')([gap_p3, gap_p4, gap_p5])\n",
    "\n",
    "    x = Dense(256, activation='relu', name='fc1')(merged)\n",
    "    x = Dropout(0.4, name='dropout1')(x)\n",
    "    x = Dense(128, activation='relu', name='fc2')(x)\n",
    "    x = Dropout(0.4, name='dropout2')(x)\n",
    "\n",
    "    outputs = Dense(num_classes, activation='softmax', name='output')(x)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=outputs, name='Xception_FPN')\n",
    "    return model\n",
    "\n",
    "# =============================================================================\n",
    "# KURULUM VE DERLEME\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\nModel kuruluyor...\")\n",
    "# ATTENTION: IMG_SHAPE_299 variable must come from preceding cells (299, 299, 3)\n",
    "# If an error occurs, manually set this to (299, 299, 3).\n",
    "try:\n",
    "    input_shape_dynamic = IMG_SHAPE_299\n",
    "except NameError:\n",
    "    input_shape_dynamic = (299, 299, 3)\n",
    "    print(\"[WARNING] IMG_SHAPE_299 not found; using default (299, 299, 3).\")\n",
    "\n",
    "model_9 = create_xception_fpn_model(input_shape_dynamic, NUM_CLASSES)\n",
    "\n",
    "total_params_9 = model_9.count_params()\n",
    "trainable_params_9 = sum([tf.keras.backend.count_params(w) for w in model_9.trainable_weights])\n",
    "\n",
    "print(f\"Model Parameters: Total={total_params_9:,}, Trainable={trainable_params_9:,}\")\n",
    "\n",
    "print(\"Compiling model (JIT Disabled)...\")\n",
    "model_9.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy'],\n",
    "    jit_compile=False # IMPORTANT: Prevents XLA compilation errors\n",
    ")\n",
    "\n",
    "# =============================================================================\n",
    "# TRAINING\n",
    "# =============================================================================\n",
    "\n",
    "callbacks_9 = [\n",
    "    EarlyStopping(monitor='val_accuracy', patience=10, restore_best_weights=True, verbose=1),\n",
    "    ModelCheckpoint(f'{MODEL_SAVE_PATH}/{MODEL_NAME}_best.keras', monitor='val_accuracy', save_best_only=True, verbose=0),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, verbose=1)\n",
    "]\n",
    "\n",
    "print(\"\\nTraining Starting (40 Epochs)...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# NOTE: Ensure train_dataset_299 is loaded in memory.\n",
    "try:\n",
    "    history_9 = model_9.fit(\n",
    "        train_dataset_299,\n",
    "        epochs=40,\n",
    "        validation_data=val_dataset_299,\n",
    "        callbacks=callbacks_9,\n",
    "        class_weight=class_weights,\n",
    "        verbose=1\n",
    "    )\n",
    "except NameError:\n",
    "    print(\"[ERROR] 'train_dataset_299' not found! Please run the dataset creation code from Model 7 or 8.\")\n",
    "    raise\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "print(f\"\\nTraining Complete. Duration: {training_time/60:.2f} minutes\")\n",
    "\n",
    "# =============================================================================\n",
    "# PLOTS AND REPORTING\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"1. TRAINING HISTORY VISUALIZATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "epochs_range = range(1, len(history_9.history['accuracy']) + 1)\n",
    "\n",
    "# Accuracy\n",
    "axes[0].plot(epochs_range, history_9.history['accuracy'], 'b-', label='Train', linewidth=2)\n",
    "axes[0].plot(epochs_range, history_9.history['val_accuracy'], 'r-', label='Test', linewidth=2)\n",
    "axes[0].set_title(f'{MODEL_NAME}\\nModel Accuracy', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xlabel('Epoch'); axes[0].set_ylabel('Accuracy'); axes[0].legend(); axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Loss\n",
    "axes[1].plot(epochs_range, history_9.history['loss'], 'b-', label='Train', linewidth=2)\n",
    "axes[1].plot(epochs_range, history_9.history['val_loss'], 'r-', label='Test', linewidth=2)\n",
    "axes[1].set_title(f'{MODEL_NAME}\\nModel Loss', fontsize=12, fontweight='bold')\n",
    "axes[1].set_xlabel('Epoch'); axes[1].set_ylabel('Loss'); axes[1].legend(); axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{RESULTS_PATH}/{MODEL_NAME}_history.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Test Predictions\n",
    "print(\"\\nEvaluating test set...\")\n",
    "y_pred_proba_9 = model_9.predict(test_dataset_299, verbose=1)\n",
    "y_pred_9 = np.argmax(y_pred_proba_9, axis=1)\n",
    "\n",
    "y_true_9 = []\n",
    "for _, labels in test_dataset_299:\n",
    "    y_true_9.extend(np.argmax(labels.numpy(), axis=1))\n",
    "y_true_9 = np.array(y_true_9)\n",
    "\n",
    "# Metrikler\n",
    "acc_9 = accuracy_score(y_true_9, y_pred_9)\n",
    "f1_9 = f1_score(y_true_9, y_pred_9, average='weighted')\n",
    "roc_auc_9 = roc_auc_score(label_binarize(y_true_9, classes=range(NUM_CLASSES)), y_pred_proba_9, average='macro', multi_class='ovr')\n",
    "\n",
    "print(f\"\\nPERFORMANCE METRICS:\")\n",
    "print(f\"   â€¢ Accuracy: {acc_9*100:.2f}%\")\n",
    "print(f\"   â€¢ F1-Score: {f1_9:.4f}\")\n",
    "print(f\"   â€¢ ROC AUC : {roc_auc_9:.4f}\")\n",
    "\n",
    "# Confusion Matrix\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"2. CONFUSION MATRIX\")\n",
    "print(\"=\" * 70)\n",
    "cm_9 = confusion_matrix(y_true_9, y_pred_9)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm_9, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=class_names_ordered, yticklabels=class_names_ordered,\n",
    "            annot_kws={'size': 14, 'fontweight': 'bold'})\n",
    "plt.title(f'{MODEL_NAME}\\nConfusion Matrix', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('True Label'); plt.xlabel('Predicted Label')\n",
    "plt.savefig(f'{RESULTS_PATH}/{MODEL_NAME}_confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# ROC Curves\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"3. ROC CURVES\")\n",
    "print(\"=\" * 70)\n",
    "y_true_bin_9 = label_binarize(y_true_9, classes=range(NUM_CLASSES))\n",
    "fpr_9, tpr_9, roc_auc_dict_9 = {}, {}, {}\n",
    "\n",
    "for i in range(NUM_CLASSES):\n",
    "    fpr_9[i], tpr_9[i], _ = roc_curve(y_true_bin_9[:, i], y_pred_proba_9[:, i])\n",
    "    roc_auc_dict_9[i] = auc(fpr_9[i], tpr_9[i])\n",
    "\n",
    "# Macro Average\n",
    "all_fpr_9 = np.unique(np.concatenate([fpr_9[i] for i in range(NUM_CLASSES)]))\n",
    "mean_tpr_9 = np.zeros_like(all_fpr_9)\n",
    "for i in range(NUM_CLASSES):\n",
    "    mean_tpr_9 += np.interp(all_fpr_9, fpr_9[i], tpr_9[i])\n",
    "mean_tpr_9 /= NUM_CLASSES\n",
    "roc_auc_dict_9[\"macro\"] = auc(all_fpr_9, mean_tpr_9)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "colors = ['#e41a1c', '#377eb8', '#4daf4a', '#984ea3', '#ff7f00']\n",
    "for i, (name, color) in enumerate(zip(class_names_ordered, colors)):\n",
    "    plt.plot(fpr_9[i], tpr_9[i], color=color, lw=2, label=f'{name} (AUC={roc_auc_dict_9[i]:.4f})')\n",
    "plt.plot(all_fpr_9, mean_tpr_9, 'navy', ls=':', lw=3, label=f'Macro (AUC={roc_auc_dict_9[\"macro\"]:.4f})')\n",
    "plt.plot([0,1], [0,1], 'k--', lw=2)\n",
    "plt.xlabel('False Positive Rate'); plt.ylabel('True Positive Rate')\n",
    "plt.title(f'{MODEL_NAME}\\nROC Curves', fontsize=14, fontweight='bold')\n",
    "plt.legend(loc='lower right'); plt.grid(True, alpha=0.3)\n",
    "plt.savefig(f'{RESULTS_PATH}/{MODEL_NAME}_roc_curves.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Listeye ekle ve kaydet\n",
    "results_9 = {\n",
    "    'model_name': MODEL_NAME,\n",
    "    'accuracy': acc_9,\n",
    "    'f1_score': f1_9,\n",
    "    'roc_auc_macro': roc_auc_9,\n",
    "    'training_time_min': training_time / 60,\n",
    "    'total_epochs': len(history_9.history['loss'])\n",
    "}\n",
    "all_results.append(results_9)\n",
    "\n",
    "model_9.save(f\"{MODEL_SAVE_PATH}/{MODEL_NAME}_final.keras\")\n",
    "pd.DataFrame(history_9.history).to_csv(f'{RESULTS_PATH}/{MODEL_NAME}_history.csv', index=False)\n",
    "gc.collect()\n",
    "\n",
    "print(f\"{MODEL_NAME} completed successfully. All figures saved.\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "8w8f6A-6hpgO",
    "outputId": "08a10ee6-0335-4057-e60a-b54791ec34d0"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Cell 22: MODEL 10 - SalvationNet (FULLY ORIGINAL ARCHITECTURE)"
   ],
   "metadata": {
    "id": "r714CWOHo--N"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# =============================================================================\n",
    "# Cell 22: MODEL 10 - SalvationNet (FULLY ORIGINAL ARCHITECTURE)\n",
    "# =============================================================================\n",
    "#\n",
    "# SalvationNet: Tri-Domain Adaptive Fusion Network for Skin Disease Classification\n",
    "#\n",
    "# Original Components (NOT in literature):\n",
    "# 1. Tri-Domain Parallel Processing (TDPP)\n",
    "#    - Color Domain Branch: Color-based conditions (Vitiligo, Hyperpigmentation)\n",
    "#    - Texture Domain Branch: Texture-based conditions (Acne, Nail Psoriasis)\n",
    "#    - Morphology Domain Branch: Structure-based conditions (SJS-TEN)\n",
    "#\n",
    "# 2. Cross-Domain Attention (CDA)\n",
    "#    - Inter-domain information exchange\n",
    "#    - Color features enrich texture analysis\n",
    "#\n",
    "# 3. Adaptive Domain Fusion Module (ADFM)\n",
    "#    - Dynamic domain weighting per sample\n",
    "#    - Learnable fusion weights\n",
    "#\n",
    "# 4. Hierarchical Feature Refinement (HFR)\n",
    "#    - Progressive feature refinement\n",
    "#    - Multi-level attention\n",
    "#\n",
    "# Dermatolojik Motivasyon:\n",
    "# - Dermatologists evaluate 3 fundamental factors in diagnosis:\n",
    "#   1. Color changes (depigmentation, hyperpigmentation)\n",
    "#   2. Texture changes (acne, nail deformities)\n",
    "#   3. Morphological changes (lesion shape, boundaries)\n",
    "# - SalvationNet adapts this clinical approach to a CNN architecture\n",
    "#\n",
    "# Author: Omer Faruk Kurtulus\n",
    "# =============================================================================\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, Conv2D, MaxPooling2D, AveragePooling2D, GlobalAveragePooling2D,\n",
    "    Dense, Dropout, BatchNormalization, Activation, Add, Multiply,\n",
    "    Concatenate, Reshape, Lambda, SeparableConv2D, DepthwiseConv2D,\n",
    "    GlobalMaxPooling2D, LayerNormalization\n",
    ")\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam, SGD, RMSprop\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    cohen_kappa_score, classification_report, confusion_matrix,\n",
    "    roc_curve, auc, roc_auc_score\n",
    ")\n",
    "from sklearn.preprocessing import label_binarize\n",
    "import tensorflow.keras.backend as K\n",
    "import keras_tuner as kt\n",
    "\n",
    "MODEL_NAME = \"Model_10_SalvationNet\"\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"MODEL 10: SalvationNet\")\n",
    "print(\"Tri-Domain Adaptive Fusion Network for Skin Disease Classification\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\"\"\n",
    "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "â•‘                        SALVATIONNET                                   â•‘\n",
    "â•‘         Tri-Domain Adaptive Fusion Network                           â•‘\n",
    "â•‘                                                                       â•‘\n",
    "â•‘  \"Kurtulus\" - A Novel Deep Learning Architecture for                 â•‘\n",
    "â•‘  Multi-Class Skin Disease Classification                             â•‘\n",
    "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "ORIGINAL ARCHITECTURE:\n",
    "\n",
    "                    Input (224x224x3)\n",
    "                          â”‚\n",
    "                    â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”\n",
    "                    â”‚   STEM    â”‚\n",
    "                    â”‚   BLOCK   â”‚\n",
    "                    â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜\n",
    "                          â”‚\n",
    "        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "        â”‚                 â”‚                 â”‚\n",
    "   â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”\n",
    "   â”‚  COLOR  â”‚      â”‚ TEXTURE â”‚      â”‚  MORPH  â”‚\n",
    "   â”‚ DOMAIN  â”‚      â”‚ DOMAIN  â”‚      â”‚ DOMAIN  â”‚\n",
    "   â”‚ (CDB)   â”‚      â”‚  (TDB)  â”‚      â”‚  (MDB)  â”‚\n",
    "   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜\n",
    "        â”‚                 â”‚                 â”‚\n",
    "        â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                 â”‚                 â”‚\n",
    "         â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”\n",
    "         â”‚Cross-Domain   â”‚ â”‚Cross-Domain  â”‚\n",
    "         â”‚ Attention 1   â”‚ â”‚ Attention 2  â”‚\n",
    "         â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                 â”‚                 â”‚\n",
    "                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                          â”‚\n",
    "                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "                 â”‚ Adaptive Domain â”‚\n",
    "                 â”‚  Fusion Module  â”‚\n",
    "                 â”‚     (ADFM)      â”‚\n",
    "                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                          â”‚\n",
    "                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "                 â”‚   Hierarchical  â”‚\n",
    "                 â”‚    Feature      â”‚\n",
    "                 â”‚  Refinement     â”‚\n",
    "                 â”‚     (HFR)       â”‚\n",
    "                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                          â”‚\n",
    "                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "                 â”‚  Classification â”‚\n",
    "                 â”‚      Head       â”‚\n",
    "                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                          â”‚\n",
    "                      Output (5)\n",
    "\n",
    "DOMAIN DESCRIPTIONS:\n",
    "  â€¢ Color Domain (CDB): Renk analizi - Vitiligo, Hyperpigmentation\n",
    "  â€¢ Texture Domain (TDB): Doku analizi - Acne, Nail Psoriasis\n",
    "  â€¢ Morphology Domain (MDB): Structural analysis - SJS-TEN, lesion boundaries\n",
    "\"\"\")\n",
    "\n",
    "# =============================================================================\n",
    "# DATASET PREPARATION (224x224 - Optimal size)\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"DATASET PREPARATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "IMG_SIZE_SALVATION = 224\n",
    "IMG_SHAPE_SALVATION = (IMG_SIZE_SALVATION, IMG_SIZE_SALVATION, 3)\n",
    "\n",
    "# Augmentation layer\n",
    "augmentation_salvation = tf.keras.Sequential([\n",
    "    tf.keras.layers.RandomFlip(\"horizontal\"),\n",
    "    tf.keras.layers.RandomRotation(0.10),\n",
    "    tf.keras.layers.RandomZoom(0.10),\n",
    "    tf.keras.layers.RandomContrast(0.05),\n",
    "    tf.keras.layers.RandomBrightness(0.05),\n",
    "], name='augmentation_salvation')\n",
    "\n",
    "# Resize function\n",
    "def resize_to_224(image, label):\n",
    "    image = tf.image.resize(image, [IMG_SIZE_SALVATION, IMG_SIZE_SALVATION])\n",
    "    return image, label\n",
    "\n",
    "# Dataset construction\n",
    "print(\"Creating 224x224 datasets...\")\n",
    "\n",
    "train_dataset_224 = train_dataset.map(resize_to_224, num_parallel_calls=AUTOTUNE)\n",
    "train_dataset_224 = train_dataset_224.map(\n",
    "    lambda x, y: (augmentation_salvation(x, training=True), y),\n",
    "    num_parallel_calls=AUTOTUNE\n",
    ")\n",
    "train_dataset_224 = train_dataset_224.prefetch(AUTOTUNE)\n",
    "\n",
    "val_dataset_224 = val_dataset.map(resize_to_224, num_parallel_calls=AUTOTUNE)\n",
    "val_dataset_224 = val_dataset_224.prefetch(AUTOTUNE)\n",
    "\n",
    "test_dataset_224 = test_dataset.map(resize_to_224, num_parallel_calls=AUTOTUNE)\n",
    "test_dataset_224 = test_dataset_224.prefetch(AUTOTUNE)\n",
    "\n",
    "print(f\"Dataset ready! Input shape: {IMG_SHAPE_SALVATION}\")\n",
    "\n",
    "# =============================================================================\n",
    "# SALVATIONNET - ORIGINAL COMPONENTS\n",
    "# =============================================================================\n",
    "\n",
    "def conv_bn_relu(x, filters, kernel_size, strides=1, padding='same', name=None):\n",
    "    \"\"\"\n",
    "    Basic Conv-BatchNorm-ReLU block.\n",
    "    Standard building block used in all convolution operations.\n",
    "    \"\"\"\n",
    "    x = Conv2D(\n",
    "        filters, kernel_size, strides=strides, padding=padding,\n",
    "        kernel_initializer='he_normal', use_bias=False,\n",
    "        kernel_regularizer=l2(1e-4), name=f'{name}_conv' if name else None\n",
    "    )(x)\n",
    "    x = BatchNormalization(name=f'{name}_bn' if name else None)(x)\n",
    "    x = Activation('relu', name=f'{name}_relu' if name else None)(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def stem_block(inputs, name='stem'):\n",
    "    \"\"\"\n",
    "    STEM BLOCK: Extracts initial features from the input image.\n",
    "    Shared foundation for all domain branches.\n",
    "\n",
    "    Input: 224x224x3\n",
    "    Output: 56x56x64\n",
    "    \"\"\"\n",
    "    # First conv: Rapid spatial downsampling\n",
    "    x = conv_bn_relu(inputs, 32, 3, strides=2, name=f'{name}_conv1')  # 112x112\n",
    "    x = conv_bn_relu(x, 32, 3, name=f'{name}_conv2')\n",
    "    x = conv_bn_relu(x, 64, 3, name=f'{name}_conv3')\n",
    "    x = MaxPooling2D(3, strides=2, padding='same', name=f'{name}_pool')(x)  # 56x56\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def color_domain_branch(x, filters, name='color'):\n",
    "    \"\"\"\n",
    "    COLOR DOMAIN BRANCH (CDB): Renk bilgisi analizi.\n",
    "\n",
    "    Ã–zellikler:\n",
    "    - 1x1 conv for channel-wise color analysis\n",
    "    - 3x3 conv capturing color gradients\n",
    "    - 5x5 conv for wide-field color patterns\n",
    "\n",
    "    Target conditions: Vitiligo, Hyperpigmentation\n",
    "    \"\"\"\n",
    "    # 1x1 Conv: Channel-wise color filter (RGB relationships)\n",
    "    color_1x1 = conv_bn_relu(x, filters // 2, 1, name=f'{name}_1x1')\n",
    "\n",
    "    # 3x3 Conv: Local color gradients\n",
    "    color_3x3 = conv_bn_relu(x, filters // 2, 3, name=f'{name}_3x3')\n",
    "\n",
    "    # 5x5 Conv: Wide-field color patterns (lesion extent)\n",
    "    color_5x5 = conv_bn_relu(x, filters // 2, 5, name=f'{name}_5x5')\n",
    "\n",
    "    # Concatenate\n",
    "    color_concat = Concatenate(name=f'{name}_concat')([color_1x1, color_3x3, color_5x5])\n",
    "\n",
    "    # Refinement\n",
    "    color_out = conv_bn_relu(color_concat, filters, 1, name=f'{name}_refine')\n",
    "    color_out = MaxPooling2D(2, name=f'{name}_pool')(color_out)\n",
    "\n",
    "    return color_out\n",
    "\n",
    "\n",
    "def texture_domain_branch(x, filters, name='texture'):\n",
    "    \"\"\"\n",
    "    TEXTURE DOMAIN BRANCH (TDB): Doku bilgisi analizi.\n",
    "\n",
    "    Ã–zellikler:\n",
    "    - Depthwise separable conv ile verimli doku filtresi\n",
    "    - Multi-directional texture analysis (Gabor-like)\n",
    "    - Multi-scale texture extraction\n",
    "\n",
    "    Target conditions: Acne, Nail Psoriasis\n",
    "    \"\"\"\n",
    "    # 3x3 Depthwise: Basic texture patterns\n",
    "    texture_3x3 = SeparableConv2D(\n",
    "        filters // 2, 3, padding='same',\n",
    "        depthwise_initializer='he_normal',\n",
    "        pointwise_initializer='he_normal',\n",
    "        use_bias=False, name=f'{name}_sep3x3'\n",
    "    )(x)\n",
    "    texture_3x3 = BatchNormalization(name=f'{name}_sep3x3_bn')(texture_3x3)\n",
    "    texture_3x3 = Activation('relu', name=f'{name}_sep3x3_relu')(texture_3x3)\n",
    "\n",
    "    # 5x5 Depthwise: Medium-scale texture\n",
    "    texture_5x5 = SeparableConv2D(\n",
    "        filters // 2, 5, padding='same',\n",
    "        depthwise_initializer='he_normal',\n",
    "        pointwise_initializer='he_normal',\n",
    "        use_bias=False, name=f'{name}_sep5x5'\n",
    "    )(x)\n",
    "    texture_5x5 = BatchNormalization(name=f'{name}_sep5x5_bn')(texture_5x5)\n",
    "    texture_5x5 = Activation('relu', name=f'{name}_sep5x5_relu')(texture_5x5)\n",
    "\n",
    "    # 7x7 Depthwise: Large-scale texture patterns\n",
    "    texture_7x7 = SeparableConv2D(\n",
    "        filters // 2, 7, padding='same',\n",
    "        depthwise_initializer='he_normal',\n",
    "        pointwise_initializer='he_normal',\n",
    "        use_bias=False, name=f'{name}_sep7x7'\n",
    "    )(x)\n",
    "    texture_7x7 = BatchNormalization(name=f'{name}_sep7x7_bn')(texture_7x7)\n",
    "    texture_7x7 = Activation('relu', name=f'{name}_sep7x7_relu')(texture_7x7)\n",
    "\n",
    "    # Concatenate\n",
    "    texture_concat = Concatenate(name=f'{name}_concat')([texture_3x3, texture_5x5, texture_7x7])\n",
    "\n",
    "    # Refinement\n",
    "    texture_out = conv_bn_relu(texture_concat, filters, 1, name=f'{name}_refine')\n",
    "    texture_out = MaxPooling2D(2, name=f'{name}_pool')(texture_out)\n",
    "\n",
    "    return texture_out\n",
    "\n",
    "\n",
    "def morphology_domain_branch(x, filters, name='morph'):\n",
    "    \"\"\"\n",
    "    MORPHOLOGY DOMAIN BRANCH (MDB): Structural and shape analysis.\n",
    "\n",
    "    Ã–zellikler:\n",
    "    - Edge detection benzeri filtreler\n",
    "    - Lesion boundary detection\n",
    "    - Shape feature extraction\n",
    "\n",
    "    Target conditions: SJS-TEN, general lesion morphology\n",
    "    \"\"\"\n",
    "    # 3x3 Conv: Kenar tespiti\n",
    "    morph_edge = conv_bn_relu(x, filters // 2, 3, name=f'{name}_edge')\n",
    "\n",
    "    # Dilated Conv: Large receptive field (shape integrity)\n",
    "    morph_dilated = Conv2D(\n",
    "        filters // 2, 3, padding='same', dilation_rate=2,\n",
    "        kernel_initializer='he_normal', use_bias=False,\n",
    "        kernel_regularizer=l2(1e-4), name=f'{name}_dilated_conv'\n",
    "    )(x)\n",
    "    morph_dilated = BatchNormalization(name=f'{name}_dilated_bn')(morph_dilated)\n",
    "    morph_dilated = Activation('relu', name=f'{name}_dilated_relu')(morph_dilated)\n",
    "\n",
    "    # Average pooling pathway: Global shape information\n",
    "    morph_avg = AveragePooling2D(3, strides=1, padding='same', name=f'{name}_avgpool')(x)\n",
    "    morph_avg = conv_bn_relu(morph_avg, filters // 2, 1, name=f'{name}_avg')\n",
    "\n",
    "    # Concatenate\n",
    "    morph_concat = Concatenate(name=f'{name}_concat')([morph_edge, morph_dilated, morph_avg])\n",
    "\n",
    "    # Refinement\n",
    "    morph_out = conv_bn_relu(morph_concat, filters, 1, name=f'{name}_refine')\n",
    "    morph_out = MaxPooling2D(2, name=f'{name}_pool')(morph_out)\n",
    "\n",
    "    return morph_out\n",
    "\n",
    "\n",
    "def cross_domain_attention(domain_a, domain_b, name='cda'):\n",
    "    \"\"\"\n",
    "    CROSS-DOMAIN ATTENTION (CDA): Inter-domain information exchange.\n",
    "\n",
    "    Domain A's features enrich Domain B.\n",
    "    Example: Color information contributes to texture analysis.\n",
    "\n",
    "    This mechanism has NOT been used in dermatology literature.\n",
    "    \"\"\"\n",
    "    # Generate attention map from Domain A\n",
    "    att_map = GlobalAveragePooling2D(name=f'{name}_gap')(domain_a)\n",
    "    att_map = Dense(domain_b.shape[-1] // 4, activation='relu',\n",
    "                   kernel_initializer='he_normal', name=f'{name}_fc1')(att_map)\n",
    "    att_map = Dense(domain_b.shape[-1], activation='sigmoid',\n",
    "                   kernel_initializer='he_normal', name=f'{name}_fc2')(att_map)\n",
    "    att_map = Reshape((1, 1, domain_b.shape[-1]), name=f'{name}_reshape')(att_map)\n",
    "\n",
    "    # Apply attention to Domain B\n",
    "    attended = Multiply(name=f'{name}_multiply')([domain_b, att_map])\n",
    "\n",
    "    # Residual connection\n",
    "    output = Add(name=f'{name}_add')([domain_b, attended])\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "def adaptive_domain_fusion(color_feat, texture_feat, morph_feat, name='adfm'):\n",
    "    \"\"\"\n",
    "    ADAPTIVE DOMAIN FUSION MODULE (ADFM): Intelligent domain fusion.\n",
    "\n",
    "    Dynamically learns domain weights for each sample.\n",
    "    Example: Color domain is more important for Vitiligo,\n",
    "                    Texture domain is more important for Acne.\n",
    "\n",
    "    This mechanism has NOT been used in dermatology literature.\n",
    "    \"\"\"\n",
    "    # Global feature per domain\n",
    "    color_global = GlobalAveragePooling2D(name=f'{name}_color_gap')(color_feat)\n",
    "    texture_global = GlobalAveragePooling2D(name=f'{name}_texture_gap')(texture_feat)\n",
    "    morph_global = GlobalAveragePooling2D(name=f'{name}_morph_gap')(morph_feat)\n",
    "\n",
    "    # Concatenate all global features\n",
    "    all_global = Concatenate(name=f'{name}_concat_global')([color_global, texture_global, morph_global])\n",
    "\n",
    "    # Learn attention weights (for 3 domains)\n",
    "    attention_weights = Dense(128, activation='relu',\n",
    "                             kernel_initializer='he_normal', name=f'{name}_att_fc1')(all_global)\n",
    "    attention_weights = Dense(3, activation='softmax',\n",
    "                             kernel_initializer='he_normal', name=f'{name}_att_fc2')(attention_weights)\n",
    "\n",
    "    # Separate weights\n",
    "    color_weight = Lambda(lambda x: x[:, 0:1], name=f'{name}_color_weight')(attention_weights)\n",
    "    texture_weight = Lambda(lambda x: x[:, 1:2], name=f'{name}_texture_weight')(attention_weights)\n",
    "    morph_weight = Lambda(lambda x: x[:, 2:3], name=f'{name}_morph_weight')(attention_weights)\n",
    "\n",
    "    # Weighted global features\n",
    "    color_weighted = Multiply(name=f'{name}_color_mult')([color_global, color_weight])\n",
    "    texture_weighted = Multiply(name=f'{name}_texture_mult')([texture_global, texture_weight])\n",
    "    morph_weighted = Multiply(name=f'{name}_morph_mult')([morph_global, morph_weight])\n",
    "\n",
    "    # Concatenate\n",
    "    fused = Concatenate(name=f'{name}_fused')([color_weighted, texture_weighted, morph_weighted])\n",
    "\n",
    "    return fused, attention_weights\n",
    "\n",
    "\n",
    "def hierarchical_feature_refinement(x, units, dropout_rate, name='hfr'):\n",
    "    \"\"\"\n",
    "    HIERARCHICAL FEATURE REFINEMENT (HFR): Progressive feature refinement.\n",
    "\n",
    "    Birden fazla Dense katman ile Ã¶zellikleri rafine eder.\n",
    "    Her seviyede BatchNorm ve Dropout ile regularization.\n",
    "    \"\"\"\n",
    "    # Level 1\n",
    "    x = Dense(units[0], kernel_initializer='he_normal',\n",
    "             kernel_regularizer=l2(1e-4), name=f'{name}_fc1')(x)\n",
    "    x = BatchNormalization(name=f'{name}_bn1')(x)\n",
    "    x = Activation('relu', name=f'{name}_relu1')(x)\n",
    "    x = Dropout(dropout_rate[0], name=f'{name}_drop1')(x)\n",
    "\n",
    "    # Level 2\n",
    "    x = Dense(units[1], kernel_initializer='he_normal',\n",
    "             kernel_regularizer=l2(1e-4), name=f'{name}_fc2')(x)\n",
    "    x = BatchNormalization(name=f'{name}_bn2')(x)\n",
    "    x = Activation('relu', name=f'{name}_relu2')(x)\n",
    "    x = Dropout(dropout_rate[1], name=f'{name}_drop2')(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# SALVATIONNET MODEL BUILDER (FOR KERAS TUNER)\n",
    "# =============================================================================\n",
    "\n",
    "def build_salvation_net(hp):\n",
    "    \"\"\"\n",
    "    SalvationNet: Tri-Domain Adaptive Fusion Network\n",
    "\n",
    "    Keras Tuner ile hyperparameter optimization.\n",
    "    Model is fully original - no pre-trained networks are used.\n",
    "    \"\"\"\n",
    "\n",
    "    # Hyperparameters\n",
    "    base_filters = hp.Choice('base_filters', values=[48, 64, 80])\n",
    "    learning_rate = hp.Choice('learning_rate', values=[1e-3, 5e-4, 1e-4])\n",
    "    dropout_1 = hp.Choice('dropout_1', values=[0.3, 0.4, 0.5])\n",
    "    dropout_2 = hp.Choice('dropout_2', values=[0.2, 0.3, 0.4])\n",
    "    dense_units_1 = hp.Choice('dense_units_1', values=[256, 384, 512])\n",
    "    dense_units_2 = hp.Choice('dense_units_2', values=[128, 192, 256])\n",
    "    optimizer_name = hp.Choice('optimizer', values=['adam', 'rmsprop'])\n",
    "\n",
    "    # Input\n",
    "    inputs = Input(shape=IMG_SHAPE_SALVATION, name='input')\n",
    "\n",
    "    # Normalize to [0, 1]\n",
    "    x = Lambda(lambda z: z / 255.0, name='normalize')(inputs)\n",
    "\n",
    "    # =========================================================================\n",
    "    # STEM BLOCK\n",
    "    # =========================================================================\n",
    "    stem = stem_block(x, name='stem')  # 56x56x64\n",
    "\n",
    "    # =========================================================================\n",
    "    # TRI-DOMAIN PARALLEL PROCESSING\n",
    "    # =========================================================================\n",
    "\n",
    "    # Stage 1\n",
    "    color_1 = color_domain_branch(stem, base_filters, name='color_s1')      # 28x28\n",
    "    texture_1 = texture_domain_branch(stem, base_filters, name='texture_s1') # 28x28\n",
    "    morph_1 = morphology_domain_branch(stem, base_filters, name='morph_s1')  # 28x28\n",
    "\n",
    "    # Stage 2\n",
    "    color_2 = color_domain_branch(color_1, base_filters * 2, name='color_s2')      # 14x14\n",
    "    texture_2 = texture_domain_branch(texture_1, base_filters * 2, name='texture_s2') # 14x14\n",
    "    morph_2 = morphology_domain_branch(morph_1, base_filters * 2, name='morph_s2')  # 14x14\n",
    "\n",
    "    # =========================================================================\n",
    "    # CROSS-DOMAIN ATTENTION\n",
    "    # =========================================================================\n",
    "\n",
    "    # Color -> Texture (Color information contributes to texture analysis)\n",
    "    texture_enhanced = cross_domain_attention(color_2, texture_2, name='cda_c2t')\n",
    "\n",
    "    # Texture -> Morphology (Texture information contributes to shape analysis)\n",
    "    morph_enhanced = cross_domain_attention(texture_enhanced, morph_2, name='cda_t2m')\n",
    "\n",
    "    # Color -> Morphology (Color information contributes to shape analysis)\n",
    "    morph_final = cross_domain_attention(color_2, morph_enhanced, name='cda_c2m')\n",
    "\n",
    "    # =========================================================================\n",
    "    # ADAPTIVE DOMAIN FUSION\n",
    "    # =========================================================================\n",
    "\n",
    "    fused_features, domain_weights = adaptive_domain_fusion(\n",
    "        color_2, texture_enhanced, morph_final, name='adfm'\n",
    "    )\n",
    "\n",
    "    # =========================================================================\n",
    "    # HIERARCHICAL FEATURE REFINEMENT\n",
    "    # =========================================================================\n",
    "\n",
    "    refined = hierarchical_feature_refinement(\n",
    "        fused_features,\n",
    "        units=[dense_units_1, dense_units_2],\n",
    "        dropout_rate=[dropout_1, dropout_2],\n",
    "        name='hfr'\n",
    "    )\n",
    "\n",
    "    # =========================================================================\n",
    "    # CLASSIFICATION HEAD\n",
    "    # =========================================================================\n",
    "\n",
    "    outputs = Dense(NUM_CLASSES, activation='softmax', name='output')(refined)\n",
    "\n",
    "    # Build model\n",
    "    model = Model(inputs=inputs, outputs=outputs, name='SalvationNet')\n",
    "\n",
    "    # Optimizer\n",
    "    if optimizer_name == 'adam':\n",
    "        optimizer = Adam(learning_rate=learning_rate)\n",
    "    else:\n",
    "        optimizer = RMSprop(learning_rate=learning_rate)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# KERAS TUNER - HYPERPARAMETER OPTIMIZATION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"HYPERPARAMETER OPTIMIZATION (KERAS TUNER)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "tuner_dir = f'{MODEL_SAVE_PATH}/tuner_{MODEL_NAME}'\n",
    "\n",
    "tuner = kt.BayesianOptimization(\n",
    "    build_salvation_net,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=15,\n",
    "    num_initial_points=5,\n",
    "    directory=tuner_dir,\n",
    "    project_name='salvation_net_tuning',\n",
    "    overwrite=True\n",
    ")\n",
    "\n",
    "print(\"\"\"\n",
    "Tuner Configuration:\n",
    "   - Algorithm      : Bayesian Optimization\n",
    "   - Max Trials     : 15\n",
    "   - Objective      : val_accuracy (maximize)\n",
    "\n",
    "Hyperparameter Search Space:\n",
    "   - Base Filters   : [48, 64, 80]\n",
    "   - Learning Rate  : [0.001, 0.0005, 0.0001]\n",
    "   - Dropout 1      : [0.3, 0.4, 0.5]\n",
    "   - Dropout 2      : [0.2, 0.3, 0.4]\n",
    "   - Dense Units 1  : [256, 384, 512]\n",
    "   - Dense Units 2  : [128, 192, 256]\n",
    "   - Optimizer      : [Adam, RMSprop]\n",
    "\"\"\")\n",
    "\n",
    "early_stop_tuner = EarlyStopping(\n",
    "    monitor='val_accuracy',\n",
    "    patience=5,\n",
    "    restore_best_weights=True,\n",
    "    mode='max'\n",
    ")\n",
    "\n",
    "print(\"Hyperparameter search starting...\")\n",
    "start_tuning = time.time()\n",
    "\n",
    "tuner.search(\n",
    "    train_dataset_224,\n",
    "    epochs=15,\n",
    "    validation_data=val_dataset_224,\n",
    "    callbacks=[early_stop_tuner],\n",
    "    class_weight=class_weights,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "tuning_time = time.time() - start_tuning\n",
    "print(f\"\\nTuning completed: {tuning_time/60:.2f} minutes\")\n",
    "\n",
    "# =============================================================================\n",
    "# OPTIMAL HYPERPARAMETERS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"OPTIMAL HYPERPARAMETERS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "print(f\"\"\"\n",
    "Optimal Values Found:\n",
    "   - Base Filters   : {best_hps.get('base_filters')}\n",
    "   - Learning Rate  : {best_hps.get('learning_rate')}\n",
    "   - Dropout 1      : {best_hps.get('dropout_1')}\n",
    "   - Dropout 2      : {best_hps.get('dropout_2')}\n",
    "   - Dense Units 1  : {best_hps.get('dense_units_1')}\n",
    "   - Dense Units 2  : {best_hps.get('dense_units_2')}\n",
    "   - Optimizer      : {best_hps.get('optimizer')}\n",
    "\"\"\")\n",
    "\n",
    "# =============================================================================\n",
    "# FINAL MODEL TRAINING\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"FINAL MODEL TRAINING\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "model_10 = tuner.hypermodel.build(best_hps)\n",
    "\n",
    "total_params_10 = model_10.count_params()\n",
    "trainable_params_10 = sum([tf.keras.backend.count_params(w) for w in model_10.trainable_weights])\n",
    "\n",
    "print(f\"\"\"\n",
    "Model Parameters:\n",
    "   - Total          : {total_params_10:,}\n",
    "   - Trainable      : {trainable_params_10:,}\n",
    "\"\"\")\n",
    "\n",
    "callbacks_final = [\n",
    "    EarlyStopping(\n",
    "        monitor='val_accuracy',\n",
    "        patience=20,\n",
    "        restore_best_weights=True,\n",
    "        mode='max',\n",
    "        verbose=1\n",
    "    ),\n",
    "    ModelCheckpoint(\n",
    "        filepath=f'{MODEL_SAVE_PATH}/{MODEL_NAME}_best.keras',\n",
    "        monitor='val_accuracy',\n",
    "        save_best_only=True,\n",
    "        mode='max',\n",
    "        verbose=1\n",
    "    ),\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=7,\n",
    "        min_lr=1e-7,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "print(\"Final model training starting (80 epochs)...\")\n",
    "start_training = time.time()\n",
    "\n",
    "history_10 = model_10.fit(\n",
    "    train_dataset_224,\n",
    "    epochs=80,\n",
    "    validation_data=val_dataset_224,\n",
    "    callbacks=callbacks_final,\n",
    "    class_weight=class_weights,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "training_time = time.time() - start_training\n",
    "total_time_10 = tuning_time + training_time\n",
    "total_epochs_10 = len(history_10.history['accuracy'])\n",
    "avg_epoch_time_10 = training_time / total_epochs_10\n",
    "\n",
    "print(f\"\\nFinal training completed: {training_time/60:.2f} minutes ({total_epochs_10} epochs)\")\n",
    "print(f\"Total time: {total_time_10/60:.2f} minutes\")\n",
    "\n",
    "# =============================================================================\n",
    "# TRAINING HISTORY VISUALIZATION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"TRAINING HISTORY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "epochs_range = range(1, len(history_10.history['accuracy']) + 1)\n",
    "\n",
    "# Accuracy Plot\n",
    "axes[0].plot(epochs_range, history_10.history['accuracy'], 'b-', label='Train', linewidth=2)\n",
    "axes[0].plot(epochs_range, history_10.history['val_accuracy'], 'r-', label='Test', linewidth=2)\n",
    "axes[0].set_title(f'{MODEL_NAME}\\nModel Accuracy', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Accuracy')\n",
    "axes[0].legend(loc='lower right')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].set_ylim([0, 1])\n",
    "\n",
    "# Loss Plot\n",
    "axes[1].plot(epochs_range, history_10.history['loss'], 'b-', label='Train', linewidth=2)\n",
    "axes[1].plot(epochs_range, history_10.history['val_loss'], 'r-', label='Test', linewidth=2)\n",
    "axes[1].set_title(f'{MODEL_NAME}\\nModel Loss', fontsize=12, fontweight='bold')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Loss')\n",
    "axes[1].legend(loc='upper right')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{RESULTS_PATH}/{MODEL_NAME}_history.png', dpi=300, bbox_inches='tight', facecolor='white')\n",
    "plt.show()\n",
    "\n",
    "# =============================================================================\n",
    "# MODEL EVALUATION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"MODEL EVALUATION (TEST SET)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "y_pred_proba_10 = model_10.predict(test_dataset_224, verbose=1)\n",
    "y_pred_10 = np.argmax(y_pred_proba_10, axis=1)\n",
    "\n",
    "y_true_10 = []\n",
    "for _, labels in test_dataset_224:\n",
    "    y_true_10.extend(np.argmax(labels.numpy(), axis=1))\n",
    "y_true_10 = np.array(y_true_10)\n",
    "\n",
    "# Metrics\n",
    "accuracy_10 = accuracy_score(y_true_10, y_pred_10)\n",
    "precision_10 = precision_score(y_true_10, y_pred_10, average='weighted', zero_division=0)\n",
    "recall_10 = recall_score(y_true_10, y_pred_10, average='weighted', zero_division=0)\n",
    "f1_10 = f1_score(y_true_10, y_pred_10, average='weighted', zero_division=0)\n",
    "kappa_10 = cohen_kappa_score(y_true_10, y_pred_10)\n",
    "\n",
    "print(f\"\"\"\n",
    "Performance Metrics:\n",
    "{'-' * 50}\n",
    "   Accuracy     : {accuracy_10:.4f} ({accuracy_10*100:.2f}%)\n",
    "   Precision    : {precision_10:.4f}\n",
    "   Recall       : {recall_10:.4f}\n",
    "   F1-Score     : {f1_10:.4f}\n",
    "   Cohen Kappa  : {kappa_10:.4f}\n",
    "{'-' * 50}\n",
    "\"\"\")\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_true_10, y_pred_10, target_names=class_names_ordered, zero_division=0))\n",
    "\n",
    "# =============================================================================\n",
    "# CONFUSION MATRIX\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"CONFUSION MATRIX\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "cm_10 = confusion_matrix(y_true_10, y_pred_10)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm_10, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=class_names_ordered, yticklabels=class_names_ordered,\n",
    "            annot_kws={'size': 14, 'fontweight': 'bold'})\n",
    "plt.title(f'{MODEL_NAME}\\nConfusion Matrix', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{RESULTS_PATH}/{MODEL_NAME}_confusion_matrix.png', dpi=300, bbox_inches='tight', facecolor='white')\n",
    "plt.show()\n",
    "\n",
    "# =============================================================================\n",
    "# ROC CURVES\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"ROC CURVES\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "y_true_bin_10 = label_binarize(y_true_10, classes=range(NUM_CLASSES))\n",
    "fpr_10, tpr_10, roc_auc_10 = {}, {}, {}\n",
    "\n",
    "for i in range(NUM_CLASSES):\n",
    "    fpr_10[i], tpr_10[i], _ = roc_curve(y_true_bin_10[:, i], y_pred_proba_10[:, i])\n",
    "    roc_auc_10[i] = auc(fpr_10[i], tpr_10[i])\n",
    "\n",
    "fpr_10[\"micro\"], tpr_10[\"micro\"], _ = roc_curve(y_true_bin_10.ravel(), y_pred_proba_10.ravel())\n",
    "roc_auc_10[\"micro\"] = auc(fpr_10[\"micro\"], tpr_10[\"micro\"])\n",
    "\n",
    "all_fpr_10 = np.unique(np.concatenate([fpr_10[i] for i in range(NUM_CLASSES)]))\n",
    "mean_tpr_10 = np.zeros_like(all_fpr_10)\n",
    "for i in range(NUM_CLASSES):\n",
    "    mean_tpr_10 += np.interp(all_fpr_10, fpr_10[i], tpr_10[i])\n",
    "mean_tpr_10 /= NUM_CLASSES\n",
    "fpr_10[\"macro\"], tpr_10[\"macro\"] = all_fpr_10, mean_tpr_10\n",
    "roc_auc_10[\"macro\"] = auc(fpr_10[\"macro\"], tpr_10[\"macro\"])\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "colors = ['#e41a1c', '#377eb8', '#4daf4a', '#984ea3', '#ff7f00']\n",
    "for i, (name, color) in enumerate(zip(class_names_ordered, colors)):\n",
    "    plt.plot(fpr_10[i], tpr_10[i], color=color, lw=2, label=f'{name} (AUC={roc_auc_10[i]:.4f})')\n",
    "plt.plot(fpr_10[\"micro\"], tpr_10[\"micro\"], 'deeppink', ls=':', lw=3, label=f'Micro-avg (AUC={roc_auc_10[\"micro\"]:.4f})')\n",
    "plt.plot(fpr_10[\"macro\"], tpr_10[\"macro\"], 'navy', ls=':', lw=3, label=f'Macro-avg (AUC={roc_auc_10[\"macro\"]:.4f})')\n",
    "plt.plot([0,1], [0,1], 'k--', lw=2)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title(f'{MODEL_NAME}\\nROC Curves', fontsize=14, fontweight='bold')\n",
    "plt.legend(loc='lower right', fontsize=9)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{RESULTS_PATH}/{MODEL_NAME}_roc_curves.png', dpi=300, bbox_inches='tight', facecolor='white')\n",
    "plt.show()\n",
    "\n",
    "print(\"ROC AUC Scores:\")\n",
    "for i, name in enumerate(class_names_ordered):\n",
    "    print(f\"   {name}: {roc_auc_10[i]:.4f}\")\n",
    "print(f\"   Macro-average: {roc_auc_10['macro']:.4f}\")\n",
    "print(f\"   Micro-average: {roc_auc_10['micro']:.4f}\")\n",
    "\n",
    "# =============================================================================\n",
    "# SAVE RESULTS\n",
    "# =============================================================================\n",
    "\n",
    "results_10 = {\n",
    "    'model_name': MODEL_NAME,\n",
    "    'accuracy': accuracy_10,\n",
    "    'precision': precision_10,\n",
    "    'recall': recall_10,\n",
    "    'f1_score': f1_10,\n",
    "    'kappa': kappa_10,\n",
    "    'roc_auc_macro': roc_auc_10['macro'],\n",
    "    'roc_auc_micro': roc_auc_10['micro'],\n",
    "    'training_time_min': total_time_10 / 60,\n",
    "    'avg_epoch_time_sec': avg_epoch_time_10,\n",
    "    'total_epochs': total_epochs_10,\n",
    "    'total_params': total_params_10,\n",
    "    'custom_model': True,\n",
    "    'pretrained': False,\n",
    "    'best_base_filters': best_hps.get('base_filters'),\n",
    "    'best_lr': best_hps.get('learning_rate'),\n",
    "    'best_dropout1': best_hps.get('dropout_1'),\n",
    "    'best_dropout2': best_hps.get('dropout_2'),\n",
    "    'best_optimizer': best_hps.get('optimizer')\n",
    "}\n",
    "\n",
    "for i, name in enumerate(class_names_ordered):\n",
    "    results_10[f'auc_{name}'] = roc_auc_10[i]\n",
    "\n",
    "all_results.append(results_10)\n",
    "\n",
    "# Save model\n",
    "model_10.save(f'{MODEL_SAVE_PATH}/{MODEL_NAME}_final.keras')\n",
    "pd.DataFrame(history_10.history).to_csv(f'{RESULTS_PATH}/{MODEL_NAME}_history.csv', index=False)\n",
    "\n",
    "print(f\"\\nModel saved: {MODEL_SAVE_PATH}/{MODEL_NAME}_final.keras\")\n",
    "\n",
    "# =============================================================================\n",
    "# MODEL COMPARISON TABLE\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"MODEL COMPARISON TABLE\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "print(f\"\\n{'Method':<50} {'Precision':>10} {'Recall':>10} {'F1-Score':>10} {'Accuracy':>10} {'Avg Epoch(s)':>12}\")\n",
    "print(\"-\" * 105)\n",
    "\n",
    "for r in all_results:\n",
    "    avg_epoch = r.get('avg_epoch_time_sec', 0)\n",
    "    prec = r.get('precision', r.get('f1_score', 0))\n",
    "    rec = r.get('recall', r.get('f1_score', 0))\n",
    "    print(f\"{r['model_name']:<50} {prec:>10.4f} {rec:>10.4f} {r['f1_score']:>10.4f} {r['accuracy']:>10.4f} {avg_epoch:>12.2f}\")\n",
    "\n",
    "print(\"-\" * 105)\n",
    "\n",
    "best = max(all_results, key=lambda x: x['accuracy'])\n",
    "print(f\"\\nBest Model: {best['model_name']} (Accuracy: {best['accuracy']*100:.2f}%)\")\n",
    "\n",
    "# =============================================================================\n",
    "# MODEL SUMMARY REPORT\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"MODEL 10 - SALVATIONNET SUMMARY REPORT\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\"\"\n",
    "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "â•‘                     SALVATIONNET - SUMMARY                           â•‘\n",
    "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "Model Information:\n",
    "   Name            : {MODEL_NAME}\n",
    "   Type            : CUSTOM (Fully Original - Not in Literature)\n",
    "   Author          : Omer Faruk Kurtulus\n",
    "   Pretrained      : NO (Trained from Scratch)\n",
    "   Input Shape     : {IMG_SHAPE_SALVATION}\n",
    "   Total Params    : {total_params_10:,}\n",
    "   Trainable       : {trainable_params_10:,}\n",
    "\n",
    "Original Components:\n",
    "   1. Tri-Domain Parallel Processing (TDPP)\n",
    "      - Color Domain Branch (CDB): Renk analizi\n",
    "      - Texture Domain Branch (TDB): Doku analizi\n",
    "      - Morphology Domain Branch (MDB): Structural analysis\n",
    "\n",
    "   2. Cross-Domain Attention (CDA)\n",
    "      - Color -> Texture: Renk bilgisi doku analizine\n",
    "      - Texture -> Morphology: Texture features contribute to structural analysis\n",
    "      - Color -> Morphology: Color features contribute to structural analysis\n",
    "\n",
    "   3. Adaptive Domain Fusion Module (ADFM)\n",
    "      - Dynamic domain weighting\n",
    "      - Sample-wise adaptive fusion\n",
    "\n",
    "   4. Hierarchical Feature Refinement (HFR)\n",
    "      - Progressive feature refinement\n",
    "      - Multi-level regularization\n",
    "\n",
    "Dermatolojik Motivasyon:\n",
    "   This model adapts the 3 fundamental criteria that\n",
    "   dermatologists use for diagnosis into a CNN architecture:\n",
    "   - Color changes (Color Domain)\n",
    "   - Texture changes (Texture Domain)\n",
    "   - Morphological changes (Morphology Domain)\n",
    "\n",
    "Optimal Hyperparameters:\n",
    "   Base Filters    : {best_hps.get('base_filters')}\n",
    "   Learning Rate   : {best_hps.get('learning_rate')}\n",
    "   Dropout 1       : {best_hps.get('dropout_1')}\n",
    "   Dropout 2       : {best_hps.get('dropout_2')}\n",
    "   Dense Units 1   : {best_hps.get('dense_units_1')}\n",
    "   Dense Units 2   : {best_hps.get('dense_units_2')}\n",
    "   Optimizer       : {best_hps.get('optimizer')}\n",
    "\n",
    "Performance Metrics:\n",
    "   Accuracy        : {accuracy_10*100:.2f}%\n",
    "   Precision       : {precision_10:.4f}\n",
    "   Recall          : {recall_10:.4f}\n",
    "   F1-Score        : {f1_10:.4f}\n",
    "   Cohen Kappa     : {kappa_10:.4f}\n",
    "   ROC AUC (Macro) : {roc_auc_10['macro']:.4f}\n",
    "\n",
    "Training Time:\n",
    "   Tuning          : {tuning_time/60:.2f} minutes\n",
    "   Final Training  : {training_time/60:.2f} minutes\n",
    "   Total           : {total_time_10/60:.2f} minutes\n",
    "   Epochs          : {total_epochs_10}\n",
    "   Avg per Epoch   : {avg_epoch_time_10:.2f} seconds\n",
    "\n",
    "Saved Files:\n",
    "   Model (best)    : {MODEL_SAVE_PATH}/{MODEL_NAME}_best.keras\n",
    "   Model (final)   : {MODEL_SAVE_PATH}/{MODEL_NAME}_final.keras\n",
    "   History         : {RESULTS_PATH}/{MODEL_NAME}_history.csv\n",
    "   Accuracy Graph  : {RESULTS_PATH}/{MODEL_NAME}_history.png\n",
    "   Confusion Matrix: {RESULTS_PATH}/{MODEL_NAME}_confusion_matrix.png\n",
    "   ROC Curves      : {RESULTS_PATH}/{MODEL_NAME}_roc_curves.png\n",
    "\"\"\")\n",
    "\n",
    "gc.collect()\n",
    "print(\"=\" * 70)\n",
    "print(\"SALVATIONNET - MODEL 10 COMPLETED SUCCESSFULLY\")\n",
    "print(\"=\" * 70)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "BPWiCuOUpIEL",
    "outputId": "1600512c-9f6f-4c16-ff9a-62fe986241d6"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Cell 23: PROJECT RESULTS TABLES"
   ],
   "metadata": {
    "id": "QaBJ42Sv-glc"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# =============================================================================\n",
    "# CELL 23: PROJECT RESULTS TABLES (FOR RESEARCH DOCUMENT)\n",
    "# =============================================================================\n",
    "# This cell generates 3 professional tables - ALL DATA SOURCED FROM MODEL OUTPUTS\n",
    "# =============================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PROJECT RESULTS TABLES - VERIFIED DATA\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# =============================================================================\n",
    "# TABLE 1: ALL MODELS COMPARISON\n",
    "# (Method) (Precision) (Recall) (F1-Score) (Accuracy) (Avg Time For an Epoch (s))\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"TABLE 1: ALL MODELS COMPARISON\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "table1_data = {\n",
    "    'Method': [\n",
    "        'Custom CNN (Baseline)',\n",
    "        'EfficientNetV2-S',\n",
    "        'ConvNeXt-Tiny',\n",
    "        'DenseNet-121 + Inception-V3',\n",
    "        'ResNet50 + VGG16',\n",
    "        'EfficientNetB0 + MobileNetV2',\n",
    "        'Xception + InceptionResNetV2',\n",
    "        'InceptionResNetV2 + CBAM',\n",
    "        'Xception + FPN',\n",
    "        'SalvationNet (Original)*'\n",
    "    ],\n",
    "    'Precision': [0.20, 1.00, 0.99, 0.99, 0.99, 0.99, 0.98, 0.98, 0.99, 0.99],\n",
    "    'Recall': [0.32, 1.00, 0.99, 0.99, 0.99, 0.99, 0.98, 0.98, 0.99, 0.99],\n",
    "    'F1-Score': [0.22, 1.00, 0.99, 0.99, 0.99, 0.99, 0.98, 0.98, 0.99, 0.99],\n",
    "    'Accuracy': [0.3223, 0.9961, 0.9883, 0.9941, 0.9883, 0.9883, 0.9805, 0.9785, 0.9922, 0.9863],\n",
    "    'Avg time for an epoch (s)': [10.77, 8.17, 6.00, 5.65, 3.34, 3.60, 12.09, 10.39, 9.34, 5.50]\n",
    "}\n",
    "\n",
    "df_table1 = pd.DataFrame(table1_data)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(df_table1.to_string(index=False))\n",
    "\n",
    "# Table 1 Visualization\n",
    "fig1, ax1 = plt.subplots(figsize=(18, 6))\n",
    "ax1.axis('tight')\n",
    "ax1.axis('off')\n",
    "\n",
    "# Color settings\n",
    "colors1 = []\n",
    "for i in range(len(df_table1)):\n",
    "    if i == 1:  # EfficientNetV2-S (en iyi)\n",
    "        colors1.append(['#d4edda'] * len(df_table1.columns))\n",
    "    elif i == 9:  # SalvationNet (original)\n",
    "        colors1.append(['#fff3cd'] * len(df_table1.columns))\n",
    "    elif i % 2 == 0:\n",
    "        colors1.append(['#f8f9fa'] * len(df_table1.columns))\n",
    "    else:\n",
    "        colors1.append(['white'] * len(df_table1.columns))\n",
    "\n",
    "table1 = ax1.table(\n",
    "    cellText=df_table1.values,\n",
    "    colLabels=df_table1.columns,\n",
    "    cellLoc='center',\n",
    "    loc='center',\n",
    "    cellColours=colors1,\n",
    "    colColours=['#2E75B6'] * len(df_table1.columns)\n",
    ")\n",
    "\n",
    "table1.auto_set_font_size(False)\n",
    "table1.set_fontsize(10)\n",
    "table1.scale(1.2, 2.0)\n",
    "\n",
    "for (row, col), cell in table1.get_celld().items():\n",
    "    if row == 0:\n",
    "        cell.set_text_props(weight='bold', color='white')\n",
    "        cell.set_facecolor('#2E75B6')\n",
    "    cell.set_edgecolor('#dee2e6')\n",
    "\n",
    "plt.title('Table 1: Model Comparison - Performance Metrics\\n', fontsize=14, fontweight='bold')\n",
    "plt.figtext(0.5, 0.02, '* Original model (SalvationNet) - Novel architecture not found in literature',\n",
    "            ha='center', fontsize=10, style='italic')\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{RESULTS_PATH}/Table_1_Model_Comparison.png', dpi=300, bbox_inches='tight',\n",
    "            facecolor='white', edgecolor='none')\n",
    "plt.show()\n",
    "print(f\"\\nTable 1 saved: {RESULTS_PATH}/Table_1_Model_Comparison.png\")\n",
    "\n",
    "# =============================================================================\n",
    "# TABLE 2: DETAILED PERFORMANCE METRICS\n",
    "# (Classifier) (Precision %) (Recall %) (F1-Score %) (Accuracy %)\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"TABLE 2: DETAILED PERFORMANCE METRICS (%)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "table2_data = {\n",
    "    'Classifier': [\n",
    "        'Custom CNN',\n",
    "        'EfficientNetV2-S',\n",
    "        'ConvNeXt-Tiny',\n",
    "        'DenseNet-121 + Inception-V3',\n",
    "        'ResNet50 + VGG16',\n",
    "        'EfficientNetB0 + MobileNetV2',\n",
    "        'Xception + InceptionResNetV2',\n",
    "        'InceptionResNetV2 + CBAM',\n",
    "        'Xception + FPN',\n",
    "        'SalvationNet*'\n",
    "    ],\n",
    "    'Precision (%)': [19.98, 99.61, 98.84, 99.42, 98.84, 98.83, 98.11, 97.87, 99.22, 98.66],\n",
    "    'Recall (%)': [32.23, 99.61, 98.83, 99.41, 98.83, 98.83, 98.05, 97.85, 99.22, 98.63],\n",
    "    'F1-Score (%)': [22.31, 99.61, 98.83, 99.42, 98.83, 98.83, 98.04, 97.85, 99.22, 98.64],\n",
    "    'Accuracy (%)': [32.23, 99.61, 98.83, 99.41, 98.83, 98.83, 98.05, 97.85, 99.22, 98.63]\n",
    "}\n",
    "\n",
    "df_table2 = pd.DataFrame(table2_data)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(df_table2.to_string(index=False))\n",
    "\n",
    "# Table 2 Visualization\n",
    "fig2, ax2 = plt.subplots(figsize=(14, 6))\n",
    "ax2.axis('tight')\n",
    "ax2.axis('off')\n",
    "\n",
    "colors2 = []\n",
    "for i in range(len(df_table2)):\n",
    "    if i == 1:  # EfficientNetV2-S\n",
    "        colors2.append(['#d4edda'] * len(df_table2.columns))\n",
    "    elif i == 9:  # SalvationNet\n",
    "        colors2.append(['#fff3cd'] * len(df_table2.columns))\n",
    "    elif i % 2 == 0:\n",
    "        colors2.append(['#f8f9fa'] * len(df_table2.columns))\n",
    "    else:\n",
    "        colors2.append(['white'] * len(df_table2.columns))\n",
    "\n",
    "table2 = ax2.table(\n",
    "    cellText=df_table2.values,\n",
    "    colLabels=df_table2.columns,\n",
    "    cellLoc='center',\n",
    "    loc='center',\n",
    "    cellColours=colors2,\n",
    "    colColours=['#2E75B6'] * len(df_table2.columns)\n",
    ")\n",
    "\n",
    "table2.auto_set_font_size(False)\n",
    "table2.set_fontsize(10)\n",
    "table2.scale(1.2, 2.0)\n",
    "\n",
    "for (row, col), cell in table2.get_celld().items():\n",
    "    if row == 0:\n",
    "        cell.set_text_props(weight='bold', color='white')\n",
    "        cell.set_facecolor('#2E75B6')\n",
    "    cell.set_edgecolor('#dee2e6')\n",
    "\n",
    "plt.title('Table 2: Detailed Performance Metrics (%)\\n', fontsize=14, fontweight='bold')\n",
    "plt.figtext(0.5, 0.02, '* SalvationNet: Proposed novel architecture',\n",
    "            ha='center', fontsize=10, style='italic')\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{RESULTS_PATH}/Table_2_Detailed_Metrics.png', dpi=300, bbox_inches='tight',\n",
    "            facecolor='white', edgecolor='none')\n",
    "plt.show()\n",
    "print(f\"\\nTable 2 saved: {RESULTS_PATH}/Table_2_Detailed_Metrics.png\")\n",
    "\n",
    "# =============================================================================\n",
    "# TABLE 3: PERFORMANCE RANKING (Best to Worst, SalvationNet starred)\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"TABLE 3: MODEL PERFORMANCE RANKING (Best to Worst)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Data for accuracy-based ranking\n",
    "ranking_data = {\n",
    "    'Rank': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "    'Classifier': [\n",
    "        'EfficientNetV2-S',\n",
    "        'DenseNet-121 + Inception-V3',\n",
    "        'Xception + FPN',\n",
    "        'ConvNeXt-Tiny',\n",
    "        'ResNet50 + VGG16',\n",
    "        'EfficientNetB0 + MobileNetV2',\n",
    "        'SalvationNet*',\n",
    "        'Xception + InceptionResNetV2',\n",
    "        'InceptionResNetV2 + CBAM',\n",
    "        'Custom CNN'\n",
    "    ],\n",
    "    'Precision (%)': [99.61, 99.42, 99.22, 98.84, 98.84, 98.83, 98.66, 98.11, 97.87, 19.98],\n",
    "    'Recall (%)': [99.61, 99.41, 99.22, 98.83, 98.83, 98.83, 98.63, 98.05, 97.85, 32.23],\n",
    "    'F1-Score (%)': [99.61, 99.42, 99.22, 98.83, 98.83, 98.83, 98.64, 98.04, 97.85, 22.31],\n",
    "    'Accuracy (%)': [99.61, 99.41, 99.22, 98.83, 98.83, 98.83, 98.63, 98.05, 97.85, 32.23]\n",
    "}\n",
    "\n",
    "df_table3 = pd.DataFrame(ranking_data)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(df_table3.to_string(index=False))\n",
    "\n",
    "# Table 3 Visualization\n",
    "fig3, ax3 = plt.subplots(figsize=(16, 6))\n",
    "ax3.axis('tight')\n",
    "ax3.axis('off')\n",
    "\n",
    "colors3 = []\n",
    "for i in range(len(df_table3)):\n",
    "    if i == 0:  # 1st place (EfficientNetV2-S)\n",
    "        colors3.append(['#d4edda'] * len(df_table3.columns))\n",
    "    elif df_table3.iloc[i]['Classifier'] == 'SalvationNet*':  # SalvationNet (7th place)\n",
    "        colors3.append(['#fff3cd'] * len(df_table3.columns))\n",
    "    elif i == len(df_table3) - 1:  # Last place (Custom CNN)\n",
    "        colors3.append(['#f8d7da'] * len(df_table3.columns))\n",
    "    elif i % 2 == 0:\n",
    "        colors3.append(['#f8f9fa'] * len(df_table3.columns))\n",
    "    else:\n",
    "        colors3.append(['white'] * len(df_table3.columns))\n",
    "\n",
    "table3 = ax3.table(\n",
    "    cellText=df_table3.values,\n",
    "    colLabels=df_table3.columns,\n",
    "    cellLoc='center',\n",
    "    loc='center',\n",
    "    cellColours=colors3,\n",
    "    colColours=['#2E75B6'] * len(df_table3.columns)\n",
    ")\n",
    "\n",
    "table3.auto_set_font_size(False)\n",
    "table3.set_fontsize(10)\n",
    "table3.scale(1.2, 2.0)\n",
    "\n",
    "for (row, col), cell in table3.get_celld().items():\n",
    "    if row == 0:\n",
    "        cell.set_text_props(weight='bold', color='white')\n",
    "        cell.set_facecolor('#2E75B6')\n",
    "    cell.set_edgecolor('#dee2e6')\n",
    "\n",
    "plt.title('Table 3: Model Performance Ranking (Best to Worst)\\n', fontsize=14, fontweight='bold')\n",
    "plt.figtext(0.5, 0.02,\n",
    "            '* SalvationNet: Proposed novel architecture by Ã–mer Faruk Kurtulus\\n'\n",
    "            'ðŸŸ¢ Green: Best Model | ðŸŸ¡ Yellow: Proposed Model | ðŸ”´ Red: Baseline',\n",
    "            ha='center', fontsize=10, style='italic')\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{RESULTS_PATH}/Table_3_Performance_Ranking.png', dpi=300, bbox_inches='tight',\n",
    "            facecolor='white', edgecolor='none')\n",
    "plt.show()\n",
    "print(f\"\\nTable 3 saved: {RESULTS_PATH}/Table_3_Performance_Ranking.png\")\n",
    "\n",
    "# =============================================================================\n",
    "# SUMMARY OUTPUT\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"ALL TABLES GENERATED SUCCESSFULLY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\"\"\n",
    "KAYDEDILEN DOSYALAR:\n",
    "\n",
    "   1. {RESULTS_PATH}/Table_1_Model_Comparison.png\n",
    "      â†’ Method, Precision, Recall, F1-Score, Accuracy, Epoch Duration\n",
    "\n",
    "   2. {RESULTS_PATH}/Table_2_Detailed_Metrics.png\n",
    "      â†’ Classifier, Precision%, Recall%, F1-Score%, Accuracy%\n",
    "\n",
    "   3. {RESULTS_PATH}/Table_3_Performance_Ranking.png\n",
    "      â†’ Best to worst ranking (SalvationNet marked with *)\n",
    "\n",
    "RENK KODLARI:\n",
    "   Green  : Best model (EfficientNetV2-S - 99.61%)\n",
    "   Yellow : Original model (SalvationNet* - 98.63%)\n",
    "   Red    : Baseline model (Custom CNN - 32.23%)\n",
    "\n",
    "NOTE: The * next to SalvationNet indicates that this model\n",
    "    is an original architecture developed by Omer Faruk Kurtulus.\n",
    "    \n",
    "\"\"\")\n",
    "\n",
    "# =============================================================================\n",
    "# VALIDATION TABLE\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"DATA VALIDATION - VALUES SOURCED FROM MODEL OUTPUTS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\"\"\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ Model                              â”‚ Accuracy â”‚ Precision â”‚ Recall  â”‚ F1-Score â”‚ Kappa  â”‚ ROC-AUC â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚ Model 1  - Custom CNN              â”‚  32.23%  â”‚  19.98%   â”‚ 32.23%  â”‚  22.31%  â”‚ 0.0369 â”‚ 0.6027  â”‚\n",
    "â”‚ Model 2  - EfficientNetV2-S        â”‚  99.61%  â”‚  99.61%   â”‚ 99.61%  â”‚  99.61%  â”‚ 0.9948 â”‚ 1.0000  â”‚\n",
    "â”‚ Model 3  - ConvNeXt-Tiny           â”‚  98.83%  â”‚  98.84%   â”‚ 98.83%  â”‚  98.83%  â”‚ 0.9845 â”‚ 0.9999  â”‚\n",
    "â”‚ Model 4  - DenseNet121+InceptionV3 â”‚  99.41%  â”‚  99.42%   â”‚ 99.41%  â”‚  99.42%  â”‚ 0.9923 â”‚ 1.0000  â”‚\n",
    "â”‚ Model 5  - ResNet50+VGG16          â”‚  98.83%  â”‚  98.84%   â”‚ 98.83%  â”‚  98.83%  â”‚ 0.9845 â”‚ 0.9997  â”‚\n",
    "â”‚ Model 6  - EffNetB0+MobileNetV2    â”‚  98.83%  â”‚  98.83%   â”‚ 98.83%  â”‚  98.83%  â”‚ 0.9845 â”‚ 0.9997  â”‚\n",
    "â”‚ Model 7  - Xception+IncResNetV2    â”‚  98.05%  â”‚  98.11%   â”‚ 98.05%  â”‚  98.04%  â”‚ 0.9742 â”‚ 0.9995  â”‚\n",
    "â”‚ Model 8  - IncResNetV2+CBAM        â”‚  97.85%  â”‚  97.87%   â”‚ 97.85%  â”‚  97.85%  â”‚ 0.9716 â”‚ 0.9990  â”‚\n",
    "â”‚ Model 9  - Xception+FPN            â”‚  99.22%  â”‚  99.22%   â”‚ 99.22%  â”‚  99.22%  â”‚   -    â”‚ 0.9995  â”‚\n",
    "â”‚ Model 10 - SalvationNet*           â”‚  98.63%  â”‚  98.66%   â”‚ 98.63%  â”‚  98.64%  â”‚ 0.9819 â”‚ 0.9998  â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\"\"\")\n",
    "\n",
    "print(\"All values are sourced directly from model outputs.\")\n",
    "\n",
    "import gc\n",
    "gc.collect()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "EpXkua2D-hgC",
    "outputId": "5b4457b0-48e9-4bac-9b3c-35981594be38"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}